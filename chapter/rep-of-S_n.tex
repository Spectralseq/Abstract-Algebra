\chapter{Representation of symmetric group}
\section{Facts about \boldmath\texorpdfstring{$S^\lambda$}{S}}
Define $A=\C[S_n]$ and
\[M^\lambda=Aa_\lambda,\quad \widetilde{M}^\lambda=Ab_\lambda,\quad S^\lambda=Ab_\lambda a_\lambda,\quad \widetilde{S}^\lambda=Aa_\lambda b_\lambda.\]
The we have the following surjective $A$-homomorphisms
\[\begin{tikzcd}[row sep=5pt]
A\ar[r,"\alpha"]&M^\lambda\ar[r,"\tilde{\beta}"]&\widetilde{S}^\lambda\\
A\ar[r,"\beta"]&\widetilde{M}^\lambda\ar[r,"\tilde{\alpha}"]&S^\lambda
\end{tikzcd}\]
Now we are going to find the kernel of these four maps. For simplicity we let $T$ be the tableau associated with $\lambda$.
\begin{lemma}
With the notations above, we have
\[\ker\alpha=A\langle\sigma-1:\sigma\in R(T)\rangle,\quad\ker\beta=A\langle\tau-\sgn(\tau)1:\sigma\in C(T)\rangle.\]
\end{lemma}
\begin{proof}
We only show the equaality for $\ker\alpha$. One direction is clear. Now let $\pi\in S_n$ be such that $\pi a_\lambda=0$. Then
\[-\pi=\sum_{\sigma\in R(T),\sigma\neq 1}\pi\sigma=\sum_{\sigma\in R(T),\sigma\neq 1}\pi(\sigma-1)+(|R(T)|-1)\pi\]
whence we see $\pi$ is in the right-hand side of the equality above, whence the claim.
\end{proof}
Let $Q^\lambda$ be the submodule of $\widetilde{M}^\lambda$ generated by all elements of the form
\[[T]-\sum[S]\]
where $S$ is obtained from $T$ by exchanging columns, and similarly let $\widetilde{Q}^\lambda$ be the submodule of $M^\lambda$ generated by all elements of the form
\[\{T\}-\sum(-1)^k\{W\}\]
where $W$ is obtained from $T$ by exchanging rows, and $k$ is the number of boxes that are exchanged.
\begin{lemma}
With the notations above,
\begin{itemize}
\item[(a)] The classes $[T]$, as $[T]$ varies over the standard tableaux on $\lambda$, span the quotient space $\widetilde{M}^\lambda/Q^\lambda$.
\item[(b)] The classes $\{T\}$, as $\{T\}$ varies over the standard tableaux on $\lambda$, span the quotient space $M^\lambda/\widetilde{Q}^\lambda$.  
\end{itemize}
\end{lemma}
\begin{proof}
This follows from a suitable ordering and an induction argument.
\end{proof}
\begin{proposition}
We have
\[\ker\tilde{\alpha}=Q^\lambda,\quad \ker\tilde{\beta}=\widetilde{Q}^\lambda.\]
\end{proposition}
\begin{proof}
It suffices to prove that $Q^\lambda\sub\ker\tilde{\alpha}$, since then we get a surjection $\widetilde{M}^\lambda/Q^\lambda\to S^\lambda$, which is an isomorphism since $\dim(S^\lambda)=f^\lambda$. Now if $H(A,B)$ denotes the permutations that ouccur in an exchange of columns, then we have
\[(1-\sum_{\tau\in H(A,B)}\tau)b_\lambda a_\lambda.\]
Since $(1-\sum_{\tau\in H(A,B)})b_\lambda$ corresponds to $[T]-\sum[S]$, we get the claim.
\end{proof}
\section{Weyl's construction}
Let $A=\bigoplus_{i\in I}A_i=\bigoplus_{i\in I}\mathcal{M}_{r_i}(\C)$ be a finite-dimensional semisimple algebra over $\C$. If we consider any finite-dimensional module right $M$ over $A$ we have seen that $M$ is isomorphic to a finite sum
\[M=\bigoplus_{i\in I}M_i=\bigoplus_{i\in I}S_i^{n_i}\]
where $S_i$ are irreducible right $A$-modules with $\dim(S_i)=r_i$. By definition, $A$ acts on $S_i$ by the standard right representation on $\C^{r_i}$ and as zero on other components. Therefore, we can identify $M_i$ as $\mathcal{M}_{n_i\times r_i}(\C)$ with multiplication on the right, and $M_i$ is decomposed by its rows.\par
We have also that
\[B:=\End_A(M)=\bigoplus_{i\in I}\End_A(M_i)=\bigoplus_{i\in I}\End_A(S_i^{n_i})=\bigoplus_{i\in I}\mathcal{M}_{n_i}(\C):=\bigoplus_{i\in I}B_i\]
where the block $B_i$ acts on $S_i^{n_i}$ as $r_i$ copies of its standard left matrix representation, and as zero on the other isotypic components. In other words, if $T=(a_{i,j})\in B_i$, $v=(v_1,\dots,v_{n_i})\in S_i^{n_i}$ with $v_j=\sum_{k=1}^{n_i}b_{jk}e_k$, then with $U=(b_{j,k})$ we have
\begin{align*}
T\cdot v&=T\begin{pmatrix}
v_1\\
v_2\\
\vdots\\
v_{n_i}
\end{pmatrix}=TU\begin{pmatrix}
e_1\\
e_2\\
\vdots\\
e_{r_i}
\end{pmatrix}
\end{align*}
Thus with the identification made above, $B_i$ acts on $M_i$ by multiplication on the left, and the $r_i$ subspaces of $\mathcal{M}_{n_i\times r_i}(\C)$ formed by the columns decompose this space into irreducible representations of $B_i$ isomorphic to the standard representation $\C^{n_i}$.
\begin{lemma}
Let $m,r,n$ be positive integers. Then
\[\mathcal{M}_{m\times r}(\C)\otimes_{\mathcal{M}_{r\times r}(\C)}\mathcal{M}_{r\times n}(\C)=\mathcal{M}_{m\times n}(\C)\]
as $(\mathcal{M}_{m}(\C),\mathcal{M}_{n}(\C))$-bimodules.
\end{lemma}
\begin{proposition}
Let $M$ be a finite-dimensional right $A$-module.
\begin{itemize}
\item[(a)] If $S$ is an irreducible left $A$-module, then $M\otimes_AS$ is an irreducible left $B$-module.
\item[(b)] If $S_i$ are the distinct irreducible left $A$-modules, with $r_i$ the dimension of $S_i$, then
\[M\cong\bigoplus_i(M\otimes_AS_i)^{r_i}\cong\bigoplus_i(Mc_i)^{r_i}\]
is the decomposition of $M$ into irreducible left $B$-modules.
\end{itemize}
\end{proposition}
\begin{proof}
For (a), first consider the case $M$ is irreducible. Then by the argument above, we can identify $S$ with a minimal left ideal of $A$ consisting of tuples of matrices which are zero except in one factor, and in this factor are all zero except for one column. Similarly, $M$ can be identified with the right ideal of tuples which are zero except in one factor, and in that factor all are zero except in one row. Then $M\otimes_AS$ will be zero unless the factor is the same for $M$ and $S$, in which case $M\otimes_AS$ can be identified with the product of these two factors, which is isomorphic to $\C$. This completes the proof when $M$ is irreducible. For the general case, decompose $M=\bigoplus_iS_i^{n_i}$ into a sum of irreducible right $A$-modules, so $M\otimes_AS=\bigoplus_i(S_i\otimes_AS)^{n_i}=\C^{n_j}$ for some $j$, which is visibly irreducible over $B$.\par
Now (b) follows since the isomorphism $A\cong\bigoplus_iW_i^{\oplus n_i}$ implies
\[M\cong M\otimes_AA\cong M\otimes_A(\bigoplus_iS_i^{\oplus n_i})\cong\bigoplus_i(M\otimes_AS_i)^{n_i}.\]
This completes the proof.
\end{proof}
\begin{corollary}
Given a semisimple $\C$-algebra $A$, a right $A$-module $M$, and let $B=\End_A(M)$ its centralizer. Then there exist two sequences of integers $n_1,\dots,n_k$ and $r_1,\dots,r_k$ and an isomorphism of $M$ with $\bigoplus_i\C^{n_i}\otimes\C^{r_i}$ under which the algebras $A,B$ are identified with the algebras $\bigoplus_i\mathcal{M}_{r_i}(\C)$ and $\bigoplus_i\mathcal{M}_{n_i}(\C)$.
\end{corollary}
Now let $V$ be a finite dimensional vector space and $M=V^{\otimes n}$. The endomorphisms of $M$ induced by endomorphisms of $V$ are certainly in $B$. Although $B$ is generally much larger than $\End(V)$, we have the following result.
\begin{proposition}
Let $A=\C S_n$. Then the algebra $B=\End_{A}(V^{\otimes n})$ is spanned as a linear subspace of $\End(V^{\otimes n})$ by $\End(V)$. Moreover, a subspace of $V^{\otimes n}$ is a sub-$B$-module if and only if it is invariant over $\GL(V)$.
\end{proposition}
\begin{proof}
Note that if $W$ is any finite-dimensional vector space, then $\mathrm{Sym}^n(W)$ is the subspace of $W^{\oplus n}$ spanned by all $w^n$ as $w$ runs through $W$, where
\[v_1\cdot v_2\cdots v_n=\frac{1}{n!}\sum_{\sigma\in S_n}v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(n)}\]
In fact, we have
\[n!v_1\cdot\cdots\cdot v_n=\sum_{I\sub\{1,\dots,n\}}(-1)^{n-|I|}\Big(\sum_{i\in I}v_i\Big)^n\]
Applying this to $\End(V)$ proves the first statement, since
\[\End(V^{\otimes n})=(V^*)^{\otimes n}\otimes V^{\otimes n}=(V^*\otimes V)^{\otimes n}=\End(V)^{\otimes n}.\]
with compatible actions of $S_n$. Since the image of $\GL(V)$ is closed in $\End(V^{\otimes n})$, the second claim follows.
\end{proof}
\begin{lemma}
We have
\[v_1\cdot v_2\cdots v_n=\frac{1}{n!}\sum_{\sigma\in S_n}v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(n)}\]
\end{lemma}
\begin{proof}
Consider a monomial $v_1^{i_1}\cdots v_n^{i_n}$ in the summation on the right. If at least one of the numbers $i_1,\dots,i_n$ is $0$, say $i_k=0$, then this monomial occurs an even number of times, and the occurrences cancel each other out, because every occurrence in an addend for some subset $I$ can be contrasted with a different occurrence, with opposite sign, in the addend for the subset $I\cup\{k\}$ (if $k\notin I$) or the subset $I\setminus\{k\}$ (if $k\in I$). So the only surviving monomials are those where all $n$ numbers $i_1,i_2,\dots,i_n$ are positive. But the only such monomial is $v_1v_2\cdots v_n$, and it is easy to see that it appears only once (namely, in the addend for $I=[n]$) and its coefficient is $n!$. Thus the claim follows.
\end{proof}
Now define $V^\lambda=V^{\otimes n}c_\lambda$. Then we get an isomorphism of $\GL(V)$-modules
\[V^{\otimes n}\cong\bigoplus_\lambda (V^\lambda)^{\oplus f^\lambda}.\]
\begin{theorem}
Let $V$ be a finite dimensional vector space with $\dim(V)=k$ and $\lambda=(\lambda_1,\dots,\lambda_k)$ a partition of $n$. Then
\begin{itemize}
\item[(a)] Each $V^\lambda$ is an irreducible representation of $\GL(V)$, and we have
\[\dim(V^\lambda)=s_\lambda(1,\dots,1)=\prod_{1\leq i<j\leq k}\frac{\lambda_i-\lambda_j+j-i}{j-i}.\] 
\item[(b)] For any $g\in\GL(V)$, the trace of $g$ on $V^\lambda$ is the value of $s_\lambda$ on the eigenvalues $x_1,\dots,x_k$ of $g$ on $V$:
\[\chi_{V^\lambda}(g)=s_\lambda(x_1,\dots,x_k)\] 
\end{itemize}
\end{theorem}
\begin{proof}
Let $\GL(V)$ acts on $\mathrm{Sym}^{d}(V)$. If $\{e_1,\dots,e_m\}$ is a basis for $V$, then $\{e_{i_1}\cdots e_{i_d}\}$ is a basis for $\mathrm{Sym}^d(V)$, and $g=\mathrm{diag}(x_1,\dots,x_m)$ acts on $\{e_{i_1}\cdots e_{i_d}\}$ by multiplying by $x_{i_1}\cdots x_{i_d}$. Hence $\tr(g)=h_d(x_1,\dots,x_m)$. Generalizing this, we see the trace of $g$ on $V^{\otimes n}\otimes_{\C S_n}M^\lambda$ is $h_\lambda(x_1,\dots,x_m)$. But since $M^\lambda=\bigoplus_\mu K_{\mu\lambda}S^\mu$, we have
\[h_\lambda(x_1,\dots,x_m)=\sum_\mu K_{\mu\lambda}\chi_{V^\lambda}(x_1,\dots,x_m).\]
Since $h_\lambda=\sum_\mu K_{\mu\lambda}s_\lambda$, this proves $\chi_{V^\lambda}(x_1,\dots,x_m)=s_\lambda(x_1,\dots,x_m)$.
\end{proof}
Let $V$ be a vector space and $\lambda$ an partition. We write $V^{\times\lambda}$ for the cartesian product of $n=|\lambda|$ copies of $V$, but labelled by the $n$ boxes of $\lambda$. If $V$ has a basis $\{v_1,\dots,v_m\}$, then for any filling $T$ on $\lambda$ with entries in $\{1,\dots,m\}$ we get an element of $V^{\times\lambda}$
\begin{lemma}\label{Weyl module char by conditions}
If $V$ is free with basis $\{v_1,\dots,v_m\}$, then $V^\lambda\cong F/Q$, where $F$ is free on elements $v_T$ for all fillings on $\lambda$ with entries in $\{1,\dots,m\}$, and $Q$ is generated by the following elements:
\begin{itemize}
\item[(a)] $v_T$ if $T$ has two equal entries in any column.
\item[(b)] $v_T+v_{T'}$ if $T'$ is obtained from $T$ by permuting two elements in a column.
\item[(c)] $v_T-\sum v_S$, where $S$ is obtained from $T$ by an exchange for a given subset of a column of $T$.
\end{itemize}
\end{lemma}
\subsection{The determinant construction}
Let $V$ be an $m$-dimensional vector space. Let $\{Z_{ij}:1\leq i\leq n,1\leq j\leq m\}$ be indeterminates and let $\C[Z]=\C[\{Z_{ij}\}]$. For each $p$-tuple $(i_1,\dots,i_p)$ of integers from $\{1,\dots,m\}$ with $p\leq n$, we define
\[D_{i_1,\dots,i_p}=\det\begin{bmatrix}
Z_{1,i_1}&Z_{1,i_2}&\cdots&Z_{1,i_p}\\
Z_{2,i_1}&Z_{2,i_2}&\cdots&Z_{2,i_p}\\
\vdots&\vdots&\ddots&\vdots\\
Z_{p,i_1}&Z_{p,i_2}&\cdots&Z_{p,i_p}\\
\end{bmatrix}\]
This is an alternating function on the subscripts $i_1,\dots,i_p$.\par
For a Young diagram $\lambda$ with at most $n$ rows and an arbitrary filling $T$ of $\lambda$ with numbers in $\{1,\dots,m\}$, let $D_T$ be the products of the determinants corresponding to the columns of $T$, i.e.,
\[D_T=\prod_{j=1}^{l}D_{T_{1,j},\dots,T_{\mu_j,j}}\]
where $\mu=\tilde{\lambda}$.
\begin{lemma}[\textbf{Sylvester}]
For any $p\times p$ matrices $M$ and $N$, we have
\[\det(M)\cdot\det(N)=\sum\det(M')\det(N')\]
where the sum is over all pairs $(M',N')$ of matrices obtained by interchanging a fixed set of $k$ columns of $N$ with any $k$ columns of $M$, preserving the relative ordering of columns. 
\end{lemma}
\begin{proof}
By the alternating property of determinants, we may assume that the columns interchanged in $N$ are the first $k$ columns. For vectors $v_1,\dots,v_p$ in $\C^p$, we write $|v_1,\dots,v_p|$ for the determinant of the matrix with these vectors as columns. Then we must prove that
\begin{align*}
|v_1,\dots,v_p|\cdot|w_1,\dots,w_p|=\sum_{i_1<\cdots<i_k}|v_1,\dots,w_1,\dots,w_k,\dots,v_p|\cdot|v_{i_1},\dots,v_{i_k},w_{k+1},\dots,w_p|.
\end{align*}
It suffices to show the difference of the two sides is an alternating function on the $p+1$ vectors $v_1,\dots,v_p,w_1$, since any such function must vanish. For this it suffices to prove that the two sides are equal when two successive vectors $v_i$ and $v_{i+1}$ are equal (which is immediate), and when $w_1=v_p$. In the later case, it suffices to show the following function is alternating on $v_1,\dots,v_p,w_2$:
\[|v_1,\dots,v_p|\cdot|v_p,w_2,\dots,w_p|-\sum_{i_1<\cdots<i_k}|v_1,\dots,w_1,\dots,w_k,\dots,v_p|\cdot|v_{i_1},\dots,v_{i_k},w_{k+1},\dots,w_p|\]
where the vectors $v_p,w_2,\dots,w_k$ are interchanged with the vectors $v_{i_1},\dots,v_{i_k}$. Again the case $v_i=v_{i+1}$ is immediate, and this time the case $w_2=v_p$ is also immediate.
\end{proof}
With the above lemma, we can prove the following important result.
\begin{proposition}
If $V$ is free with basis $\{v_1,\dots,v_m\}$, then there is a canonical homomorphism from $V^\lambda$ to $\C[Z]$ by mapping $v_T$ to $D_T$.
\end{proposition}
\begin{proof}
It suffices to prove that $D_T$ satisfies the conditions in Lemma~\ref{Weyl module char by conditions}. The first two follow from the property of determinant, and property (c) follows from Sylvester's lemma. For this, suppose the two columns of $T$ in which the exchange takes place has entries $i_1,\dots,i_p$ and $j_1,\dots,j_q$ (where $p\geq q$). Set
\[M=\begin{bmatrix}
Z_{1,i_1}&Z_{1,i_2}&\cdots&Z_{1,i_p}\\
Z_{2,i_1}&Z_{2,i_2}&\cdots&Z_{2,i_p}\\
\vdots&\vdots&\ddots&\vdots\\
Z_{p,i_1}&Z_{p,i_2}&\cdots&Z_{p,i_p}\\
\end{bmatrix}\quad N=\left[
\begin{BMAT}(@,20pt,30pt){c:l}{c}
\begin{matrix}
Z_{1,j_1}&Z_{1,j_2}&\cdots&Z_{1,j_q}\\
Z_{2,j_1}&Z_{2,j_2}&\cdots&Z_{2,j_q}\\
\vdots&\vdots&\ddots&\vdots\\
Z_{p,j_1}&Z_{p,j_2}&\cdots&Z_{p,j_q}\\
\end{matrix}&\begin{BMAT}(@,20pt,25pt){c}{cc}
0\\
I_{p-q}
\end{BMAT}
\end{BMAT}
\right]\]
Then the desired property follows by applying the Sylvester's lemma to these two matrices.
\end{proof}
\begin{theorem}
If $V$ is free with basis $\{v_1,\dots,v_m\}$, then $V^\lambda$ is free with basis $v_T$, as $T$ varies over the tableaux on $\lambda$ with entries in $\{1,\dots,m\}$.
\end{theorem}
\begin{proof}
The proof that $V^\lambda$ is generated by the $v_T$'s when $T$ varies over the tableaux on $\lambda$ with entries in $\{1,\dots,m\}$ is the same for that of $S^\lambda$, so we may omit it.\par
To show that $v_T$ are lineraly independent, we use the canonical map from $V^\lambda$ to $\C[Z]$. Thus we may prove that $D_T$ are lineraly independent. For this, we frist give the variables $\{Z_{ij}\}$ the lexicographical order and order the monomials lexicographically by saying $f_1<f_2$ if the smallest $Z_{i,j}$ that occurs to a different power occurs to a smaller power in $f_1$ than in $f_2$. Note that if $f_1\leq f_2$ and $g_1\leq g_2$ then $f_1g_1\leq f_2g_2$. It the follows from this definition that the smallest monomial that appears in a determinant $D_{i_1,\dots,i_p}$, if $i_1<\cdots<i_p$, is the diagonal term $Z_{1,i_1}Z_{2,i_2}\cdots Z_{p,i_p}$. Therefore the smallest term in $D_T$, if $T$ has increasing terms, is $\prod_{ij}Z_{i,j}^{m_T(i,j)}$, where $m_T(i,j)$ is the number of times $j$ occurs in the $i$-th row of $T$. This monomial occurs with coefficient $1$.\par
Now we order the tableaux by saying that $T<T'$ if in the first row where they differ, the first entry where they differ in this row, is smaller in $T$ than $T'$. Equivalently, the smallest $i$ for which there is a $j$ with $m_T(i,j)\neq m_{T'}(i,j)$, and the smallest such $j$, has $m_T(i,j)<m_{T'}(i,j)$. It follows that if $T<T'$ then the smallest monomial in $D_T$ is smaller than that in $D_{T'}$. From this the linearly independence of $D_T$ follows: if $\sum r_TD_T=0$ then take $T$ minimal with $r_T\neq 0$, we see the coefficient of $\prod_{ij}Z_{i,j}^{m_T(i,j)}$ in $\sum r_TD_T$ is $r_T$, contradiction.
\end{proof}
\begin{corollary}
The map from $V^\lambda$ to $\C[Z]$ is injective and its image $D^\lambda$ is free on the polynomials $D_T$, as $T$ varies over the tableaux on $\lambda$ with entries in $\{1,\dots,m\}$.
\end{corollary}
Finally, since the general linear group $\GL(V)$ acts on $V^\lambda$, we may want to have an action of $\GL(V)$ on $R[Z]$ such that the induced $\GL(V)$-module structure on $D^\lambda$ is isomorphic to that on $V^\lambda$. For this, we first consider the action of $g\in\GL(V)$ on $v_T$.
\begin{lemma}
If $g=(g_{ij})\in\GL_m(\C)$ and $T$ has entries $j_1,\dots,j_n$ in its $n$ boxes, then
\[g\cdot v_T=\sum g_{i_1,j_1}\cdots g_{i_n,j_n}v_{T'}\]
where the sum is over the all $p$-tuples $(i_1,\dots,i_p)$ from $\{1,\dots,m\}$.
\end{lemma}
\begin{proof}
This follows directly from the definition of the action of $\GL(V)$ on $V^{\otimes n}$. If fact,
\[g\cdot e_{j_1}\otimes\cdots\otimes e_{j_n}=\sum_{i_1,\dots,i_n} g_{i_1,j_1}e_{i_1}\otimes\cdots\otimes g_{i_n,j_n}e_{i_n}\]
from which the claim follows.
\end{proof}
Now for $g\in\GL_m(\C)$, define the action of $g$ on $\C[Z]$ by
\[(g\cdot f)(A)=f(A\cdot g)\]
where $A\in\mathcal{M}_{n\times m}(\C)$ and we view $\C[Z]$ a function ring over $\mathcal{M}_{n\times m}(\C)$. For example, if $g=(g_{ij})$ and $A=(a_{ij})$, then
\[(g\cdot Z_{ij})(A)=Z_{ij}(A\cdot g)=\sum_kA_{ik}g_{kj}\]
which can also be written as
\[g\cdot Z_{ij}=\sum_kZ_{ik}g_{kj}.\]
\begin{lemma}
For a $p$-tuple $(j_1,\dots,j_p)$, we have
\[g\cdot D_{j_1,\dots,j_p}=\sum g_{i_1,j_1}\cdots g_{i_n,j_n}D_{i_1,\dots,i_p}\]
where the sum is over the all $p$-tuples $(i_1,\dots,i_p)$ from $\{1,\dots,m\}$.
\end{lemma}
\begin{proof}
Let $A=(a_{ij})\in \mathcal{M}_{n\times m}(\C)$, then the action of $g$ on $D_{i_1,\dots,i_p}$ is defined to be
\begin{align*}
(g\cdot D_{j_1,\dots,j_p})(A)&=\sum_{\sigma\in S_p}\sgn(\sigma)Z_{\sigma(1),j_1}(A\cdot g)\cdots Z_{\sigma(p),j_p}(A\cdot g)\\
&=\sum_{\sigma\in S_p}\sgn(\sigma)(g\cdot Z_{\sigma(1),j_1})(A)\cdots(g\cdot Z_{\sigma(p),j_p})(A)
\end{align*}
With this, we can then write
\begin{align*}
g\cdot D_{j_1,\dots,j_p}&=\sum_{\sigma\in S_p}\sum_{i_1,\dots,i_p}\sgn(\sigma)Z_{\sigma(1),i_1}g_{i_1,j_1}\cdots Z_{\sigma(p),i_p}g_{i_p,j_1}\\
&=\sum g_{i_1,j_1}\cdots g_{i_n,j_n}D_{i_1,\dots,i_p}
\end{align*}
which proves the claim.
\end{proof}
\subsection{Representation of \boldmath\texorpdfstring{$\GL_m(\C)$}{G}}
A vector $v$ in a polynomial representation $V$ of $\GL_m(\C)$ is called a \textbf{weight vector} with weight $\alpha$, if for any $x=\mathrm{diag}(x_1,\dots,x_m)$, we have
\[x\cdot v=\alpha(x)v.\]
Since $H$ is abelian and we work over $\C$, any representation $V$ can be decomposed into
\[V=\bigoplus_\alpha V_\alpha\]
where $V_\alpha$ is a weight space. Moreover, since $H$ is a multiplication group, for each weight $\alpha$ and $x,y\in H$, $v\in V$, we have
\[\alpha(xy)=(xy)\cdot v=x\cdot(\alpha(y)v)=\alpha(x)\alpha(y)v.\]
This shows $\alpha$ is a homomorphism from $H$ into $\C$ (in fact into $\C^*$). 
\begin{lemma}
Any polynomial homomorphism $\alpha$ from $\C^{*}$ to itself must be of the form $z\mapsto z^n$.
\end{lemma}
\begin{proof}
Just compare the coefficients of $\alpha(z^m)$ and $\alpha(z)^m$.
\end{proof}
Since any weight for $\GL_m(\C)$ is polynomial, it can be written as $\alpha=(\alpha_1,\dots,\alpha_m)$, where $\alpha(x)=x_1^{\alpha_1}\cdots x_m^{\alpha_m}$. Let $B\sub\GL_m(\C)$ be the Borel subgroup of all upper triangular matrices. A vector $v\in V$ is called a \textbf{highest weight vector} if $B\cdot v=\C^*v$.\par
Now let $V$ be a vector space and consider the representation $V^\lambda$ of $\GL_m(\C)$.
\begin{proposition}
If $T$ is a $\lambda$-tableau of shape $\mu=(\mu_1,\dots,\mu_m)$, then the weight of $v_T$ is $\mu$.
\end{proposition}
\begin{proof}
This follows from the definition of $v_T$.
\end{proof}
Since when $T$ varies over tableaux over $\lambda$ with entries in $\{1,\dots,m\}$, the elements $v_T$ form a a basis for $V^\lambda$, we get the following corollary.
\begin{corollary}
The weight decomposition of $V^\lambda$ is given by
\[V^\lambda=\bigoplus_{\mu\preceq\lambda}V_\mu^\lambda\]
where $V_\mu^\lambda$ is the $\mu$-weight space spanned by $e_T$ with $T$ varies over $\lambda$-tableau with type $\mu$.
\end{corollary}
\begin{remark}
Recall that for two roots $\lambda$ and $\mu$ we say $\lambda\preceq\mu$ if
\[\mu-\lambda=k_1\beta^1+k_2\beta^2+\cdots+k_{m-1}\beta^{m-1}\]
where $\beta^i$ are roots and $k_i$ positive integers. If we take
\[\beta^1=(1,-1,0,\dots,0),\quad \beta^2=(0,1,-1,0,\dots,0),\quad \beta^{m-1}=(0,\dots,0,1,-1)\]
then we see for $\lambda=(\lambda_1,\dots,\lambda_m)$ and $\mu=(\mu_1,\dots,\mu_m)$, $\mu\preceq\lambda$ if and only if
\[\lambda_1+\cdots+\lambda_i\leq\mu_1+\cdots+\mu_i\quad\forall 1\leq i\leq m.\]
This coincides with our order on compositions.
\end{remark}
\begin{lemma}
Let $g=(g_{ij})\in B$ and $T$ be a tableau on $\lambda$, so that
\[g\cdot v_T=\sum g_{i_1,j_1}\cdots g_{i_n,j_n}v_{T'}.\]
Then the type of any tableaux $T'$ on the right is bigger than $T$. 
\end{lemma}
\begin{proof}
Since $g\in B$ we have $g_{ij}=0$ when $i>j$. Therefore every entry in $T'$ is not bigger than the corresponding entry in $T$. From this and the definition of type, the claim is now clear.
\end{proof}
\begin{proposition}
The only highest weight vector in $V^\lambda$ is the vector $v_T$, where $T=U_\lambda$ is the tableau whose $i$-th row contains only the integer $i$. 
\end{proposition}
\begin{proof}
We use the formula $g\cdot v_T=\sum g_{i_1,j_1}\cdots g_{i_n,j_n}v_{T'}$. It follows immediate that if $g_{ij}=0$ when $i>j$, then every entry in $T'$ is not smaller than the corresponding one in $T$ (but their elements are both contained in $\{1,\dots,m\}$). If $T=U(\lambda)$ then we see such tableau can not be $U(\lambda)$, whence $g\cdot v_T=v_T$. On the other hand, assume that $T\neq U(\lambda)$, and the $p$-th row is the first row that contains an entry $q$ bigger than $p$. Then define $g$ in $B$ to be the elementray matrix with $g_{ij}=1$ if $i=j$ or if $i=p,j=q$, and $g_{ij}=0$ otherwise. Then we see
\[g\cdot v_T=\sum v_{T'}\]
where the sum is over all fillings $T'$ obtained from $T$ by exchanging some set (possibly empty) of the $q$'s appearing in $T$ to $p$'s. In particular, if $T'$ is the tableau obtained from $T$ by changing all $q$'s in the $p$-th row to $p$'s, then $v_{T'}$ occurs in $g\cdot v_T$ with coefficient $1$, which means $v_T$ is not a highest weight vector.
\end{proof}
\begin{remark}
Note that the vector $v_T$ with $T=U(\lambda)$ has weight $\lambda=(\lambda_1,\dots,\lambda_k,0,\dots,0)$. In fact, if $T$ is a $\lambda$-tableau of shape $\mu=(\mu_1,\dots,\mu_m)$, then the weight of $v_T$ is exactly $\mu$. Since we have $\mu\succeq\lambda$, we see the highest weight of $V^\lambda$ must be $\lambda$, with weight vector $v_{U(\lambda)}$.
\end{remark}
\begin{proposition}
The highest weight vector $v_T$ in $V^\lambda$ generates $V^\lambda$.
\end{proposition}
\begin{proof}

\end{proof}
\begin{proposition}
The character of $V^\lambda$ is given by
\[\chi_{V^\lambda}(x)=s_\lambda(x_1,\dots,x_m).\]
\end{proposition}
\begin{proof}
This follows from the remark above.
\end{proof}
\begin{proposition}
If $\lambda$ is a partition with at most $m$ rows, then the representation $V^\lambda$ is irreducible with highest weight $\lambda=(\lambda_1,\dots,\lambda_m)$. These are all the irreducible polynomial representations of $\GL_m(\C)$.
\end{proposition}
\begin{proof}
Let $W\sub V^\lambda$ be a subrepresentation. Then by passing to differential, we see $W\sub V^\lambda$ are representations of $\gl_m(\C)$. Also, the highest weight vector $v_\lambda$ is a highest weight vector for $\gl_m(\C)$ (hence for $\sl_m(\C)$), with weight $\lambda$. Since $V^\lambda$ has a unique such vector, it follows that it is irreducible.\par
Now, note that the character $\chi_U$ of a polynomial representation of $\GL_m(\C)$ is symmetric because the matrix $\mathrm{diag}(x_1,\dots,x_m)$ and $\mathrm{diag}(x_{\sigma(1)},\dots,x_{\sigma(m)})$ are conjugate. Since $s_\lambda$ is a basis for the symmetric functions, it follows that the character of $\chi_U$ can be written into sums of that of the $V^\lambda$'s. On the other hand, the character of a polynomial representation of $\GL_m(\C)$ determines the action of diagonalizable matrices in $\GL_m(\C)$ and hence the action of $\GL_m(\C)$ (they are dense in $\GL_m(\C)$).
\end{proof}
\begin{example}
Let $V=\C^2$ with basis $\{v_1,v_2\}$ and consider $\lambda=(3)$. Then
\[V^\lambda=\mathrm{span}\{v_1^3,v_1^2v_2,v_1v_2^2,v_3^2\}.\]
For the weight space decomposition of $V^\lambda$, we have
\[V^\lambda=V_{(3,0)}\oplus V_{(2,1)}\oplus V_{(1,2)}\oplus V_{(0,3)}\]
where $(3,0)$ is the highest weight.
\end{example}
\begin{proposition}
As $\GL_m(\C)$ representations, we have
\begin{itemize}
\item[(a)] $\mathrm{Sym}^{\lambda_1}V\otimes\cdots \mathrm{Sym}^{\lambda_k}V=\bigoplus_\mu(V^\mu)^{K_{\mu\lambda}}$.
\item[(b)] $\bigwedge^{\lambda_1}V\otimes\cdots\bigwedge^{\lambda_k}V=\bigoplus_\mu(V^\mu)^{K_{\tilde{\mu}\lambda}}$.
\item[(c)]  $V^\lambda\otimes V^\mu=\bigoplus(V^\nu)^{c_{\lambda,\mu}^{\nu}}$
\end{itemize}
\end{proposition}
For each $m$, let $G(m)$ be the grothendieck group of polynomial representations of $\GL_m(\C)$. Then we can define a production on $G=\bigoplus_mG(m)$ by $[V]\cdot[W]=[V\otimes W]$. The mapping that to a representation $M$ of $S_n$ assigns the representation $V^{\otimes n}\otimes_{\C[S_n]}M$ defines a additive homomorphism from $R_n$ to $G_m$, and hence a homomorphism from $R$ to $G(m)$. The character map $\chi$ determines a homomorphism from $G(m)$ to the ring $\Lambda(m)$ of symmetric polynomials in the variables $x_1,\dots,x_m$, which is an injection since a polynomial representation is determined by its character. We therefore have maps
\[\begin{tikzcd}
\Lambda\ar[r]&R\ar[r]&G\ar[r]&\Lambda(m)
\end{tikzcd}\]