\chapter{Lie Algebras and Representations}
\section{Root decomposition}
\subsection{Cartan subalgebras}
For a better understanding of the structure of a Lie algebra, one decomposes it into simultaneous eigenspaces of linear maps of the form $\ad(x)$. Cartan subalgebras $\h\leq\g$ provide maximal subsets of $\g$, for which there exist simultaneous generalized eigenspaces for all operators $\ad(x)$, $x\in\h$, whenever $\g$ is a complex Lie algebra.
\subsubsection{Weight and root decompositions}
Root decompositions are the simultaneous eigenspace decompositions of the
type mentioned above. They are special cases of weight decompositions.
\begin{definition}
Let $(V,\rho)$ be a representation of the Lie algebra $\h$. For a function $\lambda:\h\to\K$, we define the corresponding \textbf{weight space} and the corresponding \textbf{generalized weight space} by
\[V_{\lambda}(\h)=\bigcap_{x\in\h}V_{\lambda(x)}(\rho(x)),\quad V^{\lambda}(\h)=\bigcap_{x\in\h}V^{\lambda(x)}(\rho(x))\]
Any function $\lambda:\h\to\K$ for which $V_{\lambda}(\h)\neq\{0\}$ is called a \textbf{weight} of the representation $(V,\rho)$. We write $\mathcal{P}_{\h}(V)$ for the set of weights of $(V,\rho)$.
\end{definition}
\begin{lemma}\label{Lie nilpotent subalgebra decomposition of representation}
Let $(V,\rho)$ be a finite-dimensional representation of the nilpotent Lie algebra $\h$ such that for every $x\in\h$, $\rho(x)$ is split. Then each weight is linear and $V$ decomposes as
\[V=\bigoplus_{\lambda\in\h^*}V^\lambda(\h)\]
Moreover, each generalized weight space $V^{\lambda}(\h)$ is $\h$-invariant.
\end{lemma}
\begin{proof}
Since the assertion of the lemma only refers to the Lie algebra $\rho(\h)$, we may replace $\h$ by $\rho(\h)$ and assume that $\h\sub\gl(V)$ is a nilpotent subalgebra consisting of split endomorphisms.\par
For each $x\in\h$, we have $\ad(x)(\h)\sub\h$, so that we also get $(\ad(x))_s(\h)\sub\h$ by Proposition~\ref{Jordan decomposition prop}. Since $\ad(x)|_{\h}$ is nilpotent, Corollary~\ref{Jordan decomposition of ad} and Proposition~\ref{Jordan decomposition prop} show that
\[0=(\ad(x)|_{\h})_s=(\ad(x))_s|_{\h}=\ad(x_s)|_{\h}.\]
It follows that $[x_s,\h]=\{0\}$. In view of Theorem~\ref{Jordan decomposition}(d), we further have $[x_s,y_s]=0$ for $x,y\in\h$, so that $\h_s:=\{x_s:x\in\h\}$ is a commutative set of diagonalizable endomorphisms, hence simultaneously diagonalizable. Let $\lambda:\h\to\K$ be a weight of $\h$. We define a map $\tilde{\lambda}:\h_s\to\K$ by $\tilde{\lambda}(x_s)=\lambda(x)$, so that
\[V_{\tilde{\lambda}}(\h_s)=\bigcap_{x\in\h}V_{\tilde{\lambda}(x_s)}(x_s)=\bigcap_{x\in\h}V^{\lambda(x)}(x)=V^{\lambda}(\h)\]
Since $V_{\tilde{\lambda}}(\h_s)$ is the simultaneous eigenspace of $\h_s$, we have a decomposition
\[V=\bigoplus_{\lambda\in\mathcal{P}_{\h}(V)}V_{\tilde{\lambda}}(\h_s).\]
In view of $[\h,\h_s]=\{0\}$, each sapce $V_{\tilde{\lambda}}(\h_s)$ is $\h$-invariant. Let $\rho_{\lambda}$ denote the representation of $\h$ on this subspace. For any $x\in\h$, we then have $\rho_{\tilde{\lambda}}(x)_s=\tilde{\lambda}(x)\id$. Set $n:=\dim V_{\tilde{\lambda}}(\h_s)$, we now see that
\[\lambda(x)=\tilde{\lambda}(x_s)=\frac{1}{n}\tr(\rho_{\tilde{\lambda}}(x))\]
is linear. This completes the proof.
\end{proof}
\begin{definition}
If $\h$ is a nilpotent subalgebra of the Lie algebra $\g$, then the weights of the representation $\rho=\ad|_{\h}$ which are different from zero are called \textbf{roots of $\g$ with respect to $\h$}. The set of all roots is denoted $\Phi(\g,\h)$. The generalized weight spaces $\g^{\lambda}(\h)$ are called \textbf{root spaces}.
\end{definition}
Sometimes we write $\g^{\lambda}$ instead of $\g^{\lambda(\h)}$. If $0\neq\mu\in\h^*$ is not a root, then we put $\g^{\mu}=\{0\}$.
\begin{proposition}\label{Lie algebra root space prop}
Let $\g$ be a finite-dimensional Lie algebra and $\h$ a nilpotent subalgebra of $\g$.
\begin{itemize}
\item[(a)] $[\g^{\lambda},\g^{\mu}]\sub\g^{\lambda+\mu}$ for any $\lambda,\mu\in\h^*$.
\item[(b)] $\g^0$ is a subalgebra of $\g$.
\end{itemize}
\end{proposition}
\begin{proof}
We only need to prove (a). For $x\in\g^{\lambda},y\in\g^{\mu}$ and $h\in\h$, we have
\[(\ad(h)-\lambda(h)-\mu(h))^n([x,y])=\sum_{i=0}^{n}\binom{n}{i}[(\ad(h)-\lambda(h))^ix,(\ad(h)-\mu(h))^{n-i}y].\]
If $n$ is sufficiently large, then for every summand either the left factor or the right factor in the bracket vanishes, so that the whole sum vanishes. This proves that $[x,y]\in\g^{\lambda+\mu}$.
\end{proof}
\subsubsection{Cartan subalgebras}
In the following, we want to decompose a Lie algebra into root spaces with respect to a nilpotent subalgebra $\h$. Since we want such decompositions to be as fine as possible, $\g^0(\h)$ should be as small as possible, hence equal to $\h$. The following result prepares the definition of a Cartan subalgebra.
\begin{proposition}\label{Lie subalgebra Cartan iff}
For a subalgebra $\h$ of a finite-dimensional Lie algebra $\g$, the following are equivalent:
\begin{itemize}
\item[(\rmnum{1})] $\h$ is nilpotent and self-normalizing, i.e., $\h=\n_\g(\h)$.
\item[(\rmnum{2})] $\h=\g^0(\h)$.
\end{itemize}
If these conditions are satisfied, then $\h$ is a maximal nilpotent subalgebra of $\g$.
\end{proposition}
\begin{proof}
If $\h$ is nilpotent, then we have $\h\sub\g^0(\h)$. Assume that $\g^0(\h)$ strictly contains $\h$ and consider the representation
\[\rho:\h\to\gl(\g^0(\h)/\h),\quad\rho(h)(x+\h)=[h,x]+\h\]
whose image consists of nilpotent endomorphisms. By Theorem~\ref{Engel's theorem linear form}, there exists some $x\in\g^0(\h)\setminus\h$ with $\ad(h)x\in\h$ for all $h\in\h$. But this contradicts $\n_{\g}(\h)=\h$. This prves (\rmnum{1}) $\Rightarrow$ (\rmnum{2}).\par
For (\rmnum{2}) $\Rightarrow$ (\rmnum{1}), from $\h\sub\g^0(\h)$ we derive with Engel's Theorem that $\h$ is nilpotent. To see that $\h$ is self-normalizing, let $x\in\n_{\g}(\h)$. Then $\ad(h)x\in\h$ for all $h\in\h$, and therefore $\ad(h)^nx=0$ for sufficiently large $n$. Hence $x\in\g^0(\h)=\h$.\par
If (\rmnum{1}) and (\rmnum{2}) are satisfied and $\n\sups\h$ is a nilpotent subalgebra, then $\n\sub\g^0(\h)=\h$. Therefore, $\h$ is maximally nilpotent.
\end{proof}
\begin{definition}
Let $\g$ be a finite-dimensional Lie algebra. A nilpotent subalgebra $\h$ is called a \textbf{Cartan subalgebra} of $\g$ if it is self-normalizing, i.e., $\n_{\g}(\h)=\h$.
\end{definition}
\begin{example}
\mbox{}
\begin{itemize}
\item[(\rmnum{1})] If $\g$ is nilpotent, then $\g$ is the only Cartan subalgebra of $\g$ because any Cartan subalgebra is maximally nilpotent.
\item[(\rmnum{2})] If $\g_i$ are Lie algebras with Cartan subalgebras $\h_i$, then $\bigoplus_i\h_i$ is a Cartan subalgebra of $\bigoplus_i\g_i$.
\item[(\rmnum{3})] Let $\g=\R h+\R p+\R q+\R z$ be the oscillator algebra. Then $\h=\R h+\R z$ is a Cartan subalgebra of $\g$.
\item[(\rmnum{4})] In $\g=\gl_n(\K)$, the Lie subalgebra $\h$ of the diagonal matrices is a Cartan subalgebra.
\item[(\rmnum{5})] Every one-dimensional subspace of $\g=\so(3)$ is a Cartan subalgebra.
\item[(\rmnum{6})] Every Cartan subalgebra is maximally nilpotent, but not every maximally nilpotent subalgebra is a Cartan subalgebra. The subalgebra
\[
\n:=\R\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\leq\g=\sl_2(\R)\]
is maximally nilpotent and not self-normalizing. Its normalizer is the only proper subalgebra
\[\b=\R\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}+\R\begin{pmatrix}
1&0\\
0&-1
\end{pmatrix}\]
containing $\n$. It is solvable but not nilpotent.
\end{itemize}
\end{example}
\begin{definition}
Let $\g$ be a finite-dimensional Lie algebra and $\h\leq\g$ be a Cartan subalgebra. We call $\h$ a \textbf{splitting Cartan subalgebra} if $\ad(x)$ splits for each $x\in\h$. If in addition $\ad(x)$ is diagonalizable for each $x\in\h$, we call $\h$ a \textbf{toral Cartan subalgebra}. In this case, $\g^\lambda=\g_{\lambda}$ for each $\lambda\in\h^*$, and in particular $\h=\g^0$ implies that $\h$ is abelian.
\end{definition}
Note that if $\h$ is a splitting Cartan subalgebra, then by Lemma~\ref{Lie nilpotent subalgebra decomposition of representation} we have the \textbf{root space decomposition}:
\[\g=\h\oplus\bigoplus_{\lambda\in\Phi}\g^{\lambda}.\]
\begin{proposition}\label{Lie Cartan subalgebra prop}
\mbox{}
\begin{itemize}
\item[(a)] A subalgebra $\h$ of the real Lie algebra $\g$ is a Cartan subalgebra if and only if $\h_{\C}$ is a Cartan subalgebra of $\g_{\C}$.
\item[(b)] Let $\varphi:\g\to\tilde{\g}$ be a surjective homomorphism and $\h\leq\g$ a Cartan subalgebra. Then $\varphi(\h)$ is a Cartan subalgebra of $\tilde{\g}$.
\item[(c)] If $\h$ is a Cartan subalgebra of $\g$ contained in the subalgebra $\k$, then $\h$ also is a Cartan subalgebra of $\k$.
\end{itemize}
\end{proposition}
\begin{proof}
Part (a) follows from $\n_{\g}(\h)_{\C}=\n_{\g_{\C}}(\h_{\C})$ and Lemma~\ref{Lie algebra nilpotent prop}.\par
By (a), we may assume that $\K=\C$. Proposition~\ref{Lie algebra nilpotent prop} shows that $\tilde{\h}:=\varphi(\h)$ is nilpotent. By Proposition~\ref{Lie subalgebra Cartan iff}, it suffices to show $\tilde{\g}^0(\tilde{\h})=\tilde{\h}$. Clearly, $\varphi(\g^0(\h))=\varphi(\h)=\tilde{\h}$. Moreover, Lemma~\ref{Lie nilpotent subalgebra decomposition of representation} yields decompositions
\[\g=\g^0(\h)\oplus\bigoplus_{0\neq\lambda\in\h^*}\g^{\lambda}(\h),\quad\tilde{\g}=\tilde{\g}^0(\tilde{\h})\bigoplus_{0\neq\tilde{\lambda}\in\tilde{\h}^*}\tilde{\g}^{\tilde{\lambda}}(\tilde{\h}).\]
Now we claim that for each root $\lambda\neq 0$,
\begin{align}\label{Lie Cartan subalgebra prop-1}
\varphi(\g^{\lambda}(\h))\sub\bigoplus_{0\neq\tilde{\lambda}\in\tilde{\h}^*}\tilde{\g}^{\tilde{\lambda}}(\tilde{\h}).
\end{align}
To see this, for each $0\neq\lambda\in\h^*$ with $\g^{\lambda}(\h)\nsubseteq\ker\varphi$, we define $\tilde{\lambda}\in\h^*$ by $\tilde{\lambda}(\varphi(h))=\lambda(h)$. Choose $x\in\g^{\lambda}(\h)$ with $\varphi(x)\neq 0$. Since $x\in\g^{\lambda}(\h)$, there exists $n\in\N$ such that
\[(\ad(h)-\lambda(h))^nx=0.\]
Applying $\varphi$ on the both sides then gives
\begin{align}\label{Lie Cartan subalgebra prop-2}
(\ad(\varphi(h))-\lambda(h))\varphi(x)=0.
\end{align}
If $\varphi(h)=0$, since $\varphi(x)\neq 0$, this then implies $\lambda(h)=0$. Therefore the map $\tilde{\lambda}$ is well-defined. By $(\ref{Lie Cartan subalgebra prop-2})$ and the definition of $\tilde{\lambda}$ we also see that
\[\varphi(\g^{\lambda}(\h))\sub\tilde{\g}^{\tilde{\lambda}}(\tilde{\h}).\]
Therefore $(\ref{Lie Cartan subalgebra prop-1})$ is proved. Since $\varphi$ is surjective, it follows that $\tilde{\h}=\varphi(\h)=\tilde{\g}^0(\tilde{\h})$.\par
For (c), since $\n_{\g}(\h)=\h$, we also have $\n_{\k}(\h)=\h$, so $\h$ is also a Cartan subalgebra of $\k$.
\end{proof}
\begin{proposition}
Let $\a\sub\g$ be an abelian subalgebra for which all operators $\ad(x)$, $x\in\a$, are semisimple. Then the Cartan subalgebras of the centralizer $\z_{\g}(\a)$ are precisely the Cartan subalgebras of $\g$ containing $\a$. In particular, such Cartan subalgebras exist.
\end{proposition}
\begin{proof}
Let $\c:=\z_{\g}(\a)$ and fix a Cartan subalgebra $\h$ of $\c$. Since $\a$ is central in $\c$, we find $\a\sub\z(\c)\sub\n_{\c}(\h)=\h$. For the normalizer $\n:=\n_{\g}(\h)$, we have
\[[\a,\n]\sub[\h,\n]\sub\h\sub\n.\]
Since each $\ad_{\g}(x)$, $x\in\a$, is semisimple, Lemma~\ref{Lie semisimple endo semisimple module} shows that $\n$ is a semisimple $\a$-module, so that there exists an $\a$-invariant subspace $\l\sub\n$ with $\n=\h\oplus\l$. Then
\[[\a,\l]\sub[\a,\n]\cap\l\sub\h\cap\l=\{0\}\]
implies $\l\sub\c$. Therefore, $\n$ is contained in $\c$, and since $\h$ is a Cartan subalgebra of $\c$, we get $\n=\h$, showing that $\h$ is a Cartan subalgebra of $\g$.\par
If, conversely, $\h$ is a Cartan subalgebra of $\g$ containing $\a$, then
\[\h\sub\g^0(\h)\sub\g^0(\a)=\g_0(\a)=\c.\]
Since $\h$ is self-normalizing in $\g$, it is also self-normalizing in $\c$, hence a Cartan subalgebra of $\c$.
\end{proof}
\subsubsection{Cartan subalgebras and regular elements}
Next we want to ensure that Cartan subalgebras actually exist. To this end, for every $x\in\g$, we consider the generalized eigenspace $\g^0(\ad(x))$. This space is always different from zero since it obviously contains $x$. We make the following definition.
\begin{definition}
The number
\begin{align}\label{Lie algebra regular element def}
\rank(\g)=\min\{\dim\g^0(\ad(x)):x\in\g\}
\end{align}
is called the \textbf{rank of $\g$}. An element $x\in\g$ is called \textbf{regular} if $\dim\g^0(\ad(x))=\rank(\g)$. We write $\mathrm{reg}(\g)$ for the set of regular elements in $\g$.
\end{definition}
Let $V$ be a vector space over $\K$. A polynomial function on $V$ is a map $f:V\to\K$ such that, for one (hence every) choice of a basis for $V$, $f(v)$ is a polynomial in the coordinates of $v$. For example, for an endomorphism $\varphi$ of $V$, let
\[p_\varphi(t)=\det(tI-\varphi)=t^n+p_{n-1}(\varphi)t^{n-1}+\cdots+p_1(\varphi)t+p_0(\varphi).\]
Then $p_i(\varphi)=(-1)^{n-i}\tr(\wedge^{n-i}\varphi)$ is a polynomial function on $\End(V)$. Similarly, if $\g$ is a finite-dimension Lie algebra, then for $x\in\g$,
\[p_{\ad(x)}(t)=t^n+p_{n-1}(x)t^{n-1}+\cdots+p_1(x)t+p_0(x)\]
where $p_i(x)$ is a polynomial function on the vector space $\g$. Since $\dim\g^0(\ad(x))$ is the multiplicity of $0$ as a root of the characteristic polynomial of $\ad(x)$, we have
\[\rank(\g)=\min\{k\in\N:p_k\not\equiv 0\}.\]
and an element is regular if and only if $p_{r}(x)\neq 0$, where $r=\rank(\g)$.
\begin{example}
Let $\g=\gl(V)$ with $\dim V=n$, and let $x$ be a semisimple element of $\g$. If $\lambda_1,\dots,\lambda_n$ are eigenvalues of $x$ on $V$, then $\{\lambda_i-\lambda_j:1\leq i\leq j\leq n\}$ is the family of eigenvalues of $\ad(x)$ on $\g$, and so
\[p_{\ad(x)}(t):=\det(tI-\ad(x))=\prod_{i,j=1}^{n}(t-\lambda_i+\lambda_j).\]
It follows that the rank of $\gl(V)$ is $n$, and an element $x$ of $\gl(V)$ is regular if and only if is semisimple with distinct eigenvalue.
\end{example}
\begin{lemma}\label{Lie algebra regular element prop}
The set $\reg(\g)$ of regular elements has the following properties:
\begin{itemize}
\item[(a)] $\g\setminus\reg(\g)$ is the zero-set of a nonconstant polynomial.
\item[(b)] $\reg(\g)$ is open and dense. For $\K=\C$, it is connected.
\item[(c)] $\reg(\g)$ is invariant under the automorphism group $\Aut(\g)$ of $\g$.
\item[(d)] If $\K=\R$, then $\reg(\g)=\g\cap\reg(\g_{\C})$ and $\mathrm{rank}_{\R}(\g)=\mathrm{rank}_{\C}(\g_{\C})$.
\end{itemize}
\end{lemma}
\begin{proof}
Part (a) follows from the fact that
\[\reg(\g)=\{x\in\g:p_r(x)\neq 0\}\]
where $r=\rank(\g)$. Part (b) also follows from this observation.\par
For $\gamma\in\Aut(\g)$ we have
\[\ad(\gamma(x))(y)=[\gamma(x),y]=\gamma([x,\gamma^{-1}(y)])=\gamma\circ\ad(x)\circ\gamma^{-1}(y).\]
Therefore, $\ad(\gamma(x))$ and $\ad(x)$ have the same characteristic polynomial. We conclude that all polynomials $p_k$ are invariant under $\Aut(\g)$. In particular, $\reg(\g)$ is invariant.\par
Since $\det(M_{\C})=\det(M)$ for each $M\in\End(\g)$, the polynomials
$p_k^{\C}$ on the complexified Lie algebra $\g_{\C}$ satisfy $p_k^{\C}|_{\g}=p_k$ for each $k$. Hence $p_k$ vanishes if and only $p_k^{\C}$ does. In particular, $\mathrm{rank}_{\R}(\g)=\mathrm{rank}_{\C}(g_{\C})$, and (d) follows.
\end{proof}
The following result leads us to the relation of regular elements and Cartan subalgebras. It also shows that Cartan subalgebras do exist.
\begin{proposition}\label{Lie algebra regular element generate Cartan algebra}
If $x\in\g$ is a regular element, then $\g^0(\ad(x))$ is a Cartan subalgebra of $\g$. Its dimension is $\rank(\g)$.
\end{proposition}
\begin{proof}
We consider the following sets:
\[U_1=\{y\in\g^0(\ad(x)):\ad(y)|_{\g^0(\ad(x))}\text{ is not nilpotent}\}\]and
\[U_2=\{y\in\g^0(\ad(x)):\ad(y)|_{\g/\g^0(\ad(x))}\text{ is invertible}\}.\]
These sets are open in $\g^0(\ad(x))$ (given by the non vanishing of at least one coefficient of the characteristic polynomial of $\ad(y)$ for the first one and by the non vanishing of the determinant for the second one). Note that $U_2$ is nonempty because it contains $x$.\par
According to Engel's theorem, to show that $\g^0(\ad(x))$ is nilpotent, it suffices to show that $U_1$ is empty. If $U_1$ is nonempty, then both $U_1$ and $U_2$ are dense open sets of $\g^0(\ad(x))$, so there exists a $y\in U_1\cap U_2$. But for such a $y$ we have
\[\dim\g^0(\ad(y))<\dim\g^0(\ad(x)),\]
contradicting the regularity of $x$. Hence $\g^0(\ad(x))$ is nilpotent.\par
It remains to show that, $\g^0(\ad(x))$ equals its normalizer. If $z$ normalizes $\g^0(\ad(x))$, then $[z,x]\in\g^0(\ad(x))$, i.e., $(\ad(x))^n([z,x])=0$ for some $n\in\N$. But then $(\ad(x))^{n+1}z=0$, so $z\in\g^0(\ad(x))$.
\end{proof}
In fact, every Cartan subalgebra arises in this patten. To show this, we first consider regular elements in a Cartan subalgebra. We recall that for each inner derivation $\ad(x)$, the map $e^{\ad(x)}$ is an automorphism of $\g$. The elements of the group
\[\Inn(\g)=\langle e^{\ad(x)}:x\in\g\rangle\]
generated by these automorphisms are called \textbf{inner automorphisms}. We work on $\Inn(\g)$ because it has better functorial properties.
\begin{proposition}
Let $\varphi:\g\to\g'$ be an epimorphism. If $\sigma'\in\Inn(\g')$, then there exists a $\sigma\in\Inn(\g)$ such that the following diagram commutes:
\[
\begin{tikzcd}
\g\ar[r,"\phi"]\ar[d,swap,"\sigma"]&\g'\ar[d,"\sigma'"]\\
\g\ar[r,"\phi"]&\g'
\end{tikzcd}\]
\end{proposition}
\begin{proof}
It suffices to prove the case $\sigma'=e^{\ad(x')}$ with $x'\in\g'$. Choose $x\in\g$ so that $\varphi(x)=x'$, then we observe that
\[\phi\circ e^{\ad(x)}(z)=\phi(z+[x,z]+\cdots)=\phi(z)+[\phi(x),\phi(z)]+\cdots=e^{\ad(x')}\circ\phi(z).\]
Thus the claim follows.
\end{proof}
\begin{proposition}\label{Lie algebra regular elements in Cartan subalgebra}
Let $\h\sub\g$ be a splitting Cartan subalgebra and $\Phi:=\Phi(\g,\h)$. Then the following assertions hold
\begin{itemize}
\item[(a)] $\reg(\g)\cap\h=\h\setminus\bigcup_{\alpha\in\Phi}\ker\alpha$.
\item[(b)] $\rank(\g)=\dim\h$.
\end{itemize}
\end{proposition}
\begin{proof}
Since $\h$ is a splitting Cartan subalgebra, we have a decomposition
\begin{align}\label{Lie algebra regular elements in Cartan subalgebra-1}
\g=\h\oplus\bigoplus_{\alpha\in\Phi}\g^\alpha(\h).
\end{align}
Thus for each $h\in\h$, we have
\begin{align}\label{Lie algebra regular elements in Cartan subalgebra-2}
\g^0(\ad(h))=\h\oplus\bigoplus_{\alpha(h)=0}\g^{\alpha}(\h).
\end{align}
If $h\in\h$ is regular, then by Proposition~\ref{Lie algebra regular element generate Cartan algebra}, we know that $\g^0(\ad(h))$ is a Cartan subalgebra, and hence equals to $\h$. In view of $(\ref{Lie algebra regular elements in Cartan subalgebra-2})$, this implies $h\in\h\setminus\bigcup_{\alpha\in\Phi}\ker\alpha$.\par
Conversely, if $h\in\h\setminus\bigcup_{\alpha\in\Phi}\ker\alpha$, we need to prove that $h$ is regular. That is, $\g^0(\ad(h))$ has minimal dimension among $\{\g^0(\ad(x)):x\in\g\}$. From $(\ref{Lie algebra regular elements in Cartan subalgebra-2})$, we know that $\dim\g^0(\ad(h))=\dim\h$, so it suffices to prove that $\dim\h=\rank(\g)$. To this end, we only need to find a regular element in $\h$, by the argument in the first paragraph. We consider the map
\[\varPhi:\g\times\h\to\g,\quad (x,h)\mapsto e^{\ad(x)}h.\]
Then we observe that
\[d\varPhi_{(0,h)}(v,w)=\lim_{t\to 0}\frac{1}{t}\Big(\sum_{k=0}^{\infty}\frac{\ad(tv)^k(h+tw)}{k!}-h\Big)=[v,h]+w\]
for $v\in\g,w\in\h$. We therefore have
\[\im(d\varPhi_{(0,h)})=[h,\g]+\h.\]
For every $x\in\g^\alpha(\ad(h))$, there exists $n\in\N$ such that $(\ad(h)-\alpha(h))^nx=0$. If $\alpha(h)\neq 0$, this implies $x\in\ad(h)(\g)$. We thus conclude that
\begin{align}\label{Lie algebra regular elements in Cartan subalgebra-3}
\im(d\varPhi_{0,h})\sups\h\oplus\bigoplus_{\alpha(h)\neq 0}\g^\alpha(\h).
\end{align}
If $h\in\h\setminus\bigcup_{\alpha\in\Phi}\ker\alpha$, then $(\ref{Lie algebra regular elements in Cartan subalgebra-1})$ and $(\ref{Lie algebra regular elements in Cartan subalgebra-3})$ implies that $d\varPhi_{(0,h)}$ is surjective, and the Implicit Function Theorem implies that the image of $\varPhi$ is a neighborhood of $\varPhi(0,h)=h$. Since $\reg(\g)$ is a dense subset of $\g$, the image of $\varPhi$ then contains a regular element $x$ which we write as $x=\varPhi(a,b)=e^{\ad(a)}b$ for some $a\in\g$ and $b\in\h$. Then $b=e^{-\ad(a)}x$ is also regular (Lemma~\ref{Lie algebra exp(ad) is auto} and \ref{Lie algebra regular element prop}) and contained in $\h$. Thus $\h$ contains regular elements and the proof of (a) and (b) is finished.
\end{proof}
The following theorem clarifies the connection between Cartan subalgebras and regular elements.
\begin{theorem}\label{Lie algebra regular element and Cartan algebra}
Let $\g$ be a finite-dimensional Lie algebra.
\begin{itemize}
\item[(a)] For any regular element $x\in\g$, $\g^0(\ad(x))$ is a Cartan subalgebra of $\g$.
\item[(b)] Every Cartan subalgebra $\h$ contains regular elements and if $x\in\h$ is regular, then $\h=\g^0(\ad(h))$.
\item[(c)] All Cartan subalgebras have the same dimension $\rank(\g)$.
\item[(d)] For any Cartan subalgebra $\h$, the set $\Inn(\g)(\h\cap\reg(\g))$ is open in $\g$.
\end{itemize}
\end{theorem}
\begin{proof}
The case $\K=\C$ is clear. For $\K=\R$, we note that $\h\sub\g$ is a Cartan subalgebra if and only if Proposition~\ref{Lie Cartan subalgebra prop}, and the polynomial $p_r^{\C}:\g\to\C$ vanishes on $\g_{\C}$ if and only if it vanishes on $\g$. This proves the first three claims. For (d), we also consider the map
\[\varPhi:\g\times\h\to\g,\quad (x,h)\mapsto e^{\ad(x)}h\]
with
\[d\varPhi_{(0,x)}(v,w)=[x,v]+w.\]
If $x\in\h$ is regular, then $\h=\g^0(\ad(x))$, so that the induced map $\ad_{\g/\h}(x)(y+\h):=[x,y]+h$ is invertible because its kernel is trivial. This implies that $\g\sub\h+[x,\g]$, which means that $d\varPhi(0,x)$ is surjective, so that the Implicit Function Theorem implies that $\varPhi(\g\times(\reg(\g)\cap\h))$ is a neighborhood of $x=\varPhi(0,x)$. Since $\im\varPhi=\Inn(\reg(\g)\cap\h)$, this implie the later is open in $\g$.
\end{proof}
Let $\g$ be a finite-dimensional Lie algebra. Define an equivalence relation on the set $\reg(\g)$ of regular elements via
\[x\sim y\iff\text{there exists $\gamma\in\Inn(\g)$ such that $\gamma(\g^0(\ad(x)))=\g^0(\ad(y))$}.\]
\begin{lemma}
The equivalence classes of $\sim$ are open subsets of $\reg(\g)$.
\end{lemma}
\begin{proof}
Fix $x\in\reg(\g)$ and set $\h:=\g^0(\ad(x))$. In view of Theorem~\ref{Lie algebra regular element and Cartan algebra}, the set $\Inn(\g)(\reg(\g)\cap\h)$ is an open neighborhood of $x$. Each element in this set is of the form $\gamma(y)$ for $y\in\reg(\g)\cap\h$. Note that for every $n\in\N$ we have
\[\ad(\gamma(y))^n=\gamma\circ\ad(y)^n\circ\gamma^{-1}\]
and so 
\[\g^0(\ad(\gamma(y)))=\gamma(\g^0(\ad(y))).\]
This implies that $\gamma(y)\sim x$, so all equivalence classes of $\sim$ are open.
\end{proof}
\begin{proposition}\label{Lie algebra inn(g) acts transitive on Cartan}
If $\g$ is a finite-dimensional Lie algebra, then the group $\Inn(\g)$ acts transitively on the set of Cartan subalgebras of $\g$.
\end{proposition}
\begin{proof}
According to Lemma~\ref{Lie algebra regular element prop}, $\reg(\g)$ is connected. On the other hand, it is the disjoint union of the open equivalence classes of the relation $\sim$. Hence only one such class exists. Since every Cartan subalgebra of $\g$ is of the form $\g^0(\ad(x))$ by Theorem~\ref{Lie algebra regular element and Cartan algebra}, the assertion follows.
\end{proof}
\section{Abstract root systems}
\subsection{Axiomatics}
Let $\g$ be a semisimple Lie algebra and $\h$ be a splitting Cartan subalgebra of $\g$. Then for a root $\alpha\in\Phi$, we have an element $h_\alpha\in[\g_\alpha,\g_{-\alpha}]$ as in Proposition~\ref{Lie algebra spl-s-simp root string lemma}. By that proposition, $h_\alpha$ satisfies $\alpha(h_\alpha)=2$ and $\beta(h_\alpha)\in\Z$ for all $\beta\in\Phi$. Also, by Proposition~\ref{Lie algebra spl-s-simp root string lemma}, we note that
\[s_\alpha(\beta)=\beta-\beta(h_\alpha)\alpha\in\Phi\]
for $\alpha,\beta\in\Phi$, since we have $-\beta(h_\alpha)=q-p\in[-p,q]$.\par
Now if $h_\alpha'\in\h$ is the element given by $(\ref{Lie algebra dual element under Killing form})$, then we have
\[h_\alpha=\frac{2h_\alpha'}{(h_\alpha',h_\alpha')},\]
so that $h_\alpha$ corresponds to the root $2\alpha/(\alpha,\alpha)$ via the Killing form. These observations leads to our definition for an abstract root system.
\begin{definition}
Let $E$ be a \textbf{Euclidian space}, i.e., a finite-dimensional real vector space with an inner product $(\cdot,\cdot)$. A \textbf{reflection} in $E$ is a linear map which induces the map $-\id$ on a line $\R\alpha$ and the identity on the hyperplane perpendicular to $\alpha$. If $\alpha\in E\setminus\{0\}$ is given, then this reflection is given by
\[s_\alpha(\beta)=\beta-2\frac{(\beta,\alpha)}{(\alpha,\alpha)}\alpha.\]
\end{definition}
Since the number $2(\beta,\alpha)/(\alpha,\alpha)$ occurs frequently, we abbreviate it by $\langle\beta,\alpha\rangle$. Notice that $\langle\beta,\alpha\rangle$ is linear only in the first variable and $\langle\alpha,\alpha\rangle=2$.
\begin{definition}
Let $E$ be a Euclidian space and $\Phi\sub E\setminus\{0\}$ be a finite subset which spans $E$. Then $\Phi$ is called a \textbf{root system} if it satisfies the following conditions
\begin{itemize}
\item[(R1)] $\Phi\cap\R\alpha=\{\pm\alpha\}$.
\item[(R2)] $s_\alpha(\Phi)\sub\Phi$ for all $\alpha\in\Phi$.
\item[(R3)] For $\alpha\in\Phi$, $\langle\beta,\alpha\rangle\in\Z$ for all $\beta\in\Phi$.
\end{itemize}
\end{definition}
There is some redundancy in the axioms; in particular, both (R1) and (R2) imply that $\Phi=-\Phi$. In the literature (R1) is sometimes omitted, and what we have called a "root system" is then referred to as a "reduced root system." Notice that replacement of the given inner product on $E$ by a positive multiple would not affect the axioms, since only ratios of inner products occur.\par
Let $\Phi$ be a root system in $E$. Denote by $W=W(\Phi)$ the subgroup of $\GL(E)$ generated by the reflections $s_\alpha$ with $\alpha\in\Phi$. By (R2), $W$ permutes the set $\Phi$, which is finite and spans $E$. This allows us to identify $W$ with a subgroup of the symmetric group on $\Phi$, and in particular we see $W$ is finite. The group $W$ is called the \textbf{Weyl group} of $\Phi$, and plays an extremely important role in the sequel. The following proposition shows how certain automorphisms of $E$ act on $W$ by conjugation.
\begin{proposition}\label{root system invariant transform prop}
Let $\Phi\sub E$ be a root system with Weyl group $W$. If $\tau\in\GL(E)$ leaves $\Phi$ invariant, then
\begin{itemize}
\item[(a)] $\tau s_\alpha\tau^{-1}=s_{\tau(\alpha)}$ for all $\alpha\in\Phi$.
\item[(b)] $\langle\beta,\alpha\rangle=\langle\tau(\beta),\tau(\alpha)\rangle$ for all $\alpha,\beta\in\Phi$.
\end{itemize}
\end{proposition}
\begin{proof}
Note that $\tau s_\alpha\tau^{-1}(\tau(\beta))=\tau s_\alpha(\beta)\in\tau(\Phi)\sub\Phi$. Since $\Phi$ is finite and $\tau$ is bijective, we have $\tau(\Phi)=\Phi$. Hence we also have $\tau s_\alpha\tau^{-1}(\Phi)\sub\Phi$. Further, $\tau s_\alpha\tau^{-1}$ keeps the hyperplane $\tau(\alpha^\bot)$ pointwise fixed, and it maps $\tau(\alpha)$ to $-\tau(\alpha)$. Hence by linearity we get $\tau s_\alpha\tau^{-1}=s_{\tau(\alpha)}$. Now (b) follows from the equations
\[\tau s_\alpha\tau^{-1}(\tau(\beta))=\tau(\beta-\langle\beta,\alpha\rangle\alpha)=\tau(\beta)-\langle\beta,\alpha\rangle\tau(\alpha),\]
and
\[s_{\tau(\alpha)}(\tau(\beta))=\tau(\beta)-\langle\tau(\beta),\tau(\alpha)\rangle\tau(\alpha).\]
This completes the proof.
\end{proof}
There is a natural notion of isomorphism between root systems $\Phi$, $\Phi'$ in respective euclidean spaces $E$, $E'$: Call $(\Phi,E)$ and $(\Phi',E')$ is \textbf{isomorphic} if there exists a vector space isomorphism (not necessarily an isometry) $\phi:E\to E'$ sending $\Phi$ onto $\Phi'$ such that
\[\langle\phi(\alpha),\phi(\beta)\rangle=\langle\alpha,\beta\rangle\]
for each pair of roots $\alpha,\beta\in\Phi$. It follows at once that $s_{\phi(\alpha)}(\phi(\beta))=s_{\alpha}(\beta)$. Therefore an isomorphism of root systems induces a natural isomorphism $w\mapsto\phi\circ w\circ\phi^{-1}$ of Weyl groups. In view of the proposition above, an automorphism of $\Phi$ is the same thing as an automorphism of $E$ leaving $\Phi$ invariant. In particular, we can regard $W$ as a subgroup of $\Aut(\Phi)$.
\begin{example}
Let $\g$ be a semisimple Lie algebra and $\h$ a splitting Cartan subalgebra of $\g$. As we have $\Phi$ is a root system. In particular, we see that to every pair $(\g,\h)$ consisting of a semisimple complex Lie algebra $\g$ and a splitting Cartan subalgebra $\h$, a Weyl group $W(\g,\h):=W(\Phi)$ is assigned in a canonical way.
\end{example}
\begin{example}
If $\Phi\sub E$ is a root system, it is useful to work not only with $\alpha$ but also with $\check{\alpha}:=2\alpha/(\alpha,\alpha)$. We claim that $\check{\Phi}=\{\check{\alpha}:\alpha\in\Phi\}$ is also a root system, called the \textbf{dual root system} of $\Phi$.\par
To verify (R1) for $\check{\Phi}$, we note that $\check{\beta}\in\R\check{\alpha}$ implies $\beta\in\R\alpha$, and hence $\beta=\pm\alpha$, which in turn leads to $\check{\beta}=\pm\check{\alpha}$. Since $s_{\check{\alpha}}$ is the orthogonal reflection in $\check{\alpha}^\bot=\alpha^\bot$, we have $s_\alpha=s_{\check{\alpha}}$. As $s_\alpha$ is an isometry, it satisfies $s_\alpha(\check{\beta})=s_\alpha(\beta)^\vee$, so that $\check{\Phi}$ satisfies (R2). Finally, we note that for $\alpha,\beta\in\Phi$, we have
\[(\check{\alpha},\check{\alpha})=\frac{4}{(\alpha,\alpha)},\]
so that $(\check{\alpha})^\vee=\alpha$. Therefore
\[\langle\check{\alpha},\check{\beta}\rangle=(\check{\alpha},\beta)\in\Z,\]
and we conclude that $\check{\Phi}$ also is a root system.
\end{example}
\begin{remark}
The angle $\theta\in[0,\pi]$ between $\alpha$ and $\beta$ is defined by the
identity
\[\|\alpha\|\|\beta\|\cos\theta=(\alpha,\beta).\]
where $\|\alpha\|=\sqrt{(\alpha,\alpha)}$ is the norm of the Euclidian space $E$. We have
\[2\frac{\|\beta\|}{\|\alpha\|}\cos\theta=2\frac{(\beta,\alpha)}{(\alpha,\alpha)}=\langle\beta,\alpha\rangle,\quad \langle\beta,\alpha\rangle\langle\alpha,\beta\rangle=4\cos^2\theta.\]
Hence if $\langle\beta,\alpha\rangle$, $\langle\alpha,\beta\rangle\in\Z$, then we also have $4\cos^2\theta\in\Z$. The possibilities for $\|\alpha\|\leq\|\beta\|$ are given in Table~\ref{root system angle}.
\begin{table}[h]
\centering
\renewcommand\arraystretch{1.75}
\begin{tabular}{c|ccccccccccc}
\hline
$\langle\alpha,\beta\rangle$&$0$&$1$&$-1$&$1$&$-1$&$1$&$-1$&$1$&$-1$&$2$&$-2$\\
\hline
$\langle\beta,\alpha\rangle$&$0$&$1$&$-1$&$2$&$-2$&$3$&$-3$&$4$&$-4$&$2$&$-2$\\
\hline
$\theta$&$\frac{\pi}{2}$&$\frac{\pi}{3}$&$\frac{2\pi}{3}$&$\frac{\pi}{4}$&$\frac{3\pi}{4}$&$\frac{\pi}{6}$&$\frac{5\pi}{6}$&$0$&$\pi$&$0$&$\pi$\\
\hline
$\frac{\|\beta\|^2}{\|\alpha\|^2}$&\text{arb.}&$1$&$1$&$2$&$2$&$3$&$3$&$4$&$4$&$1$&$1$\\
\hline
\end{tabular}
\caption{Possible angles in a root system.}
\label{root system angle}
\end{table}
\end{remark}
The following simple but very useful criterion can be read off from Table~\ref{root system angle}.
\begin{lemma}\label{root system positive inner product prop}
Let $\Phi$ be a root system, and suppose that $\alpha,\beta\in\Phi$ are not proportional. If $(\alpha,\beta)>0$, then $\alpha-\beta\in\Phi$. If $(\alpha,\beta)>0$ then $\alpha+\beta\in\Phi$.
\end{lemma}
\begin{proof}
Note that $(\alpha,\beta)$ is positive if and only if $\langle\alpha,\beta\rangle$ is positive. Since $\alpha$ and $\beta$ are not proportional, by Table~\ref{root system angle} we have $\langle\alpha,\beta\rangle=1$ or $\langle\beta,\alpha\rangle=1$. If $\langle\alpha,\beta\rangle=1$ then $\alpha-\beta=s_\beta(\alpha)\in\Phi$. If $\langle\beta,\alpha\rangle=1$ then $\beta-\alpha=s_\alpha(\beta)\in\Phi$, hence $\alpha-\beta\in\Phi$. The second claim follows from the first.
\end{proof}
As an application, we consider a pair of nonproportional roots $\alpha,\beta$. We have the following analogue of the Root String Lemma.
\begin{proposition}\label{root space string lemma}
Let $\alpha,\beta\in\Phi$ be nonproportional. Then the set $\{k\in\Z:\beta+k\alpha\in\Phi\}$ is an interval $[-p,q]\cap\Z$. Moreover, $\langle\beta,\alpha\rangle=p-q$.
\end{proposition}
\begin{proof}
If for some $-p<k<q$ we have $\beta+k\alpha\notin\Phi$, then we can find $i<j\in[-p,q]\cap\Z$ such that
\[\beta+i\alpha\in\Phi,\quad\beta+(i+1)\alpha\notin\Phi,\quad \beta+(j-1)\alpha\notin\Phi,\quad \beta+j\alpha\in\Phi.\]
But then Lemma~\ref{root system positive inner product prop} implies both $(\beta+i\alpha,\alpha)\geq 0$, $(\beta+j\alpha,\alpha)\leq 0$. Since $i<j$ and $(\alpha,\alpha)>0$, this is absurd.\par
Now $s_\alpha$ just adds or subtracts a multiple of $\alpha$ to any root, the string $\{\beta+k\alpha:k\in[-p,q]\cap\Z\}$ is invariant under $s_\alpha$. If $s_\alpha$ takes $\beta+q\alpha$ to $\beta-d\alpha$ for some $0\leq d<p$, then it must map this root back to the highest root in the string. In turn, this reflection must take the lowest element of the root string to a higher root than $\beta+q\alpha$, which is absurd. This means we have $s_\alpha(\beta+q\alpha)=\beta-p\alpha$. Now note that
\[s_\alpha(\beta+q\alpha)=\beta+q\alpha-\langle\beta+q\alpha,\alpha\rangle\alpha=\beta-(q+\langle\beta,\alpha\rangle)\alpha,\]
so the claim follows.
\end{proof}
Note that by Proposition~\ref{root space string lemma} and Table~\ref{root system angle}, we see root strings are of length at most $4$.
\subsection{Simple roots and Weyl group}
\subsubsection{Basis and Weyl chambers}
\begin{definition}
Let $\Phi\sub E$ be a root system and $\Delta\sub\Phi$. Then $\Delta$ is called a \textbf{basis for $\Phi$} if
\begin{itemize}
\item[(B1)] $\Delta$ is a basis for the vector space $E$.
\item[(B2)] Every root $\beta\in\Phi$ is of the form $\beta=\sum_{\alpha\in\Delta}k_\alpha\alpha$, where either all $k_\alpha\in\N$, or all $k_\alpha\in-\N$.
\end{itemize}
In this case, the elements of $\Delta$ are called \textbf{simple roots}. 
\end{definition}
In view of (B1), the expression for $\beta$ in (B2) is unique. The height of the root $\beta=\sum_{\alpha\in\Delta}k_\alpha\alpha$ is then defined to be the number $\sum_{\alpha\in\Delta}k_\alpha$. Roots with positive height are called \textbf{positive} and roots with negative height are called \textbf{negative}. The set of positive roots is denoted by $\Phi^+$, the set of negative roots by $\Phi^-$. We define a partial order $\preceq$ on $E$ by
\[\alpha\preceq\beta\iff\beta-\alpha\in\sum_{\gamma\in\Phi^+}\N\gamma.\]
The only problem with the definition of base is that it fails to guarantee existence. Before proving this, we first observe the following property of a basis.
\begin{lemma}\label{root system basis negative inner product}
Let $\Phi$ be a root system and $\Delta$ be a basis for $\Phi$. Suppose that $\alpha,\beta\in\Delta$ with $\alpha\neq\beta$. Then $(\alpha,\beta)\leq 0$ and $\alpha-\beta$ is not a root.
\end{lemma}
\begin{proof}
Since $\Delta$ is a basis for $E$, $\alpha$ and $\beta$ cannot be proportional. If we have $(\alpha,\beta)>0$, then Lemma~\ref{root system positive inner product prop} shows that $\alpha-\beta\in\Phi$. But this contradicts the definition of a basis for $\Phi$.
\end{proof}
Let $\Phi\sub E$ be a root system and $\lambda\in E$. Then $\lambda$ is called \textbf{regular} if $\lambda\notin\alpha^\bot$ holds for all $\alpha\in\Phi$. Otherwise, $\lambda$ is called \textbf{singular}. For any regular element $\lambda\in E$, the set
\[\Phi^+(\lambda):=\{\alpha\in\Phi:(\lambda,\alpha)>0\}\]
is called the corresponding \textbf{positive system}. An element $\alpha\in\Phi^+(\lambda)$ is called \textbf{decomposable} if there are $\beta_1,\beta_2\in\Phi^+(\lambda)$ with $\alpha=\beta_1+\beta_2$, otherwise, it is called \textbf{indecomposable}.
\begin{theorem}\label{root system basis and Weyl chamber}
For each regular element $\lambda\in E$, the set $\Delta:=\Delta(\lambda)$ of indecomposable elements in $\Phi^+(\lambda)$ is a basis for $\Phi$. Conversely, every basis is of this form.
\end{theorem}
\begin{proof}
First, we show that every element of $\Phi^+(\lambda)$ can be written as a linear combination of elements of $\Delta(\lambda)$ with coefficients in $\N$. For this, we suppose that $\alpha\in\Phi^+(\lambda)$ cannot be written in this form, and that it has the smallest $(\alpha,\lambda)$ of all elements with this property. In particular, $\alpha\notin\Delta(\lambda)$, and there exist $\beta_1,\beta_2\in\Phi^+(\lambda)$ with $\alpha=\beta_1+\beta_2$, hence, $(\alpha,\lambda)=(\beta_1,\lambda)+(\beta_2,\lambda)$. But since $(\beta_i,\lambda)>0$ for $i=1,2$, the minimality of $(\alpha,\lambda)$ shows that the $\beta_i$ can be written as a linear combination of elements of $\Delta(\lambda)$ with coefficients in $\N$. Then this also holds for $\alpha$, contradicting our assumption.\par
As a consequence, we see that every $\beta\in\Phi$ is of the form $\beta=\sum_{\alpha\in\Delta(\lambda)}k_\alpha\alpha$, where either all $k_\alpha\in\N$ or all $k_\alpha\in-\N$. Since $\Phi$ spans the space $E$, it remains to be shown that $\Delta(\lambda)$ is linearly independent.\par
Next we show that $(\alpha,\beta)\leq 0$ for all $\alpha,\beta\in\Delta(\lambda)$ with $\alpha\neq\beta$. In fact, if $(\alpha,\beta)>0$, then $\alpha\notin-\R^+\beta$. By Table~\ref{root system angle}, $\alpha$ and $\beta$ can only be proportional if $\alpha=2\beta$ or $\beta=2\alpha$, which would contradict the indecomposability of these elements. Hence we can apply Lemma~\ref{root system positive inner product prop}, and we get $\alpha-\beta\in\Phi=\Phi^+(\lambda)\cup\Phi^+(\lambda)$. If $\alpha-\beta\in\Phi^+(\lambda)$ then $\alpha=\beta+(\alpha-\beta)$, which contradicts the assumption that $\alpha$ is indecomposable. Similarly, $\beta-\alpha\in\Phi^+(\lambda)$ gives a contradiction by $\beta=\alpha+(\beta-\alpha)$.\par
It remains to prove that $\Delta(\lambda)$ is linearly independent. Suppose that $\sum_{\alpha\in\Delta(\lambda)}r_\alpha\alpha=0$ with $r_\alpha\in\R$, and set
\[\Delta(\lambda)_+:=\{\alpha\in\Delta(\lambda):r_\alpha>0\},\quad\Delta(\lambda)_-:=\{\alpha\in\Delta(\lambda):r_\alpha<0\}.\]
Then $\nu:=\sum_{\alpha\in\Delta(\lambda)_+}|r_\alpha|\alpha=\sum_{\beta\in\Delta(\lambda)_-}|r_\beta|\beta$, so
\[(\nu,\nu)=\sum_{\alpha\in\Delta(\lambda)_+,\beta\in\Delta(\lambda)_-}|r_\alpha r_\beta|(\alpha,\beta)\leq 0,\]
which leads to $\nu=0$. But we then have $0=(\lambda,\nu)=\sum_{\alpha\in \Delta(\lambda)_\pm}|r_\alpha|(\lambda,\alpha)$, which implies $\Delta(\lambda)_\pm=\emp$ since we otherwise arrive at the contradiction $r_\alpha=0$ for some $\alpha\in\Delta(\lambda)_\pm$.\par
Conversely, let $\Delta$ be a basis for $\Phi$. We show that $\Delta$ is of the form $\Delta(\lambda)$ for some regular element $\lambda\in E$. We arrange $\Delta$ in the form $\alpha_1,\dots,\alpha_n$, and consider the dual basis $\alpha^1,\dots,\alpha^n$ for $E$ which is given by $(\alpha^i,\alpha_j)=\delta_{ij}$. Set $\lambda=\sum_{i=1}^{n}\alpha^i$, then $(\lambda,\alpha_i)>0$ for all $i$, so that $(\lambda,\alpha)>0$ for all $\alpha\in\Delta$. Since every $\beta\in\Phi$ can be written as a linear combination of the $\alpha\in\Delta$ with coefficients of the same sign, $\lambda$ is regular. Then we have $\Phi^+\sub\Phi^+(\lambda)$ and $\Phi^-\sub\Phi^-(\lambda)$, which leads to $\Phi^+=\Phi^+(\lambda)$. Since each element in $\Phi^+$ is a positive integral linear combination of elements in $\Delta$, it is then clear that $\Delta$ consists of indecomposable elements, i.e., $\Delta\sub\Delta^+(\lambda)$. But $\Delta$ and $\Delta(\lambda)$ are both basis for $E$, so we get $\Delta=\Delta(\lambda)$.
\end{proof}
Note that the following lemma is deduced in our proof. We state it for later use.
\begin{lemma}\label{root system linear independent if positive}
Let $M\sub E$ be contained in an open half space of $E$, i.e., there is a $\lambda\in E$ with $(\lambda,\alpha)>0$ for all $\alpha\in M$. If $(\alpha,\beta)\leq 0$ for all $\alpha,\beta\in M$ with $\alpha\neq\beta$. Then $M$ is linearly independent.
\end{lemma}
It is useful to introduce a bit of terminology. The hyperplanes $P_\alpha$, $\alpha\in\Phi$ partition $E$ into finitely many regions; the connected components of $E\setminus\bigcup_{\alpha\in\Phi}P_\alpha$ are called the \textbf{Weyl chambers} of $E$. Each regular element $\lambda\in E$ therefore belongs to precisely one Weyl chamber, denoted $C(\lambda)$. To say that $C(\lambda)=C(\lambda')$ is just to say that $\lambda$ and $\lambda'$ lie on the same side of each hyperplane $P_\alpha$, $\alpha\in\Phi$, i.e., that $\Phi(\lambda)=\Phi(\lambda')$, or equivalently $\Delta(\lambda)=\Delta(\lambda')$. Therefore the Weyl chambers are in natural one-to-one correspondence with basis for $\Phi$. If $\Delta$ is a basis for $\Phi$, we write $C(\Delta)=C(\lambda)$ if $\Delta=\Delta(\lambda)$, and call this the \textbf{fundamental Weyl chamber relative to $\bm{\Delta}$}. Finally, note that $C(\lambda)$ is the open convex set consisting of all $\lambda\in E$ which satisfy the inequalities $(\lambda,\alpha)>0$ for all $\alpha\in\Delta$.
\begin{example}
Let $\check{\Phi}$ be the dual system of $\Phi$, and $\check{\Delta}=\{\check{\alpha}:\alpha\in\Delta\}$. Since $(\lambda,\alpha)>0$ iff $(\lambda,\check{\alpha})>0$, we see $\Phi$ and $\check{\Phi}$ have the same Wely chambers. Moreover, if $\Delta=\Delta(\lambda)$ for some $\lambda\in C(\Delta)$, then
\[\check{\Delta}=\check{\Delta}(\lambda)=\{\check{\alpha}\in\check{\Phi}:(\lambda,\check{\alpha})>0\}.\]
This shows $\check{\Delta}$ is a basis for $\check{\Phi}$.
\end{example}
\subsubsection{Properties of simple roots}
In this part, we rule out some important properties for simple roots of a root system that will be used later to establish certain properties of Weyl group.
\begin{lemma}\label{root system nonsimple roots substract by simple}
Let $\Phi\sub E$ be a root system and $\Delta$ be a basis for $\Phi$. For $\alpha\in\Phi^+\setminus\Delta$, there exists a $\beta\in\Delta$ with $\alpha-\beta\in\Phi^+$.
\end{lemma}
\begin{proof}
Let $\alpha\in\Phi^+\setminus\Delta$. Suppose for all $\beta\in\Delta$, we have $(\alpha,\beta)\leq 0$. Then the set $\Delta\cup\{\alpha\}$ satisfies the assumptions of Lemma~\ref{root system linear independent if positive}, hence is linearly independent. Since $\Delta$ is a basis for $E$, this cannot be the case, i.e., there is a $\beta\in\Delta$ with $(\alpha,\beta)>0$. If $\alpha$ and $\beta$ are not proportional, then Lemma~\ref{root system positive inner product prop} shows that $\alpha-\beta$ is a root; if $\alpha$ and $\beta$ are proportional, then $\alpha=2\beta$ since $\beta$ is indecomposable, and so $\alpha-\beta=\beta$ is a root. Since $\alpha\in\Phi^+\setminus\Delta$, it is a linear combination of elements in $\Delta$ with at least two positive (integral) coefficients. Subtracting $\beta$ leaves at least one positive coefficient, so $\alpha-\beta$, being a root, has to be positive.
\end{proof}
\begin{corollary}\label{root system successive sum into positive root}
Every $\beta\in\Phi^+$ can be written in the form $\alpha_1+\cdots+\alpha_n$ with $\alpha_i\in\Delta$ such that $\sum_{i=1}^{k}\alpha_i\in\Phi^+$ for each $k\in\{1,\dots,n\}$.
\end{corollary}
\begin{proof}
This follows from Lemma~\ref{root system nonsimple roots substract by simple} and induction on the height of $\beta$.
\end{proof}
\begin{lemma}\label{root system reflection on complement of basis element}
If $\alpha\in\Delta$ then $s_\alpha$ permutes the set $\Phi^+\setminus\{\alpha\}$.
\end{lemma}
\begin{proof}
Let $\beta\in\Phi^+\setminus\{\alpha\}$ and $\beta=\sum_{\gamma\in\Delta}k_\gamma\gamma$ with $k_\gamma\in\N$. Since $\Phi\cap\R\alpha=\{\pm\alpha\}$, $\beta$ is not proportional to $\alpha$. Hence there is a $\gamma\neq\alpha$ with $k_\gamma>0$. But the coefficient of $\gamma$ in $s_\alpha(\beta)=\beta-\langle\beta,\alpha\rangle\alpha$ is still $k_\gamma$. In other words, $s_\alpha(\beta)$ has at least one positive coefficient (relative to $\Delta$), forcing it to be positive. Moreover, $s_\alpha(\beta)\neq\alpha$ since $\alpha$ is the image of $-\alpha$.
\end{proof}
\begin{corollary}\label{root system half sum of positive under reflection}
Let $\delta=\frac{1}{2}\sum_{\beta\in\Phi^+}\beta$. Then $s_\alpha(\delta)=\delta-\alpha$ for all $\alpha\in\Delta$.
\end{corollary}
\begin{proof}
This is immediate from Lemma~\ref{root system reflection on complement of basis element}.
\end{proof}
\begin{lemma}\label{root system iterated reflection}
Let $\alpha_1,\dots,\alpha_r\in\Delta$ and set $s_i:=s_{\alpha_i}$. If $s_1\cdots s_{r-1}(\alpha_r)\in\Phi^-$, then there is an $i\in\{1,\dots,r-1\}$ such that
\[w=s_1\cdots s_r=s_1\cdots\hat{s}_i\cdots\hat{s}_r.\]
\end{lemma}
\begin{proof}
Set
\[\beta_i=\begin{cases}
s_{i+1}\cdots s_{r-1}(\alpha_r)&\text{for }i=0,\dots,r-2,\\
\alpha_r&\text{for }i=r-1.
\end{cases}\]
Then $\beta_0\preceq 0\preceq\beta_{r-1}$, and there is a minimal $i\in\{1,\dots,r-1\}$ with $0\preceq\beta_i$. For this $i$, we have $s_i(\beta_i)=\beta_{i-1}\preceq 0$. In view of Lemma~\ref{root system reflection on complement of basis element}, this shows $\beta_i=\alpha_i$. By Proposition~\ref{root system invariant transform prop}, for $w:=s_{i+1}\cdots s_{r-1}$, we have
\[s_i=s_{\alpha_i}=s_{\beta_i}=s_{w(\alpha_r)}=ws_rw^{-1},\]
which implies
\begin{align*}
s_1\cdots s_r&=(s_1\cdots s_{i-1})s_iws_r=(s_1\cdots s_{i-1})(ws_rw^{-1})ws_r\\
&=(s_1\cdots s_{i-1})w=s_1\cdots s_{i-1}s_{i+1}\cdots s_{r-1}.
\end{align*}
This shows the claim.
\end{proof}
\begin{corollary}\label{root system Weyl element minimal expresion prop}
Let $w=s_1\dots s_r$, where the $s_j=s_{\alpha_j}$ are reflections associated with the simple roots $\alpha_j\in\Delta$, and where $r$ is the minimal number of factors needed to represent $w$ as such a product. Then $w(\alpha_r)\in\Phi^-$.
\end{corollary}
\begin{proof}
This follows from $w(\alpha_r)=s_1\cdots s_r(\alpha_r)=s_1\cdots s_{r-1}(-\alpha_r)$ and Lemma~\ref{root system iterated reflection}.
\end{proof}
\subsubsection{The Weyl group}
Now we are in a position to prove that $W$ permutes the bases of $\Phi$ (or, equivalently, the Weyl chambers) in a simply transitive fashion and that $W$ is generated by the "simple reflections" relative to any base $\Delta$ (i.e., by the reflections $s_\alpha$, $\alpha\in\Delta$).
\begin{theorem}\label{root system Weyl group prop}
Let $\Phi$ be a root system, $W$ the corresponding Weyl group, and $\Delta$ a basis for $\Phi$.
\begin{itemize}
\item[(a)] For every regular element $\lambda\in E$, there is a $w\in W$ such that $(w(\lambda),\alpha)>0$ for all $\alpha\in\Delta$. In particular, $W$ acts transitively on the set of the Weyl chambers.
\item[(b)] Let $\Delta'$ be another basis for $\Phi$. Then there is a $w\in W$ with $w(\Delta')=\Delta$, i.e., the Weyl group also acts transitively on the set of the bases.
\item[(c)] For every root $\alpha\in\Phi$, there is a $w\in W$ with $w(\alpha)\in\Delta$.
\item[(d)] $W$ is generated by the $s_\alpha$'s, with $\alpha\in\Delta$.
\item[(e)] If $w(\Delta)=\Delta$ for some $w\in W$, then $w=\id$. In other words, $W$ acts freely on bases of $\Phi$. 
\end{itemize}
\end{theorem}
\begin{proof}
Let $W'$ be the subgroup of $W$ generated by the $s_\alpha$ with $\alpha\in\Delta$. Suppose, (c) holds for $W'$ instead of $W$. Then for $\alpha\in\Phi$, we can find $w\in W'$ with $w(\alpha)\in\Delta$. Then by $s_{w(\alpha)}=ws_\alpha w^{-1}$, we obtain that $s_\alpha=w^{-1}s_{w(\alpha)}w\in W'$. Since $W$ is generated by the $s_\alpha$ with $\alpha\in\Phi$, we get $W=W'$, and hence (d). Then (e) is an immediate consequence of Corollary~\ref{root system Weyl element minimal expresion prop} applied to $w$. Therefore, we see that it suffices to show (a)--(c) for $W'$ instead of $W$.\par
Let $\lambda\in E$ be a regular element. Choose $w\in W'$ such that the number $(w(\lambda),\delta)$ is maximal for the given $\lambda$ and $\delta=\frac{1}{2}\sum_{\beta\in\Phi^+}\beta$. For $\alpha\in\Delta$, we have $s_\alpha w\in W'$, hence, by Corollary~\ref{root system half sum of positive under reflection} and the choice of $w$,
\[(w(\lambda),\delta)\geq(s_\alpha w(\lambda),\delta)=(w(\lambda),s_\alpha(\delta))=(w(\lambda),\delta-\alpha)=(w(\lambda),\delta)-(w(\lambda),\alpha).\]
This gives $(\lambda,w^{-1}(\alpha))=(w(\lambda),\alpha)\geq 0$ for all $\alpha\in\Delta$, hence also for all $\alpha\in\Phi^+$. Since $\lambda$ is regular, all inequalities are strict, so claim (a) follows. Now (b) is a concequence of (a).\par
Now we prove (c). Because of (b), it suffices to show that $\alpha$ is an element of some basis. If $\beta\neq\pm\alpha$ is a root, then $\alpha$ and $\beta$ are not proportional since $\Phi\cap\R\alpha=\{\pm\alpha\}$. Thus $\alpha^\bot\neq\beta^\bot$, and we can find a $\lambda\in E$ such that
\[\lambda\in\alpha^\bot\setminus\bigcup_{\beta\in\Phi\setminus\{\pm\alpha\}}\beta^\bot.\]
By a small modification of $\lambda$ (adding, e.g., $\eps\alpha$ for a small $\eps>0$), we obtain a $\lambda'\in E$ with $(\lambda',\beta)\neq 0$ for $\beta\in\Phi\setminus\{\pm\alpha\}$. Then $\lambda'$ is regular, and we have $\alpha\in\Delta(\lambda')$.
\end{proof}
We can use the results above to explore more precisely the significance of the generation of $W$ by simple reflections. When $w\in W$ is written as $s_{\alpha_1}\cdots s_{\alpha_r}$ with $r$ minimal, we call the expression \textbf{reduced}, and write $\ell(w)=r$: this is the \textbf{length} of $w$, relative to $\Delta$. By definition, $\ell(1)=0$. We can characterize length in another way, as follows. Define $n(w)$ to be the number of positive roots $\alpha$ that are send to negative roots by $w$.
\begin{proposition}\label{root system Weyl group char of height}
For all $w\in W$, we have $\ell(w)=n(w)$.
\end{proposition}
\begin{proof}
Proceed by induction on $\ell(w)$. The case $\ell(w)=0$ is clear: $\ell(w)=0$ implies $w=\id$, so $n(w)=0$. Assume the lemma true for all $\tau\in W$ with $\ell(\tau)<\ell(w)$. Write $w$ in reduced form as $w=s_{\alpha_1}\cdots s_{\alpha_r}$, and set $\alpha=\alpha_r$. By Corollary~\ref{root system Weyl element minimal expresion prop}, we have $w(\alpha_r)\in\Phi^-$. Then Lemma~\ref{root system reflection on complement of basis element} implies that $n(ws_\alpha)=n(w)-1$. On the other hand, $\ell(ws_\alpha)=\ell(w)-1<\ell(w)$, so by induction $\ell(ws_\alpha)=n(ws_\alpha)$. Combining these statements, we get $\ell(w)=n(w)$.
\end{proof}
Next we look more closely at the action of $W$ on Weyl chambers. We show that the closure $\widebar{C(\Delta)}$ of the fundamental Weyl chamber relative to $\Delta$ is a \textbf{fundamental domain} for the action of $W$ on $E$, i.e., each vector in $E$ is $W$-conjugate to precisely one point of this set.
\begin{proposition}\label{root system fundamental domain of Weyl group}
Each point of $E$ is $W$-conjugate to a point in the closure of the fundamental Weyl chamber relative to a base $\Delta$. Moreover, if $\lambda,\mu\in\widebar{C(\Delta)}$ and $w(\lambda)=\mu$ for some $w\in W$, then $\lambda=\mu$.
\end{proposition}
\begin{proof}
We define a partial order on $E$: for $\lambda,\mu\in E$, $\lambda\preceq\mu$ iff $\mu-\lambda$ is a nonnegative $\R$-linear combination of simple roots. Since $\Delta$ spans $E$, this definition makes sense. If $\mu\in E$, choose $w\in W$ for which $w(\mu)$ is maximal in this partial order. We claim that $w(\mu)\in\widebar{C(\Delta)}$. In fact, if this does not hold, then there exists $\alpha\in\Delta$ such that $(w(\mu),\alpha)<0$, whence $\langle w(\mu),\alpha\rangle<0$. Now it suffices to note that
\[s_\alpha w(\mu)-w(\mu)=-\langle w(\mu),\alpha\rangle\alpha\succeq 0,\]
which contradicts the choice of $w$.\par
Let $w(\lambda)=\mu$ with $\lambda,\mu\in\widebar{C(\Delta)}$. We induct on $\ell(w)$. The case $\ell(w)=0$ is clear, since this implies $w=\id$. Let $\ell(w)>0$. By Lemma~\ref{root system Weyl group char of height}, $w$ cannot send all simple roots to positive roots. Say $w(\alpha)\in\Phi^-$ with $\alpha\in\Delta$. Now because $\lambda,\mu\in\widebar{C(\Delta)}$, 
\[0\geq(\mu,w(\alpha))=(w^{-1}(\mu),\alpha)=(\lambda,\alpha)\geq 0.\]
This forces $(\lambda,\alpha)=0$, so $s_\alpha(\lambda)=\lambda$ and we have $ws_\alpha(\lambda)=\mu$. Since $w(\alpha)\in\Phi^-$, by Lemma~\ref{root system reflection on complement of basis element} we have $n(ws_\alpha)=n(w)-1$, and Lemma~\ref{root system Weyl group char of height} implies $\ell(ws_\alpha)=\ell(w)-1$, so induction may be applied.
\end{proof}
\subsubsection{Irreducible root system}
In this part we consider irreducibility of root systems. Our starting point will be the following proposition.
\begin{proposition}\label{root system direct sum}
Suppose $(E_1,\Phi_1)$ and $(E_2,\Phi_2)$ are root systems. Consider the vector space $E_1\oplus E_2$, with the natural inner product determined by the inner products on $E_1$ and $E_2$. Then $\Phi_1\cup\Phi_2$ is a root system in $E_1\oplus E_2$, called the direct sum of $\Phi_1$ and $\Phi_2$.
\end{proposition}
\begin{proof}
It is clear that $\Phi_1\cup\Phi_2$ spans $E_1\oplus E_2$, and conditions (R1) holds because $\Phi_1$ and $\Phi_2$ are root systems. For condition (R2), if $\alpha$ and $\beta$ are both in $\Phi_1$ or both in $\Phi_2$, then $s_\alpha(\beta)\in\Phi_1\cup\Phi_2$ because $\Phi_1$ and $\Phi_2$ are root systems. If $\alpha\in\Phi_1$ and $\beta\in\Phi_2$ or vice versa, then $\langle\alpha,\beta\rangle=0$, so that $s_\alpha(\beta)=\beta\in\Phi_1\cup\Phi_2$ too. Similarly, if $\alpha$ and $\beta$ are both in $\Phi_1$ or both in $\Phi_2$, then $\langle\alpha,\beta\rangle$ is an integer because $\Phi_1$ and $\Phi_2$ are root systems, and if $\alpha\in\Phi_1$ and $\beta\in\Phi_2$ or vice versa, then $\langle\alpha,\beta\rangle=0$. Thus, condition (R3) holds for $\Phi_1\cup\Phi_2$.
\end{proof}
A root system $\Phi$ is called \textbf{irreducible} if it cannot be written as the direct sum of two root systems. That is, if it cannot by partitioned into the union of two proper subsets which are orthogonal. Simialrly, a basis for $\Phi$ is irreducible if cannot be partitioned in the way just stated. First of all, let note the following simple fact.
\begin{proposition}\label{root system irreducible iff basis is}
Let $\Delta$ be a basis for $\Phi$. Then $\Phi$ is irreducible iff $\Delta$ is irreducible.
\end{proposition}
\begin{proof}
In one direction, let $\Phi=\Phi_1\cup\Phi_2$, with $\Phi_1\bot\Phi_2$. Unless $\Delta$ is wholly contained in $\Phi_1$ or $\Phi_2$, this induces a similar partition of $\Delta$; but $\Delta\sub\Phi_1$ implies $(\Delta,\Phi_2)=0$, or $(E,\Phi_2)=0$, since $\Delta$ spans $E$. This shows the "if" direction.\par
Conversely, let $\Phi$ be irreducible, but $\Delta=\Delta_1\cup\Delta_2$ with $\Delta_1\bot\Delta_2$. Each root is conjugate to a simple root, so $\Phi=\Phi_1\cup\Phi_2$, with $\Phi_i$ the set of roots having a conjugate in $\Delta_i$. Let $E_i$ be ths subspace of $E$ spanned by $\Delta_i$. Recall that $W$ is generated by $s_\alpha$, $\alpha\in\Delta$. Since $s_\alpha(\beta)=\beta$ when $(\alpha,\beta)=0$, we see $W$ maps $\Delta_i$ into $E_i$. This implies $\Phi_i\sub E_i$, and so $\Phi_1\bot\Phi_2$. Since $\Phi$ is irreducible, we have $\Phi_1=\emp$ or $\Phi_2=\emp$, whence $\Delta_1=\emp$ or $\Delta_2=\emp$.
\end{proof}
Now we explore some properties for irreducible root systems.
\begin{proposition}\label{root system irreducible unique maximal root}
Let $\Phi$ be an irreducible root system and $\Delta$ be a basis of $\Phi$. Relative to the partial ordering $\preceq$, there is a unique maximal root $\beta$. If $\beta=\sum_{\alpha\in\Delta}k_\alpha\alpha$ then all $k_\alpha>0$.
\end{proposition}
\begin{proof}
Let $\beta=\sum_{\alpha\in\Delta}k_\alpha\alpha$ be maximal in the ordering; evidently $\beta\succeq 0$. If $\Delta_1=\{\alpha\in\Delta:k_\alpha>0\}$ and $\Delta_2=\{\alpha\in\Delta:k_\alpha=0\}$ then $\Delta=\Delta_1\cup\Delta_2$ is a partition. Suppose that $\Delta_2\neq\emp$, then by Lemma~\ref{root system basis negative inner product} we have $(\alpha,\beta)\leq 0$ for any $\alpha\in\Delta_2$; since $\Phi$ is irreducible, at least one $\alpha\in\Delta_2$ must be nonorthogonal to $\Delta_1$, forcing $(\alpha,\alpha')<0$ for some $\alpha'\in\Delta_1$, whence $(\alpha,\beta)<0$. This implies $\alpha+\beta$ is a root (Lemma~\ref{root system positive inner product prop}), contradicting the maximality of $\beta$. Therefore $\Delta_2=\emp$ and all $k_\alpha>0$. This argument shows also that $(\alpha,\beta)\geq 0$ for all $\alpha\in\Delta$, and there exist $\alpha\in\Delta$ with $(\alpha,\beta)>0$ (since $\Delta$ spans $E$).\par
Now let $\beta'$ be another maximal root. The preceding argument applies to $\beta'$, showing that $\beta'$ has positive coefficient on each $\alpha\in\Delta$. It then follows that $(\beta',\beta)>0$, and $\beta-\beta'$ is a root unless $\beta=\beta'$. But if $\beta-\beta'$ is a root, then either $\beta\preceq\beta'$ or else $\beta'\preceq\beta$, which is absurd. So $\beta$ is unique.
\end{proof}
\begin{proposition}\label{root system irreducible Weyl group acts irreducibly}
Let $\Phi$ be an irreducible root system. Then $W$ acts irreducibly on $E$ (that is, $W$ has no nontrivial invariant subspace). In particular, the $W$-orbit of any root $\alpha$ spans $E$.
\end{proposition}
\begin{proof}
The span of the $W$-orbit of a root is a (nonzero) $W$-invariant subspace of $E$, so the second statement follows from the first. As to the first, let $V$ be a nonzero subspace of $E$ invariant under $W$. Then the orthogonal complement $V^\bot$ of $V$ is also $W$-invariant (since $W$ consists of isometries), and $E=V\oplus V^\bot$. It is trivial to verify that for $\alpha\in\Phi$, either $\alpha\in V$ or $\alpha\bot V$, since $s_\alpha(V)\sub V$. Thus $\alpha\notin V$ implies $\alpha\bot V$, so each root lies in one subspace or the other. This partitions $\Phi$ into orthogonal subsets, forcing one or the other to be empty. Since $\Phi$ spans $E$ and $V\neq\{0\}$, we conclude that $E=V$.
\end{proof}
\begin{proposition}\label{root system irreducible length type}
Let $\Phi$ be an irreducible root system. Then at most two root lengths occur in $\Phi$, and all roots of a given length are conjugate under $W$.
\end{proposition}
\begin{proof}
If $\alpha,\beta$ are arbitrary roots, then not all $w(\alpha)$, $w\in W$ can be orthogonal to $\beta$, since the $w(\alpha)$, $w\in W$ span $E$ (Proposition~\ref{root system irreducible Weyl group acts irreducibly}). If $(\alpha,\beta)\neq 0$, we know that the possible ratios of squared root lengths of $\alpha,\beta$ are $1$, $2$, $3$, $1/2$, $1/3$. These two remarks easily imply the first assertion, since the presence of three root lengths would yield also a ratio $3/2$.\par
Now let $\alpha,\beta$ have equal length. After replacing one of these by a $W$-conjugate (as above), we may assume them to be nonorthogonal (and distinct: otherwise we're done). According to Table~\ref{root system angle}, this in turn forces $\langle\alpha,\beta\rangle=\langle\beta,\alpha\rangle=\pm 1$. By replacing $\beta$ by $-\beta=s_\beta(\beta)$, we may assume that $\langle\alpha,\beta\rangle=\langle\beta,\alpha\rangle=1$. Then
\[s_\alpha s_\beta s_\alpha(\beta)=s_\alpha s_\beta(\beta-\alpha)=s_\alpha(-\beta-\alpha+\beta)=\alpha.\]
This proves the second claim.
\end{proof}
In case $\Phi$ is irreducible, with two distinct root lengths, we speak of \textbf{long} and \textbf{short} roots. If all roots are of equal length, it is conventional to call all of them long.
\begin{proposition}\label{root system irreducible maximal root is long}
Let $\Phi$ be an irreducible root system, with two distinct root lengths. Then the maximal root $\beta$ of $\Phi$ is long.
\end{proposition}
\begin{proof}
Let $\alpha\in\Phi$ be arbitrary. It will suffice to show that $(\beta,\beta)\geq(\alpha,\alpha)$. For this we may replace $\alpha$ by a $W$-conjugate lying in the closure of the fundamental Weyl chamber (relative to $\Delta$). Since $\beta-\alpha\succeq 0$, we have $(\gamma,\beta-\alpha)\geq 0$ for any $\gamma\in\widebar{C(\Delta)}$. This fact, applied to the cases $\gamma=\beta$ and $\gamma=\alpha$, yeilds $(\beta,\beta)\geq(\beta,\alpha)\geq(\alpha,\alpha)$.
\end{proof}
\subsection{Classification of root systems}
\subsubsection{Cartan matrix}
Fix an ordering $(\alpha_1,\dots,\alpha_n)$ of the simple roots. The matrix $(\langle\alpha_i,\alpha_j\rangle)$ is then called the \textbf{Cartan matrix} of $\Phi$. Its entries are called \textbf{Cartan integers}. The matrix of course depends on the chosen ordering, but this is not very serious. The important point is that the Cartan matrix is independent of the choice of $\Delta$, thanks to the fact that $W$ acts transitively on the collection of bases. The Cartan matrix is nonsingular since $\Delta$ is a basis of $E$. It turns out to characterize $\Phi$ completely.
\begin{proposition}\label{root system determined by Cartan matrix}
Let $(\Phi',E')$ be another root system, with base $\Delta'=\{\alpha_1',\dots,\alpha_n'\}$. If $\langle\alpha_i,\alpha_j\rangle=\langle\alpha_i',\alpha_j'\rangle$ for $1\leq i,j\leq n$, then the bijection $\alpha_i\mapsto\alpha_i'$ extends (uniquely) to an isomorphism $\phi:E\to E'$ mapping $\Phi$ onto $\Phi'$ satisfying $\langle\phi(\alpha),\phi(\beta)\rangle=\langle\alpha,\beta\rangle$ for all $\alpha,\beta\in\Phi$. Therefore, the Cartan matrix of $\Phi$ determines $\Phi$ up to isomorphism.
\end{proposition}
\begin{proof}
Since $\Delta$ (resp. $\Delta'$) is a basis of $E$ (resp. $E'$), there is a unique vector space isomorphism $\phi:E\to E'$ sending $\alpha_i$ to $\alpha_i'$. If $\alpha,\beta\in\Delta$, the hypothesis insures that
\[s_{\phi(\alpha)}(\phi(\beta))=s_{\alpha'}(\beta')=\beta'-\langle\beta',\alpha'\rangle\alpha'=\phi(\beta)-\langle\beta,\alpha\rangle\phi(\alpha)=\phi(\beta-\langle\beta,\alpha\rangle\alpha)=\phi(s_\alpha(\beta)).\]
In other words , the following diagram commutes for each $\alpha\in\Delta$:
\[\begin{tikzcd}
E\ar[r,"\phi"]\ar[d,swap,"s_\alpha"]&E'\ar[d,"s_{\phi(\alpha)}"]\\
E\ar[r,"\phi"]&E'
\end{tikzcd}\]
The respective Weyl groups $W$, $W'$ are generated by simple reflections, so it follows that the map $w\mapsto\phi\circ w\circ\phi^{-1}$ is an isomorphism of $W$ onto $W'$, sending $s_\alpha$ to $s_{\phi(\alpha)}$ for $\alpha\in\Delta$. But each $\beta\in\Phi$ is conjugate under $W$ to a simple root, say $\beta=w(\alpha)$. This in turn forces $\phi(\beta)=(\phi\circ w\circ\phi^{-1})(\phi(\alpha))\in\Phi'$. It follows that $\phi$ maps $\Phi$ onto $\Phi'$; moreover, the formula for a reflection shows that $\phi$ preserves all Cartan integers.
\end{proof}
\subsubsection{Dynkin diagrams}
A Dynkin diagram is a convenient graphical way of encoding the structure of a base for a root system $\Phi$, and thus also of $\Phi$ itself.
\begin{definition}
If $\Delta=\{\alpha_1,\dots,\alpha_r\}$ is a base for a root system $\Phi$, the \textbf{Dynkin diagram} for $\Phi$ is a graph having vertices $v_1,\dots,v_r$. Between two distinct vertices $v_i$ and $v_j$, we place $\langle\alpha_i,\alpha_j\rangle\langle\alpha_j,\alpha_i\rangle$ edges. In addition, if $\alpha_i$ and $\alpha_j$ are not orthogonal and have different lengths, we decorate the edges between $v_i$ and $v_j$ with an arrow pointing from the vertex associated to the longer root to the vertex associated to the shorter root.
\end{definition}
Note that by Table~\ref{root system angle}, edges $0$, $1$, $2$, $3$ correspond to angles $\pi/2$, $2\pi/3$, $3\pi/4$, $5\pi/6$, which in turn correspond to length ratios of $1$, $\sqrt{2}$, $\sqrt{3}$, respectively. Thus, the number of edges between vertices corresponding to two nonorthogonal roots is $1$, $2$, or $3$ according to whether the length ratio (of the longer to the shorter) is $1$, $\sqrt{2}$, or $\sqrt{3}$. Thinking of the arrow decorating the edges as a "greater than" sign helps one to recall which way the arrow should go.\par
Two Dynkin diagrams are said to be \textbf{isomorphic} if there is a one-to-one, onto map of the vertices of one to the vertices of the other that preserves the number of bonds and the direction of the arrows. By Proposition~\ref{root system Weyl group prop}, any two bases $\Delta_1$ and $\Delta_2$ for a fixed root system are related by the action of a unique Weyl group element $w$. Since $w$ preserves angles and lengths, the Dynkin diagrams associated to two different bases for the same root system are isomorphic.
\begin{proposition}\label{root system determined by Dynkin diagram}
\mbox{}
\begin{itemize}
\item[(a)] A root system is irreducible if and only if its Dynkin diagram is connected.
\item[(b)] If the Dynkin diagrams of two root systems $\Phi_1$ and $\Phi_2$ are isomorphic, then $\Phi_1$ and $\Phi_2$ themselves are isomorphic.
\end{itemize}
\end{proposition}
\begin{proof}
If a root system $\Phi$ decomposes into two orthogonal sets $\Phi_1$ and $\Phi_2$, then it is obvious that its Dynkin diagram is not connected, since there is no edge from any vertex of $\Phi_1$ to that of $\Phi_2$. The converse is also easy to see. The second claim is clear since a root system is determined by its Cartan matrix, which in turn is determined by the Dynkin diagram.
\end{proof}
\begin{corollary}
Let $\g$ be a semisimple Lie algebra, $\h\sub\g$ be a splitting Cartan subalgebra of $\g$, and let $\Phi$ be the root system of $\g$ relative to $\h$. Then $\g$ is simple if and only if the Dynkin diagram of $\Phi$ is connected.
\end{corollary} 
\subsection{Integral and dominant integral elements}
We now introduce a notion of integrality for elements of $E$. In the setting of the representations of a semisimple Lie algebra $\g$, the weights of a finite-dimensional representation of $\g$ are always integral elements.
\begin{definition}
An element $\lambda\in E$ is an \textbf{integral element} if for all $\alpha\in\Phi$, we have
\[\langle\lambda,\alpha\rangle\in\Z.\]
If $\Delta$ is a base for $\Phi$, an element $\lambda\in E$ is \textbf{dominant} (relative to $\Delta$) if
\[\langle\lambda,\alpha\rangle\geq 0\]
for all $\alpha\in\Delta$ and \textbf{strictly dominant} if the inequality is strict.
\end{definition}
We denote the set of integral elements in $E$ by $\Lambda$, and dominant integral element by $\Lambda^+$. Since every element in $\Phi$ is an intergral linear combination of elements in $\Delta$, we see an element $\lambda\in E$ is integral iff $\langle\lambda,\alpha\rangle\in\Z$ for all $\alpha\in\Delta$. Also, $\lambda$ is strictly dominant relative to $\Delta$ if and only if $\lambda$ is contained in the fundamental Weyl chamber $C(\Delta)$ associated to $\Delta$, and $\lambda$ is dominant if and only if $\lambda\in\widebar{C(\Delta)}$. Since $\widebar{C(\Delta)}$ is the fundamental domain of $W$ on $E$ (Proposition~\ref{root system fundamental domain of Weyl group}), any element $\lambda\in E$ is $W$-conjugate to a unique dominant element.\par
Note that by the definition of a root system, every root is an integral element. Thus, every integer linear combination of roots is also an integral element. We denote by $\Lambda_r$ the lattice in $\Lambda$ generated by roots in $\Phi$, called the \textbf{root lattice}. In most cases, however, there exist integral elements that are not expressible as an integer combination of roots.
\begin{definition}
Let $\Delta=\{\alpha_1,\dots,\alpha_r\}$ be a base. Then the \textbf{fundamental weights} (relative to $\Delta$) are the elements $\lambda_1,\dots,\lambda_r$ with the property that
\[\langle\lambda_i,\alpha_j\rangle=\delta_{ij}.\]
\end{definition}
Elementary linear algebra shows the fundamental weights always exist. Geometrically, the $j$-th fundamental weight is the unique element of $E$ that is orthogonal to each $i$, $i\neq j$, and whose orthogonal projection onto $\alpha_j$ is one-half of $\alpha_j$. Note that the set of dominant integral elements is precisely the set of linear combinations of the fundamental weights with \textit{non-negative integer} coefficients, and the set of all integral elements is the set of linear combinations of fundamental weights with arbitrary \textit{integer} coefficients.
\begin{definition}
Let $\Delta$ be a base for $\Phi$ and $\Phi^+$ the associated set of positive roots. We define the \textbf{Weyl vector} $\delta$ to be the half the sum of the positive roots:
\[\delta=\frac{1}{2}\sum_{\beta\in\Phi^+}\beta.\]
The element $\delta$ plays a key role in many of the developments in the sequel.
\end{definition}
\begin{lemma}\label{root system half sum of positive is integral}
The Weyl vector $\delta$ is a strictly dominant integral element; indeed, $\langle\delta,\alpha\rangle=1$ for each $\alpha\in\Delta$, so we have
\[\delta=\lambda_1+\cdots+\lambda_r\]
for any pair of fundamental weights $\{\lambda_1,\dots,\lambda_r\}$.
\end{lemma}
\begin{proof}
In Corollary~\ref{root system half sum of positive under reflection} we have already shown that $s_\alpha(\delta)=\delta-\alpha$ for each $\alpha\in\Delta$. From the expression of $s_\alpha$, this implies $\langle\delta,\alpha\rangle=1$. Finally, let $\Delta=\{\alpha_1,\dots,\alpha_r\}$ be a base for $\Phi$ and $\{\lambda_1,\dots,\lambda_r\}$ be the fundamental weights. Then if $\rho:=\lambda_1+\cdots+\lambda_r$, then we have
\[\langle\delta-\rho,\alpha_i\rangle=0\for i=1,\dots,r.\]
By the definition of $\langle\cdot,\cdot\rangle$, it follows that $(\delta-\rho,\alpha_i)=0$ for all $i$, and thus $\delta=\rho$ (since $\Delta$ spans $E$).
\end{proof}
We now introduce a partial ordering on the set of integral elements, which will be used later to formulate the theorem of the highest weight for representations of a semisimple Lie algebra.
\begin{definition}
Let $\Delta$ be a basis for $\Phi$ and $\lambda,\mu\in E$. Then we say $\lambda$ is \textbf{higher} than $\mu$, written $\lambda\preceq\mu$, if $\mu-\lambda$ is a nonnegative $\R$-linear combination of elements in $\Delta$. We equivalently say that $\mu$ is \textbf{lower} than $\lambda$.
\end{definition}
Note that this order already appears in the proof of Proposition~\ref{root system fundamental domain of Weyl group}, and it makes sense becasue $E$ is spanned by $\Delta$. We now develop various useful properties of this relation. From now on, we assume a basis $\Delta$ for $\Phi$ has been chosen, and that the notions of higher, lower, and dominant are defined relative to $\Delta$.
\begin{proposition}\label{root system dominant is higher than 0}
If $\lambda\in E$ is dominant, then $\lambda\succeq 0$.
\end{proposition}
For any basis $\{\alpha_1,\dots,\alpha_r\}$ of $E$, we can form a dual basis $\{\alpha_1^*,\dots,\alpha_r^*\}$ of $E$ by the relation $(\alpha_i^*,\alpha_j)=\delta_{ij}$. For proving Proposition~\ref{root system dominant is higher than 0}, the following fact for dual basis is needed.
\begin{lemma}\label{root system dual basis is acute}
Suppose $\{\alpha_1,\dots,\alpha_r\}$ is an obtuse basis for $E$, meaning that $\langle\alpha_i,\alpha_j\rangle\leq 0$ for all $i\neq j$. Then $\{\alpha_1^*,\dots,\alpha_r^*\}$ is an acute basis for $E$, meaning that $\langle\alpha_i^*,\alpha_j^*\rangle\geq 0$.
\end{lemma}
\begin{proof}
Let $\Delta=\{\alpha_1,\dots,\alpha_r\}$ and $\{\alpha_1^*,\dots,\alpha_r^*\}$ be the dual basis with respect to $(\cdot,\cdot)$. Any vector $u$ can be expanded as $u=\sum_ic_i\alpha_i$ and the coefficients may be computed as $c_i=(\alpha_i^*,\mu)$. Applying this with $\alpha_j^*$ gives
\[\alpha_j^*=\sum_{i=1}^{r}(\alpha_i^*,\alpha_j^*)\alpha_i.\]
Let $G=((\alpha_i,\alpha_j))$ and $H=((\alpha_i^*,\alpha_j^*))$. Then from the equality above we see
\[(\alpha_1^*,\cdots,\alpha_r^*)=(\alpha_1,\cdots,\alpha_r)H\]
Simialrly, we can show that
\[(\alpha_1,\cdots,\alpha_r)=(\alpha_1^*,\cdots,\alpha_r^*)G\]
This proves $G=H^{-1}$. That is, the Gram matrix of the dual basis is the inverse of the Gram matrix of the original basis.\par
We know that $(\alpha_i,\alpha_j)\leq 0$ for $i\neq j$. Now we prove that $(\alpha_i^*,\alpha_j^*)\geq$ for $i\neq j$ by induction. The case $r=2$ is more or less obvious, via an explicit expression on the inversion of a matrix. Assume now that the result holds in dimension smaller than $r\geq 3$ and consider the case of dimension $r$. Fix an index $k$ and let $P$ be the orthogonal projection onto the orthogonal complement of $v_k$, which is given by
\[P(u)=u-\frac{(u,v_k)}{(v_k,v_k)}v_k.\]
The operator $P$ is easily seen to be self-adjoint, meaning that $(Pu,v)=(u,Pv)$.\par
We now claim that $Pv_1,\dots,\widehat{Pv_k},\dots,Pv_{r}$ is an obtuse basis for $\{v_k\}^\bot$. Indeed, a little algebra shows that
\[(Pv_i,Pv_j)=(v_i,v_j)-\frac{(v_k,v_i)(v_k,v_j)}{(v_k,v_k)}\leq 0,\]
since $(v_i,v_j)$, $(v_i,v_k)$, and $(v_j,v_k)$ are all less than or equal to zero. Furthermore, for $i$ and $j$ different from $r$, we have 
\[(v_i^*,Pv_j)=(Pv_i^*,v_j)=(v_i^*,v_j)=\delta_{ij}\]
since $v_i^*$ is orthogonal to $v_k$. Thus, the dual basis to $\{Pv_1,\dots,\widehat{Pv_k},\dots,Pv_{r}\}$ consists simply of the vectors $\{v_1^*,\dots,\widehat{v}_k^*,\dots,v_{r}^*\}$ (all of which are orthogonal to $v_k$).\par
Now fix any two distinct indices $i$ and $j$. Since $r\geq 3$, we can choose some other index $k$ distinct from both $i$ and $j$. Applying our induction hypothesis to the corresponding basis for $\{v_k\}^\bot$, we conclude that $(v_i^*,v_j^*)\geq 0$, which is what we are trying to prove.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{root system dominant is higher than 0}]
If $\lambda$ is dominant, the coefficients in the expansion $\lambda=\sum_jc_j\alpha_j$ are given by
\[c_j=(\alpha_j^*,\lambda)=\sum_{i=1}^{r}(\alpha_j^*,\alpha_i^*)(\alpha_i,\lambda).\]
Since $\mu$ is dominant, we have $(\alpha_i,\lambda)\geq 0$. Furthermore, the $\alpha_i^*$'s form an obtuse basis for $E$ and thus $(\alpha_i^*,\alpha_j^*)\geq 0$ for each $i,j$. This proves $c_j\geq 0$, hence $\lambda\succeq 0$.
\end{proof}
\begin{proposition}\label{root system dominant is higher in W-conjugation}
If $\lambda$ is dominant, then $w(\lambda)\preceq\lambda$ for all $w\in W$.
\end{proposition}
\begin{proof}
By Proposition~\ref{root system fundamental domain of Weyl group}, each orbit contains exactly one dominant element, which is also the maximal element. This shows $\lambda$ is the unique maximal element in its orbit, and thus is higher than any other ones.
\end{proof}
\begin{proposition}\label{root system strict dominant bigger than delta}
If $\mu$ is a strictly dominant integral element, then $\mu\succeq\delta$.
\end{proposition}
\begin{proof}
Since $\mu$ is strictly dominant, $\mu-\delta$ will still be dominant in light of Lemma~\ref{root system half sum of positive is integral}. Thus, by Proposition~\ref{root system dominant is higher than 0}, $\mu-\delta\succeq 0$, which is equivalent to $\mu\succeq\delta$.
\end{proof}
Recall that the convex hull of a subset $S\sub E$ is the smallest convex set containing $E$. For $\mu\in E$, we let $W\cdot\mu$ denote the Weyl-group orbit of $\mu\in E$ and we let $\conv(W\cdot\mu)$ denote the convex hull of $W\cdot\mu$.
\begin{proposition}\label{root system conv hull of dominant char}
Let $\lambda\in E$ be dominant. Then
\[\mathrm{conv}(W\cdot\lambda)=\{\mu\in E:w(\mu)\preceq\lambda\text{ for all $w\in W$}\}.\]
\end{proposition}
By the characterization of the convex hull, we see $\conv(W\cdot\lambda)$ is Weyl invariant. Thus if $\mu$ belongs to $\conv(W\cdot\lambda)$, then every point in $\conv(W\cdot\mu)$ also belongs to $\conv(W\cdot\lambda)$. Then the proposition may be restated as follows: if $\lambda$ and $\mu$ are dominant, then
\[\mu\preceq\lambda\iff\conv(W\cdot\mu)\sub\conv(W\cdot\lambda).\]
We establish two lemmas that will lead to a proof of Proposition~\ref{root system conv hull of dominant char}.
\begin{lemma}\label{root system convex set separation lemma}
Suppose $K$ is a compact, convex subset of $E$ and $\mu$ is an element of $E$ that it is not in $K$. Then there is an element $\gamma$ of $E$ such that we have $(\gamma,\mu)>(\gamma,\eta)$ for all $\eta\in K$.
\end{lemma}
\begin{proof}
Since $K$ is compact, we can choose an element $\eta_0$ of $K$ that minimizes the distance to $\mu$. Set $\gamma=\mu-\eta_0$, so that
\[(\gamma,\mu-\eta_0)=(\mu-\eta_0,\mu-\eta_0)\geq 0.\]
and thus $(\gamma,\mu)\geq(\gamma,\eta_0)$.\par
Now, for any $\eta\in K$, the vector $\eta_0+t(\eta-\eta_0)$ belongs to $K$ for $0\leq t\leq 1$, and we compute that
\[\|\eta_0+t(\eta-\eta_0)-\mu\|^2=(\mu-\eta_0,\mu-\eta_0)-2t(\mu-\eta_0,\eta-\eta_0)+t^2(\eta-\eta_0,\eta-\eta_0).\]
The only way this quantity can be greater than or equal to $(\mu-\eta_0,\mu-\eta_0)$ for small positive $t$ is if
\[(\mu-\eta_0,\eta-\eta_0)=(\gamma,\eta-\eta_0)\leq 0.\]
Thus $(\gamma,\eta)\leq(\gamma,\eta_0)\leq(\gamma,\mu)$.
\end{proof}
\begin{lemma}\label{root system dominant not in convex hull lemma}
If $\lambda$ and $\mu$ are dominant and $\mu\notin\conv(W\cdot\lambda)$, there exists a dominant element $\gamma\in E$ such that
\[(\gamma,\mu)>(\gamma,w(\lambda))\]
for all $w\in W$.
\end{lemma}
\begin{proof}
By Lemma~\ref{root system convex set separation lemma}, we can find some $\gamma\in E$, not necessarily dominant, such that $(\gamma,\mu)\geq(\gamma,\eta)$ for all $\eta\in\conv(W\cdot\lambda)$. In particular, we have $(\gamma,\mu)>(\gamma,w(\lambda))$ for all $w\in W$. Choose some $w_0\in W$ so that $\gamma':=w_0(\gamma)$ is dominant. We will show that replacing $\gamma$ by $\gamma'$ makes $(\gamma,\mu)$ bigger while permuting the values of $(\gamma,w(\lambda))$.\par
By Proposition~\ref{root system dominant is higher in W-conjugation}, $\gamma\preceq\gamma'$, meaning that $\gamma'$ equals $\gamma$ plus a non-negative linear combination of positive simple roots. But since $\mu$ is dominant, it has non-negative inner product with each positive simple root, and we see that $(\gamma',\mu)>(\gamma,\mu)$. Thus
\[(\gamma',\mu)>(\gamma,\mu)>(\gamma,w(\mu))\]
for all $w\in W$. On the other hand,
\[(\gamma',w(\lambda))=(w_0(\gamma),w(\lambda))=(\gamma,(w_0^{-1}w)(\lambda)).\]
Thus, as $w$ ranges over $W$, the values of $(\gamma,w(\lambda))$ and $(\gamma',w(\lambda))$ range through the same set of real numbers. Thus, $(\gamma',\mu)>(\gamma',w(\lambda))$ for all $w$ as claimed.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{root system conv hull of dominant char}]
Since each $\mu\in E$ is $W$-conjugate to a dominant element which is maximal in the orbit of $\mu$, we may assume that $\mu$ is dominant. Assume first that $\mu\in\conv(W\cdot\lambda)$. By Proposition~\ref{root system dominant is higher in W-conjugation}, every element of the form $\sigma(\lambda)$ is lower than $\lambda$. But it is easy to see a convex combination of elements lower than $\lambda$ is still lower than $\lambda$, thus $\mu\preceq\lambda$.\par
Conversely, assume that $\mu\notin\conv(W\cdot\lambda$). Let $\gamma$ be a dominant element as in Lemma~\ref{root system dominant not in convex hull lemma}. Then since $(\gamma,\lambda)>(\gamma,\mu)$, we see $(\gamma,\lambda-\mu)<0$. Since $\gamma$ is dominant, this rules out the case $\mu\preceq\lambda$.
\end{proof}
\subsection{Examples of root systems}
\begin{example}[\textbf{The special linear Lie algebra}]
Let $\g=\sl_n(\C)=\{X\in\gl_n(\C):\tr(X)=0\}$. Then the subalgebra $\h$ of diagonal matrices in $\g$ is abelian, and for the matrix units $E_{ij}$ with a single nonzero entry $1$ in position $(i,j)$, we have
\[[\lambda,E_{ij}]=(\lambda_i-\lambda_j)E_{ij}\for\lambda\in\C^n.\]
We conclude that $\h$ is maximal abelian, $\g^0(\h)=\g_0(h)=\h$, so that $\h$ is a Cartan subalgebra, and that the one-dimensional subspace $\C E_{ij}$ is a root space corresponding to the root $\eps_i-\eps_j$, where $\eps_i(\lambda):=\lambda_i$. The corresponding root system is
\[A_{n-1}:=\{\eps_i-\eps_j:1\leq i\neq j\leq n\}.\]
\end{example}
\section{Representation theory of Lie algebras}
\subsection{The universal enveloping algebra}
Representing a Lie algebra by linear maps leads to a mapping of the Lie algebra into an associative algebra such that the Lie bracket turns into the commutator bracket. A priori it is not clear that an injective map of this type exists, not even if we allow the associative algebra to be infinite-dimensional. The point of the enveloping algebra $U(\g)$ of a Lie algebra $\g$ is that every representation of $\g$ on $V$ factors through a homomorphism $U(\g)\to\End(V)$ of associative algebras.
\begin{definition}
Let $\g$ be a Lie algebra. A pair $(U(\g),\iota)$, consisting of a unital associative algebra $U(\g)$ and a homomorphism $\iota:\g\to U(\g)$ of Lie algebras, is called a \textbf{(universal) enveloping algebra} of $\g$ if it has the following universal property. For each homomorphism $\varphi:\g\to \mathfrak{Lie}(A)$ of g into the Lie algebra $A$, where $A$ is a unital associative algebra, there exists a unique homomorphism $\widetilde{\varphi}:U(\g)\to A$ of unital associative algebras with $\widetilde{\varphi}\circ\iota=\varphi$.
\[\begin{tikzcd}
\g\ar[r,"\varphi"]\ar[d,"\iota"]&A\\
U(\g)\ar[ru,swap,"\widetilde{\varphi}"]
\end{tikzcd}\]
\end{definition}
The universal property determines a universal enveloping algebra uniquely in the following sense:
\begin{lemma}[\textbf{Uniqueness of the Enveloping Algebra}]
If $(U(\g),\iota)$ and $(\widetilde{U}(\g),\tilde{\iota})$ are two enveloping algebras of the Lie algebra $\g$, then there exists an isomorphism $\varphi:U(\g)\to\widetilde{U}(\g)$ of unital associative algebras satisfying $\varphi\circ\iota=\tilde{\iota}$.
\end{lemma}
\begin{proof}
Since $\tilde{\iota}:\g\to\widetilde{U}(\g)$ is a homomorphism of Lie algebras, the universal property of the pair $(U(\g),\iota)$ implies the existence of a unique algebra homomorphism $\varphi:U(\g)\to\widetilde{U}(\g)$ with $\varphi\circ\iota=\tilde{\iota}$. Similarly, the universal property of $(\widetilde{U}(\g),\tilde{\iota})$ implies the existence of an algebra homomorphism $\psi:\widetilde{U}(\g)\to U(\g)$ with $\psi\circ\tilde{\iota}=\iota$. Then $\psi\circ\varphi:U(\g)\to U(\g)$ is an algebra homomorphism with $(\psi\circ\varphi)\circ\iota=\iota$, so that the uniqueness part of the universal property of $(U(\g),\iota)$ yields $\psi\circ\varphi=\id_{U(\g)}$. We likewise get $\varphi\circ\psi=\id_{\widetilde{U}(\g)}$, showing that $\varphi$ is an isomorphism of unital algebras.
\end{proof}
\begin{proposition}[\textbf{Existence of the Enveloping Algebra}]
Each Lie algebra $\g$ has an enveloping algebra $(U(\g),\iota)$.
\end{proposition}
\begin{proof}
Let $T(\g)$ be the tensor algebra of $\g$ and consider the ideal $J$ of $T(\g)$ generated by the set
\[\{x\otimes y-y\otimes x-[x,y]:x,y\in\g\}.\]
Then $U(\g)=T(\g)/J$ is a unital associative algebra and the map
\[\iota:\g\to U(\g),\quad x\mapsto x+J\]
is a linear map satisfying
\[\iota([x,y])=[x,y]+J=x\otimes y-y\otimes x+J=[\iota(x),\iota(y)].\]
so that $\iota$ is a homomorphism of Lie algebras $\g\to\mathfrak{Lie}(U(\g))$.\par
To verify the universal property for $(U(\g),\iota)$, let $\varphi:\g\to \mathfrak{Lie}(A)$ be a homomorphism of Lie algebras, where $A$ is a unital associative algebra. In view of the universal property of $T(\g)$, there exists an algebra homomorphism $\hat{\varphi}:T(\g)\to A$ with $\hat{\varphi}(x)=\varphi(x)$ for all $x\in\g$. Then $J\sub\ker\hat{\varphi}$ and so $\hat{\varphi}$ factors through an algebra homomorphism $\widetilde{\varphi}:U(\g)\to A$ with $\widetilde{\varphi}\circ\iota=\varphi$. To see that $\widetilde{\varphi}$ is unique, it suffices to note that $\iota(\g)$ and $1$ generate $U(\g)$ as an associative algebra because $\g$ and $1$ generate $T(\g)$ as an associative algebra.
\end{proof}
Note that the universal property of $(U(\g),\iota)$ implies that each representation $(\pi,V)$ of $\g$ defines a representation $\widetilde{\pi}:U(\g)\to\End(V)$, which is uniquely determined by $\widetilde{\pi}\circ\iota=\pi$. From the construction of $U(\g)$, we also know that the algebra $U(\g)$ is generated by $\iota(\g)$. This implies that for each $v\in V$, the subspace $U(\g)v\sub V$ is the smallest subspace containing $v$ and invariant under $\g$, i.e., the $\g$-submodule of $V$ generated by $v$. Hence the enveloping algebra provides a tool to understand $\g$-submodules of a $\g$-module. But before we are able to use this tool effectively, we need some more information on the structure of $U(\g)$.\par
Note that the canonical map $\iota:\g\to U(\g)$ by definition is a homomorphism of Lie algebras. But we do not know yet if it is injective, so that we obtain an embedding of $\g$ into an associative algebra. Our next goal is the Poincar\'e-Birkhoff-Witt Theorem which entails in particular that $\iota$ is injective.
Let $\{x_1,\dots,x_n\}$ be a basis for $\g$ and set $\xi_i:=\iota(x_i)$. For a finite sequence $I=(i_1,\dots,i_k)$ of natural numbers between $1$ and $n$, we set $\xi_I=\xi_{i_1}\cdots\xi_{i_k}$. If $i\in\N$, then we write $i\leq I$ if $i\leq i_j$ for all $j=1,\dots,k$. We write $U_p(\g):=\sum_{i\leq p}\iota(\g)^i$ and note that these subspaces are all finite-dimensional and satisfy
\[U_p(\g)U_q(\g)\sub U_{p+q}(\g)\for p,q\in\N.\]
Next we construct a suitable basis for $U(\g)$.
\begin{lemma}\label{Lie algebra enveloping algebra permutation lemma}
Let $y_1,\dots,y_p\in\g$ and $\sigma\in\mathfrak{S}_p$, then
\[\iota(y_1)\cdots\iota(y_p)-\iota(y_{\sigma(1)})\cdots\iota(y_{\sigma(p)})\in U_{p-1}(\g).\]
\end{lemma}
\begin{proof}
Since every permutation is a composition of transpositions of neighboring elements, it suffices to prove the claim for $\sigma=(i,i+1)$. But then we have
\begin{align*}
\iota(y_1)\cdots\iota(y_p)-\iota(y_{\sigma(1)})\cdots\iota(y_{\sigma(p)})&=\iota(y_1)\cdots[\iota(y_i)\iota(y_{i+1})-\iota(y_{i+1})\iota(y_i)]\cdots\iota(y_p)\\
&=\iota(y_1)\cdots\iota([y_i,y_{i+1}])\cdots\iota(y_p)\in U_{p-1}(\g).
\end{align*}
This proves the claim.
\end{proof}
\begin{lemma}\label{Lie algebra enveloping algebra generating p-length}
The vector space $U_p(\g)$ is spanned by the $\xi_I$ with increasing sequences $I$ of length less than or equal to $p$.
\end{lemma}
\begin{proof}
It is clear that $U_p(\g)$ is spanned by the $\xi_I$ for all sequences $I$ of length less than or equal to $p$. By induction on $p$, we have the claim for $U_{p-1}(\g)$. But since for a rearrangement $I'$ of the sequence $I$, we have $\xi_I-\xi_{I'}\in U_{p-1}(\g)$ by Lemma~\ref{Lie algebra enveloping algebra permutation lemma}, we also obtain the claim for $U_p(\g)$.
\end{proof}
Let $P:=\K[z_1,\dots,z_n]$ be the associative algebra of polynomials over $\K$ in the commuting variables $z_1,\dots,z_n$. For $i\in\N$, let $P_i$ be the set of all polynomials of degree less or equal to $i$. As in $U(\g)$, we write $z_I=z_{i_1}\cdots z_{i_k}$ for a finite sequence $I$ of natural numbers between $1$ and $n$. For the empty sequence, we set $z_\emp=1$.
\begin{proposition}\label{Lie algebra act on polynomial ring}
There exists a $\g$-module structure on $P$ with $x_i\cdot z_I=z_iz_I$ for $i\leq I$.
\end{proposition}
\begin{proof}
By induction on $k\in\N$, we construct linear maps $\rho_k:\g\to\Hom(P_k,P_{k+1})$ with the following properties:
\begin{itemize}
\item[(a)] $\rho_k(x_i)z_I=z_iz_I$ for $i\leq I$ and $z_I\in P_k$.
\item[(b)] $\rho_k(x_i)z_I-z_iz_I\in P_j$ for $z_I\in P_j$ and $j\leq k$.
\item[(c)] $\rho_k(x_i)\rho_k(x_j)z_J-\rho_k(x_j)\rho_k(x_i)z_J=\rho_k([x_i,x_j])z_J$ for $z_J\in P_{k-1}$.
\item[(d)] $\rho_k(x)|_{P_{k-1}}=\rho_{k-1}(x)$ for $x\in\g$.
\end{itemize}
To simplify the notation, we will use $x_i(z_I)$ to denote $\rho_k(x_i)z_I$, and $\rho_k(x_i)\rho_k(x_j)z_I$ will be abbreviated by $x_ix_j(z_I)$. For $k=0$, we put $x_i(1):=z_i$ for $i=1,\dots,k$. Then (a) and (b) are satisfied because $x_1,\dots,x_k$ is a basis for $\g$, and (c) and (d) are empty conditions.\par
Now we assume that we already have $\rho_{k-1}$. In view of (d), we have to show that the maps $\rho_{k-1}(x_i)$ can be extended to maps $P_k\to P_{k+1}$ so that (a)---(c) are satisfied. Since $P$ is commutative, the $z_I$ with increasing $I$ form a basis for $P$, so we only consider such sequences. Thus, let $I:=(i_1,\dots,i_k)$ be increasing. For $I_1:=(i_2,\dots,i_k)$, by (a) and (b), we have
\[z_I=z_{i_1}z_{I_1}=x_{i_1}(z_{I_1}),\quad x_i(z_{I_1})-z_iz_{I_1}\in P_{k-1}.\]
It is clear that for $i\leq I$ we must set $x_i(z_I)=z_iz_I$, so we only need to consider the case otherwise. Since $I$ is increasing, in this case we have $i_1<i$. Now we observe that, if the map $\rho_k$ is already defined, then by condition (c) we must have
\[x_i(z_I)=x_ix_{i_1}(z_{I_1})=x_{i_1}x_i(z_{I_1})+[x_i,x_{i_1}](z_{I_1}).\]
Also note that, since $i_1\leq(i,I_1)$, we have $x_{i_1}(z_iz_{I_1})=z_{i_1}z_iz_{I_1}=z_iz_I$. So the equation above can be written as
\[x_i(z_I)=z_iz_I+x_{i_1}[x_i(z_{I_1})-z_iz_{I_1}]+[x_i,x_{i_1}](z_{I_1}).\]
This is our definition of $x_i(z_I)$. Namely, we set
\[x_i(z_I):=\begin{cases}
z_iz_I&\text{if }i\leq I,\\
z_iz_I+x_{i_1}[x_i(z_{I_1})-z_iz_{I_1}]+[x_i,x_{i_1}](z_{I_1})&\text{otherwise}.
\end{cases}\]
For this definition, (a) and (b) are obviously satisfied. But we still have to verify (c).\par
Let $J$ be an increasing sequence such that $z_J\in P_{k-1}$. If $i\neq j$ and one of them is less than $J$, then by $[x_i,x_j]=-[x_j,x_i]$, we may assume that $j<i$ and $j\leq J$. With (a) and (b), we calculate that
\begin{equation*}
x_ix_j(z_J)=x_i(z_jz_J)=x_i(z_{(j,J)})=z_iz_jz_J+x_j[x_i(z_J)-z_iz_J]+[x_i,x_j](z_J).
\end{equation*}
On the other hand, since $j\leq(i,J)$, we have $x_j(z_iz_J)=z_jz_iz_J$, so
\begin{equation*}
x_jx_i(z_J)=x_j(z_iz_J)+x_j[x_i(z_J)-z_iz_J]=z_jz_iz_J+x_j[x_i(z_J)-z_iz_J].
\end{equation*}
Therefore (c) is satisfied in this case.\par
Now assume that $J=(j_1,\dots,j_{k-1})$ and $j_1<i,j$. We set $J_1=(j_2,\dots,j_{k-1})$. Since $j_1\leq J_1$, it follows from (a) and the previous argument that
\begin{align*}
x_j(z_J)=x_j(z_{j_1}z_{J_1})=x_jx_{j_1}(z_{J_1})=x_{j_1}x_j(z_{J_1})+[x_j,x_{j_1}](z_{J_1}).
\end{align*}
Note that $j_1\leq(j,J_1)$, so the previous argument implies
\[x_ix_{j_1}(z_jz_{J_1})=x_{j_1}x_i(z_jz_{J_1})+[x_i,x_{j_1}](z_jz_{J_1}).\]
On the other hand, since $z_{J_1}\in P_{k-2}$, we have $x_j(z_{J_1})-z_jz_{J_1}\in P_{k-2}$. So by condition (c) we have a similar equation for $x_ix_{j_1}[x_j(z_{J_1})-z_jz_{J_1}]$. From the definition of $x_j(z_{J_1})$, we get
\[x_ix_{j_1}x_j(z_{J_1})=x_{j_1}x_ix_j(z_{J_1})+[x_i,x_{j_1}]x_j(z_{J_1}).\]
Also, by (c), we have
\[x_i[x_j,x_{j_1}]z_{J_1}=[x_j,x_{j_1}]x_iz_{J_1}+[x_i,[x_j,x_{j_1}]]z_{J_1},\]
and therefore
\begin{equation}\label{Lie algebra act on polynomial ring-1}
\begin{aligned}
x_ix_j(z_J)=x_{j_1}x_ix_j(z_{J_1})+[x_i,x_{j_1}]z_{J_1}+[x_j,x_{j_1}]x_i(z_{J_1})+[x_i,[x_j,x_{j_1}]](z_{J_1}).
\end{aligned}
\end{equation}
Since $i,j$ are both larger than $j_1$, by exchanging $i$ and $j$ in $(\ref{Lie algebra act on polynomial ring-1})$ we get a similar expression for $x_jx_i(z_J)$. Now substract these two equations and apply the Jacobi identity and condition (c) for $\rho_{k-1}$, we get
\begin{align*}
x_ix_j(z_J)-x_jx_i(z_J)&=x_{j_1}x_ix_j(z_{J_1})-x_{j_1}x_jx_i(z_{J_1})+[x_i,[x_j,x_{j_1}]](z_{J_1})-[x_j,[x_i,x_{j_1}]](z_{J_1})\\
&=x_{j_1}[x_i,x_j](z_{J_1})+[x_i,[x_j,x_{j_1}]](z_{J_1})+[x_j,[x_{j_1},x_{i}]](z_{J_1})\\
&=[x_i,x_j]x_{j_1}(z_{J_1})+([x_{j_1},[x_i,x_j]]+[x_i,[x_j,x_{j_1}]]+[x_j,[x_{j_1},x_{i}]])(z_{J_1})\\
&=[x_i,x_j]x_{j_1}(z_{J_1})=[x_i,x_j](z_J).
\end{align*}
This completes our induction. In view of (d), we obtain a well-defined map $\rho:\g\to\gl(P)$ by $\rho|_{P_k}=\rho_k$ and (c) implies that it is a homomorphism of Lie algebras, so that we obtain on $P$ a $\g$-module structure, and (a) ensures that it has the required property.
\end{proof}
\begin{proposition}
The $\xi_I$ with increasing $I$ form a basis for $U(\g)$. In particular, the canonical map $\iota:\g\to U(\g)$ is injective.
\end{proposition}
\begin{proof}
Let $\rho:\g\to\End(P)$ be the Lie algebra homomorphism defining the module structure constructed in Proposition~\ref{Lie algebra act on polynomial ring} and $\tilde{\rho}:U(\g)\to\End(P)$ the algebra homomorphism determined by $\tilde{\rho}\circ\iota=\rho$. Then the property $\rho(x_i)z_I=z_iz_I$ for $i\leq I$ ensures that for $i_1\leq\cdots\leq i_k$ we have
\[\tilde{\rho}(\xi_{i_1}\cdots\xi_{i_k})(1)=\tilde{\rho}(\xi_{i_1})\cdots\tilde{\rho}(\xi_{i_k})(1)=\rho(x_{i_1})\cdots\rho(x_{i_k})(1)=z_{i_1}\cdots z_{i_k}.\]
Then the linear map
\[\varphi:U(\g)\to P,\quad\xi\mapsto\tilde{\rho}(\xi)(1)\]
maps the set of the $\xi_I$ with increasing $I$ to the (linearly independent) set of the $z_I$ with increasing $I$. Therefore the $\xi_I$'s are linearly independent. The claim now follows from Lemma~\ref{Lie algebra enveloping algebra generating p-length}.
\end{proof}
\begin{theorem}[\textbf{Poincar\'e-Birkhoff-Witt Theorem}]
Let $\g$ be a Lie algebra and $\{x_1,\dots,x_n\}$ be a basis for $\g$. Then $\{x_1^{\mu_1}\cdots x_n^{\mu_n}:\mu_i\in\N\}$ is a basis for $U(\g)$.
\end{theorem}
\begin{corollary}\label{Lie algebra PBW Theorem on subalgebra}
Let $\h$ be a subalgebra of $\g$, and extend an ordered basi $\{h_1,\dots,h_r\}$ of $\h$ to an ordered basis $\{h_1,\dots,h_r,x_{r+1},\dots,x_n\}$ of $\g$. Then the homomorphism $U(\h)\to U(\g)$ induced by the injection $\h\to\g\to U(\g)$ is itself injective, and $U(\g)$ is a free $U(\h)$-module with basis $\{x_{r+1}^{\mu_1}\cdots x_n^{\mu_n}:\mu_i\in\N\}$.
\end{corollary}
\subsection{Generators and relations for semisimple Lie algebras}
In this part, we shall use the root decomposition of a semisimple Lie algebra to find a description by generators and relations (Serre's Theorem).
\subsubsection{A generating set for semisimple Lie algebras}
\begin{proposition}\label{Lie semisimple algebra Serre relation}
Let $\g$ be a semisimple Lie algebra and $\h\sub\g$ a splitting Cartan subalgebra. Fix a positive system $\Phi^+\sub\Phi$ and let $\Delta\sub\Phi^+$ be the set of simple roots. For each $\alpha\in\Delta$, we fix a corresponding $\sl_2$-triple $(h_\alpha,e_\alpha,f_\alpha)$. Then the following assertions hold:
\begin{itemize}
\item[(a)] The subspace $\n:=\sum_{\beta\in\Phi^+}\g_\beta$ is a nilpotent subalgebra generated by $\{e_\alpha:\alpha\in\Delta\}$.
\item[(b)] The Lie algebra $\g$ is generated by the $\sl_2$-subalgebras $\g(\alpha)$, $\alpha\in\Delta$, i.e., $\{h_\alpha,e_\alpha,f_\alpha:\alpha\in\Delta\}$ generates $\g$. These elements satisfy the relations
\begin{equation}\label{Serre relation-1}
\begin{aligned}
&[h_\alpha,h_\beta]=0,\quad [h_\alpha,e_\beta]=\beta(h_\alpha)e_\beta,\quad[h_\alpha,f_\beta]=-\beta(h_\alpha)f_\beta,\quad [e_\alpha,f_\beta]=\delta_{\alpha,\beta}h_\alpha,
\end{aligned}
\end{equation}
and if $\alpha\neq\beta$ in $\Delta$, we further have
\begin{align}\label{Serre relation-2}
\ad(e_\alpha)^{1-\beta(h_\alpha)}e_\beta=0,\quad \ad(f_\alpha)^{1-\beta(h_\alpha)}f_\beta=0.
\end{align}
The relations $(\ref{Serre relation-1})$ and $(\ref{Serre relation-2})$ are called the \textbf{Serre relations}.
\end{itemize}
\end{proposition}
\begin{proof}
If $\beta,\gamma\in\Phi^+$, then either $\beta+\gamma\in\Phi^+$ or $\beta+\gamma$ is not a root. Hence $\n$ is a subalgebra of $\g$. Let $\lambda\in\h^*$ be a regular element of $\Phi$ such that $\Phi^+=\Phi^+(\lambda)$, and let
\[m=\min\{(\beta,\lambda):\beta\in\Phi^+\},\quad M=\max\{(\beta,\lambda):\beta\in\Phi^+\}.\]
For any $n\in\N$ with $nm>M$ we then have $C^n(\n)=\{0\}$, showing that $\n$ is nilpotent.\par
In view of Corollary~\ref{root system successive sum into positive root}, every $\beta\in\Phi^+$ can be written in the form $\alpha_1+\cdots+\alpha_n$ with $\alpha_i\in\Delta$ such that $\sum_{i=1}^{k}\alpha_i\in\Phi^+$ for each $k\in\{1,\dots,n\}$. Then Proposition~\ref{Lie algebra semisimple root decomposition prop} implies that
\[\g_\beta=[\g_{\alpha_n},[\g_{\alpha_{n-1}},[\cdots,[\g_{\alpha_2},\g_{\alpha_1}]]]\]
and this proves (a).\par
From (a) we also derive that $\k=\sum_{\beta\in\Phi^-}\g_\beta$ is generated by $\{f_\beta:\beta\in\Delta\}$. Therefore, the Lie algebra generated by the $\g(\alpha)$, $\alpha\in\Delta$ contains all root spaces. Since $\h^*=\mathrm{span}\,\Delta$, we have $\h=\mathrm{span}\{h_\alpha:\alpha\in\Delta\}$, so $\g$ is generated by $\g(\alpha)$, $\alpha\in\Delta$.\par
It remains to verify the Serre relation. Since $h_\alpha,h_\beta\in\h$, we have $[h_\alpha,h_\beta]=0$. Let $y\in\g_{-\beta}$, then we note that
\[\kappa([h_\alpha,e_\beta],y)=\kappa(h_\alpha,[e_\beta,y])=\kappa(e_\beta,y)\kappa(h_\alpha,h_\beta')=\beta(h_\alpha)\kappa(e_\beta,y).\]
Since $\kappa$ is nondegenerate on $\g_{-\beta}$, we have $[h_\alpha,e_\beta]=\beta(h_\alpha)e_\beta$. Similarly we can show that $[h_\alpha,f_\beta]=-\beta(h_\alpha)f_\beta$. Finally, we note that $\alpha-\beta\notin\Phi$ for $\alpha\neq\beta\in\Delta$, so $[e_\alpha,f_\beta]=0$ in this case.\par
If $\alpha\neq\beta$, then let $V$ be the $\g(\alpha)$-submodule of $\g$ generated by $f_\beta$. Since $[e_\alpha,f_\beta]=0$ and $[h_\alpha,f_\beta]=-\beta(h_\alpha)f_\beta$, by Proposition~\ref{sl_2(K) representation submodule} we have $M\cong L(-\beta(h_\alpha))$, and in particular $\dim M=1-\beta(h_\alpha)$. Thus
\[\ad(f_\alpha)^{1-\beta(h_\alpha)}f_\beta=\ad(f_\alpha)^{\dim M}f_\beta=0.\]
The first relation in $(\ref{Serre relation-2})$ can be obtained similarly, with the $\sl_2$-triple $(-h_\alpha,f_\alpha,e_\alpha)$ and the $\g(\alpha)$-submodule generated by $e_\beta$.
\end{proof}
\begin{example}
We have seen how to find a natural root decomposition of the Lie algebra $\sl_n(\K)$ with respect to the Cartan subalgebra $\h$ of diagonal matrices. In the root system
\[\Phi=\{\eps_i-\eps_j:1\leq i\neq j\leq n\},\]
the subset
\[\Phi^+:=\{\eps_i-\eps_j:1\leq i<j\leq n\}\]
is a natural positive system with root basis
\[\Delta=\{\eps_1-\eps_2,\quad,\eps_{n-1}-\eps_n\}.\]
Then
\[\n=\sum_{\alpha\in\Phi^+}\g_\alpha=\mathrm{span}\{E_{ij}:i<j\}\]
is the Lie algebra of strictly upper triangular matrices. It is generated by the root vectors $E_{i,i+1}$, $i=1,\dots,n-1$. For each pair of indices $i\neq j$, we have
\[\g(\eps_i-\eps_j)=\mathrm{span}\{E_{ij},E_{ji},E_{ii}-E_{jj}\}\cong\sl_2(\K),\]
and the subalgebras $\g(\eps_1-\eps_2),\dots,\g(\eps_{n-1}-\eps_n)$ sitting on the diagonal, generate $\sl_n(\K)$.
\end{example}
\subsubsection{Free Lie algebra}
Free Lie algebras are defined via universal properties. Their existence then has to be proved separately. The point of introducing them is the possibility to describe Lie algebras via generators and relations.
\begin{definition}
Let $X$ be a set. A pair $(L,\eta)$ of a Lie algebra $L$ and a map $\eta:X\to L$ is said to be a \textbf{free Lie algebra over $\bm{X}$} if it has the following universal property. For each map $\alpha:X\to\g$ into a Lie algebra $\g$, there exists a unique morphism $\tilde{\alpha}:L\to\g$ of Lie algebras with $\tilde{\alpha}=\alpha\circ\eta$.
\[\begin{tikzcd}
L\ar[r,dashed,"\exists!\tilde{\alpha}"]&\g\\
X\ar[ru,swap,"\alpha"]\ar[u,"\eta"]&
\end{tikzcd}\]
\end{definition}
For two free Lie algebras $(L_1,\eta_1)$ and $(L_2,\eta_2)$ over $X$ there exists a unique isomorphism $\varphi:L_1\to L_2$ with $\varphi\circ\eta_1=\eta_2$. In fact, we choose $\varphi$ as the unique morphism of Lie algebras with $\varphi\circ\eta_1=\eta_2$. Then the unique morphism of Lie algebras $\psi:L_2\to L_1$ with $\psi\circ\eta_2=\eta_1$ satisfies $\psi\circ\varphi\circ\eta_1=\eta_1$, so that the uniqueness in the universal property implies $\psi\circ\varphi=\id_{L_1}$, and likewise $\varphi\circ\psi=\id_{L_2}$.
\begin{remark}
If $(L,\eta)$ is a free Lie algebra over $X$, then $L$ is generated as a Lie algebra by $\eta(X)$. To see this, let $L_0\sub L$ be the Lie subalgebra generated by $\eta(X)$ and $\eta_0:X\to L_0$ be the corestriction of $\eta$. Then $(L_0,\eta_0)$ also has the universal property, so the existence of an isomorphism $\varphi:L_0\to L$ with $\varphi\circ\eta_0=\eta$. Since $L_0$ is generated by $\eta_0(X)=\eta(X)$, the Lie algebra $L$ is generated by $\eta(X)$, which leads to $L=L_0$.
\end{remark}
\begin{proposition}
For each set $X$, there exists a free Lie algebra $(L(X),\eta)$ over $X$.
\end{proposition}
\begin{proof}
Let $X$ be a set. We set $X_1=X$ and
\[X_n=\bigcup_{p+q=n,p,q\in\N}X_p\times X_q.\]
The \textbf{free magma} $M(X)$ is defined as the disjoint union $M(X)=\bigcup_nX_n$. One should think of it as a set containing all possible nonassociative products of elements of $X$. The maps
\[X_n\times X_m\to X_{n+m},\quad(x,y)\mapsto x\cdot y:=(x,y)\]
can be put together to a multiplication
\[M(X)\times M(X)\to M(X),\quad (x,y)\mapsto x\cdot y.\]
The free vector space $AM(X):=\K[M(X)]$, with basis $M(X)$, thus inherits a bilinear multiplication extending the multiplication on $M(X)$. Let $\n\sub AM(X)$ denote the two sided ideal generated by the expressions of the form
\[Q(x)=x\cdot x,\quad x\in AM(X),\]
and
\[J(x,y,z)=x\cdot(y\cdot z)+x\cdot(y\cdot z)+x\cdot(y\cdot z),\quad x,y,z\in AM(X).\]
We claim that the quotient algebra $L(X):=AM(X)/\n$ is a free Lie algebra over $X$ with respect to the map
\[\eta:X\to L(X),\quad x\mapsto x+\n.\]
First, we note that the quotient space $L(X)$ inherits a bilinear multiplication turning it into a Lie algebra. We have to show that for each map $\varphi:X\to\g$ into a Lie algebra $\g$ there exists a unique Lie algebra homomorphism $L(\varphi):L(X)\to\g$ with $L(\varphi)(x+\n)=\varphi(x)$ for each $x\in X$. In fact, one inductively defines mappings
\[\varphi_n:X_n\to\g,\quad (X,y)\mapsto[\varphi_p(x),\varphi_q(x)],\quad (x,y)\in X_p\times X_q,p+q=n.\]
and puts them together to a map $\varphi:M(X)\to\g$ satisfying $\varphi(x\cdot y)= [\varphi(x),\varphi(y)]$. This map extends uniquely to a linear map $AM(X)\to\g$ with the same property, which we still denote by $\varphi$. Since $\g$ is a Lie algebra, the map $\varphi$ factors to a Lie algebra homomorphism $L(\varphi):L(X)\to\g$ which is uniquely determined by $L(\varphi)(x+\n)=\varphi(x)$.
\end{proof}
In the following, we denote a free Lie algebra over $X$ by $(L(X),\eta)$. Let $X$ be a set, $R\sub L(X)$ a subset and $I_R\sub L(X)$ the ideal generated by $R$. Then $L(X,R):=L(X)/I_R$ is called the \textbf{Lie algebra defined by the generators $\bm{X}$ and the relations $\bm{R}$}.
\begin{example}
If $X=\{p,q,z\}$ and $R=\{[p,q]-z,[p,z],[q,z]\}$, then $L(X,R)$ is the $3$-dimensional Heisenberg Lie algebra. It is defined by the generators $p,q,z$ and the relations $[p,q]=z$, $[p,z]=[q,z]=0$.
\end{example}
\subsubsection{Serre's theorem}
Now we return to the situation of Proposition~\ref{Lie semisimple algebra Serre relation}. Let $X=\{\hat{h}_\alpha,\hat{e}_\alpha,\hat{f}_\alpha:\alpha\in\Delta\}$ and consider in the free Lie algebra $L(X)$ the relations
\begin{align*}
R:=&\{[\hat{h}_\alpha,\hat{h}_\beta],[\hat{h}_\alpha,\hat{e}_\beta]-\beta(h_\alpha)\hat{e}_\beta,[\hat{h}_\alpha,\hat{f}_\beta]+\beta(h_\alpha)\hat{f}_\beta,[\hat{e}_\alpha,\hat{f}_\beta]-\delta_{\alpha,\beta}h_\alpha\}\\
&\cup\{\ad(\hat{e}_\alpha)^{1-\beta(h_\alpha)}\hat{e}_\beta,\ad(\hat{f}_\alpha)^{1-\beta(h_\alpha)}\hat{f}_\beta:\alpha\neq\beta\in\Delta\}.
\end{align*}
Then Proposition~\ref{Lie semisimple algebra Serre relation} implies the existence of a unique surjective Lie algebra homomorphism
\[q:\hat{\g}:=L(X,R)\to\g\quad\text{with}\quad q(\hat{h}_\alpha)=h_\alpha,\quad q(\hat{e}_\alpha)=e_\alpha,\quad q(\hat{f}_\alpha)=f_\alpha.\]
The goal of this part is to prove Serre's Theorem that $q$ is an isomorphism, i.e., that the Lie algebra $\g$ is defined by the generating set $X$ and the relations $R$. Let $\hat{\g}_+$, $\hat{\g}_-$ and $\hat{\h}$ be the Lie subalgebras of $\hat{\g}$ generated by the $\hat{e}_\alpha$'s, $\hat{f}_\alpha$'s, and $\hat{h}_\alpha$'s, respectively. Since the elements $h_\alpha$, $\alpha\in\Delta$, are linearly independent in $\h$, this also holds for the elements $\hat{h}_\alpha$ in $\hat{\h}$, so that $q|_{\h}:\hat{\h}\to\h$ is a linear isomorphism. For $\gamma\in\h^*$, we put $\hat{\gamma}:=\gamma\circ q\in\hat{\h}^*$.
\begin{lemma}\label{Lie algebra normalizer of subset is subalgebra}
Let $\b$ be a subalgebra of a Lie algebra $\g$ and $M\sub\g$ be a subset. Then $\b_M:=\{x\in\b:[M,x]\sub\b\}$ is a subalgebra of $\b$.
\end{lemma}
\begin{proof}
For $m\in M$ and $x,y\in\b_M$, by the Jacobi identity,
\[[m,[x,y]]=[[m,x],y]+[x,[m,y]]\in[\b,\b]+[\b,\b]\sub\b\]
so the claim follows.
\end{proof}
\begin{lemma}\label{Lie algebra generated by Serre relation prop}
The Lie algebra $\hat{\g}$ has the following properties:
\begin{itemize}
\item[(a)] $\hat{\g}$ is the direct sum of $\hat{\h}$-weight spaces.
\item[(b)] $\hat{\g}=\hat{\g}_+\oplus\hat{\h}\oplus\hat{\g}_-$ is a direct sum of subalgebras.
\item[(c)] $\hat{\g}_{\hat{\alpha}}=\K\hat{e}_\alpha$ and $\hat{\g}_{-\hat{\alpha}}=\K\hat{f}_\alpha$ for $\alpha\in\Delta$.
\item[(d)] The derivations $\ad(\hat{e}_\alpha)$ and $\ad(\hat{f}_\alpha)$ of $\hat{\g}$ are locally nilpotent, i.e., $\hat{\g}=\hat{\g}^0(\ad(\hat{e}_\alpha))=\hat{g}^0(\ad(\hat{f}_\alpha))$ for each $\alpha\in\Delta$.
\item[(e)] The set $\hat{\Phi}$ of roots of $\hat{\g}$ is invariant under the group $\hat{W}$ generated by the reflections defined by $s_{\hat{\alpha}}(\hat{\beta}):=\hat{\beta}-\hat{\beta}(\hat{h}_\alpha)\hat{\alpha}$. This is a finite group isomorphic to $W$.
\item[(f)] $\hat{\Phi}=\hat{W}(\hat{\Delta})$.
\end{itemize}
\end{lemma}
\begin{proof}
Let $\hat{g}_f=\sum_{\gamma\in\hat{\h}^*}\hat{\g}_\gamma$ be the sum of all $\hat{\h}$-weight spaces in $\hat{\g}$. In view of $[\hat{\g}_\gamma,\hat{\g}_\delta]\sub\hat{\g}_{\gamma+\delta}$, this is a subalgebra of $\hat{\g}$. We also have $\hat{\h}\sub\hat{\g}_0$ and $\hat{e}_\alpha\in\hat{\g}_{\hat{\alpha}}$ and $\hat{f}_\alpha\in\hat{\g}_{-\hat{\alpha}}$. Since $\hat{\g}$ is generated by these elements, it follows that $\hat{\g}=\hat{\g}_f$ is the direct sum of $\hat{\h}$-weight spaces. The directness of the sum of the weight spaces can be obtained from Lemma~\ref{Lie nilpotent subalgebra decomposition of representation}.\par
Clearly, $\hat{\h}+\hat{\g}_\pm$ are subalgebras of $\hat{\g}$
because $\hat{\g}_\pm$ is generated by $\hat{\h}$-eigenvectors. For $\alpha\in\Delta$, we consider the subalgebra
\[\b_\alpha:=\{x\in\hat{\h}+\hat{\g}_+:[x,\hat{f}_\alpha]\sub\hat{\h}+\hat{\g}_+\}.\]
For $\beta\in\Delta$, we have $[\hat{f}_\alpha,\hat{e}_\beta]=0$ for $\alpha\neq\beta$ and $[\hat{f}_\alpha,\hat{e}_\beta]\in\hat{\h}$. Hence the subalgebra $\b_\alpha$ contains each $\hat{e}_\beta$, and therefore $\hat{\g}_+$. From that we derive
\[[\hat{f}_\alpha,\hat{\h}+\hat{\g}_+]\sub\hat{\h}+\hat{\g}_++\K\hat{f}_\alpha\]
which in turn implies that each $\hat{f}_\alpha$ normalizes the subspace $\hat{\g}_d:=\hat{\h}+\hat{\g}_++\hat{\g}_-$. Similarly, we obtain $e_\alpha\in\n_{\hat{\g}}(\hat{\g}_d)$, so that $\hat{\g}_d$ is a subalgebra of $\hat{\g}$. Since it contains all generators, we obtain $\hat{\g}_d=\hat{\g}$.
\end{proof}
\begin{theorem}[\textbf{Serre's Theorem}]
The homomorphism $q:\hat{\g}=L(X,R)\to\g$ is an isomorphism of Lie algebras.
\end{theorem}
\subsection{Highest weight theorem}
We know already from Weyl's Theorem that any finite-dimensional module over a semisimple Lie algebra $\g$ is semisimple. This reduces the classification of finite-dimensional modules to the classification of simple ones. In this part, we address this problem for the class of those semisimple Lie algebras which are split, i.e., contain a toral Cartan subalgebra. Note that $\sl_n(\K)$ and in particular any complex semisimple Lie algebra is split.\par
Throughout this subsection, $\g$ denotes a semisimple Lie algebra and $\h\sub\g$ a toral Cartan subalgebra. For $\lambda\in\h^*$ and a representation $(V,\rho)$ of $\g$, we write
\[V_\lambda:=\bigcap_{x\in\h}V_{\lambda(x)}(\rho(x)).\]
for the corresponding weight space in $V $and $\mathcal{P}(V)=\{\lambda\in\h^*:V_\lambda\neq\{0\}\}$ for the set of $\h$-weights of $V$. We simply write $\Phi:=\Phi(\g,\h)$ for the set of roots and $\g_\alpha:=\g_\alpha(\h)$, $\alpha\in\Phi$, for the root spaces. For $\lambda\in\mathcal{P}(V)$, the \textbf{multiplicity} of $\lambda$ is defined to be the dimension of $V_\lambda$.
\subsubsection{Standard cyclic modules}
Let $V$ be a $\g$-module. A \textbf{highest weight} for $V$ is a weight $\lambda$ that is higher than any other weights of $V$. A \textbf{maximal vector} of $V$ is a nonzero weight vector $v\in V$ that is killed by all $\g_\alpha$ for $\alpha\in\Delta$.
\begin{definition}
Let $\Delta\sub\Phi$ be a basis. The \textbf{Borel subalgebra} for $\Delta$ is defined to be
\[\b=\h+\sum_{\alpha\in\Phi^+}\g_\alpha.\]
Since $[\b,\b]=\sum_{\alpha\in\Phi^+}\g_\alpha$ is nilpotent, we see $\b$ is solvable.
\end{definition}
\begin{proposition}\label{Lie algebra rep maximal vector iff Borel}
Let $V$ be a $\g$-module. Then a vector $v\in V$ is a maximal vector if and only if it is a common eigenvector for the Borel subalgebra $\b$.
\end{proposition}
\begin{proof}
Let $W=\K v$. Then $W$ is a representation of $\b$ and therefore also of $\h$, so $v$ is an eigenvector of $\h$ and we have a linear map $\lambda:\h\to\K$ given by $h\cdot v=\lambda(h)v$. Thus $W= W_\lambda$. For any positive root $\alpha$ we have $\g_\alpha\sub\b$ and $\g_\alpha\cdot v\sub W_{\lambda+\alpha}=\{0\}$. Hence $v$ is a maximal vector of weight $\lambda$. The converse is obvious.
\end{proof}
Note that since $\b$ is a solvable Lie subalgebra, by Lie's theorem there always exists a common eigenvector in $V$. Thus maximal vectors always exist in a $\g$-module. Also, it is clear that any nonzero vector of highest weight is a maximal vector, while the converse is not ture.
\begin{definition}
A \textbf{standard cyclic module} of highest weight $\lambda$ is a representation $V$ which is generated by a single maximal vector $v$ of weight $\lambda$.
\end{definition}
By definition, if $V$ is a standard cyclic module with maximal vector $v$, then we have $V=U(\g)\cdot v$. The fact that the finite-dimensional Lie algebra $\g$ can have infinite-dimensional cyclic modules comes from the fact that $U(\g)$ is infinite-dimensional in general.\par
Let $V$ be a standard cyclic module of highest weight $\lambda$. We want to show that $\lambda$ is indeed the highest weight of $V$, and each weight space $V_\lambda$ is finite-dimensional.
\begin{theorem}\label{Lie algebra standard cyclic module prop}
Let $V$ be a standard cyclic module of highest weight $\lambda$, with maximal vector $v\in V_\lambda$. Let $\Phi^+=\{\beta_1,\dots,\beta_m\}$. Then
\begin{itemize}
\item[(a)] $V$ is spanned by the vectors $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}\cdot v$, $i_j\in\N$; in particular, $V$ is a direct sum of its weight spaces.
\item[(b)] $\lambda$ is the highest weight for $V$.
\item[(c)] For each $\mu\in\mathcal{P}(V)$, $\dim V_\mu<\infty$, and $\dim V_\lambda=1$.
\item[(d)] Each submodule of $V$ is the direct sum of its weight spaces.
\item[(e)] $V$ is an indecomposable $\g$-module, with a unique maximal (proper) submodule and a corresponding unique irreducible quotient.
\item[(f)] Every nonzero homomorphic image of $V$ is also standard cyclic of weight $\lambda$. 
\end{itemize}
\end{theorem}
\begin{proof}
Decompose $\g$ as $\g=\n^-+\b$ where $\n^-=\sum_{\alpha\in\Phi^-}\g_\alpha$. Let $\Delta=\{\alpha_1,\dots,\alpha_r\}$. Then $\{h_{\alpha_i},f_{\alpha_i},e_{\alpha_i}:i=1,\dots,r\}$ is a basis of $\g$, to which we apply the Poincar\'e-Birkhoff-Witt Theorem. Since $v$ is a common eigenvector for $\b$, it follows that $V=U(\n^-)\cdot v$. Now $U(\n^-)$ has a basis $\{f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}:i_j\in\N\}$, so (a) follows. Note that, since $\g_\alpha$ maps $V_\lambda$ to $V_{\lambda+\alpha}$, the vector $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}\cdot v$ has weight $\lambda-\sum_ji_j\beta_j$, which is lower than $\lambda$. This implies (b), since each $\beta_j$ is positive.\par
For $\mu\in\h^*$, there are only finitely many vectors of the form $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}\cdot v$ for which $\lambda-\sum_ji_j\beta_j$ equals $\mu$. In view of (a), these span the weight space $V_\mu$. Moreover, the only vector $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}\cdot v$ for $\lambda-\sum_ji_j\beta_j=\lambda$ is $v$, thus $\dim V_\lambda=1$. This proves (c).\par
For (d), let $W$ be a submodule of $V$. Then since each $h\in\h$ is diagonalizable on $V$, it is also diagonalizable on $W$ with the same eigenvalues. This implies $\h$ can by simultaneously diagonalizable on $W$, which means $W$ is a direct sum of weights spaces.\par
We conclude from (c) and (d) that every proper submodule of $V$ lies in the sum of weight sapces other than $V_\lambda$, so the sum of all these submodules is still proper. This implies $V$ has a unique maximal proper submodule, and hence a unique simple quotient module. In turn, $V$ cannot by written as the sum of two proper submodules, since there are both contained in $W$. In other words, $V$ is indecomposable. Finally, (f) is clear.
\end{proof}
\begin{corollary}
If $V$ is a simple standard cyclic module of highest weight $\lambda$ with maximal vector $v$, then $v$ is the the unique maximal vector of $V$, up to nonzero scalar multiples.
\end{corollary}
\begin{proof}
If $w$ is another maximal vector, then $V$ is generated by $w$. Therefore the above theorem applies to $v$ and to $w$. If $w$ has weight $\lambda'$ then $\lambda'\preceq\lambda$ and $\lambda\preceq\lambda'$, which implies $\lambda=\lambda'$. But $\dim V_\lambda=1$, so $w$ is a multiple of $v$.
\end{proof}
Now we show that for each $\lambda\in\h^*$, there exists one and only one (up to isomorphism) standard cyclic $\g$-module of highest weight $\lambda$, which may be infinite-dimensional.
\begin{theorem}\label{Lie algebra simple standard cyclic module isomorphic if highest weight}
Let $V$ and $W$ be standard cyclic $\g$-module of highest weight $\lambda$. If $V$ and $W$ are simple, then they are isomorphic. 
\end{theorem}
\begin{proof}
Let $V$ and $W$ be two such modules. We choose nonzero elements $v\in V_\lambda$ and $w\in W_\lambda$. Set $M:=V\oplus W$ and $m:=v+w$. Then $m$ is a common eigenvector for $\b$ and the submodule $U$ of $M$ generated by $m$ is a standard cyclic module of highest weight $\lambda$. Let $\pi_V:U\to V$ and $\pi_W:U\to V$ be the canonical projections with respect to the direct sum $V\oplus W$. Then both $\pi_V$ and $\pi_W$ are homomorphisms of $\g$-modules. From $\pi_V(m)=v$ and $\pi_W(m)=w$, we see that $\pi_V$ and $\pi_W$ are surjective. As simple quotient of the standard cyclic module $U$, we see $V$ and $W$ are isomorphic.
\end{proof}
\begin{theorem}
For any $\lambda\in\h^*$, there exists a simple standard cyclic module $V(\lambda)$ with highest weight $\lambda$.
\end{theorem}
\begin{proof}
Fix $\lambda\in\h^*$. We begin with a one-dimensional vector space $D(\lambda)$, having $v$ as a basis, and extend $\lambda$ on $\b$ by letting it to be zero on $\n^+=\sum_{\alpha\in\Phi^+}\g_\alpha$. Then we define a $\b$-action on $D(\lambda)$ by $b\cdot v=\lambda(b)v$ for $b\in\b$. Since $[\b,\b]=\sum_{\alpha\in\Phi^+}\g_\alpha\sub\ker\lambda$, this makes $D(\lambda)$ a $\b$-module. Of course, $D(\lambda)$ is equally well a $U(\b)$-module, so it make sense to form the tensor product
\[W(\lambda)=U(\g)\otimes_{U(\b)}D(\lambda),\]
which becomes a $U(\g)$-module under the natural left action of $U(\g)$. (The module $W(\lambda)$ is called the \textbf{Verma module} of weight $\lambda$.)\par
We claim that $W(\lambda)$ is standard cyclic of highest weight $\lambda$. On the one hand, $1\otimes v$ evidently generates $W(\lambda)$. One the other hand, $1\otimes v$ is nonzero in $W(\lambda)$ because $U(\g)$ is a free $U(\b)$-module (Corollary~\ref{Lie algebra PBW Theorem on subalgebra}) with basis $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}$. Moreover, for $b\in\b$,
\[b\cdot 1\otimes v=b\otimes v=1\otimes bv=\lambda(b)\otimes v.\]
Therefore $1\otimes v$ is a maximal vector. Using Theorem~\ref{Lie algebra standard cyclic module prop}, there is a maximal submodule $M(\lambda)$ in $W(\lambda)$, and the quotient $V(\lambda)=W(\lambda)/M(\lambda)$ is a standard cyclic module with highest weight $\lambda$. In particular, such a module exists for each $\lambda\in\h^*$.
\end{proof}
\begin{example}
In the $\sl_2(\C)$ case, Verma modules can be constructed explicitly as follows. For any complex number $\lambda$, construct an infinite-dimensional vector space $W(\lambda)$ with basis $v_0,v_1,\dots$ (The elements of $V(\lambda)$ are finite linear combinations of the $v_j$'s.) We define an action of $\sl_2(\C)$ on $W(\lambda)$ by
\[h\cdot v_j=(\lambda-2j)v_j,\quad e\cdot v_j=j(\lambda-(j-1))v_{j-1},\quad f\cdot v_j=v_{j+1}.\]
When $\lambda$ is a non-negative integer, the space $M(\lambda)$ spanned by $v_{\lambda+1},v_{\lambda+2},\dots$ is invariant under the action of $\sl_2$. After all, this space is clearly invariant under the action of $f$ and $h$, and it is invariant under $e$ because we have
\[e\cdot v_{\lambda+1}=(\lambda+1)(\lambda-\lambda)v_\lambda=0.\]
Since $M(\lambda)$ is invariant, the quotient vector space $V(\lambda)=W(\lambda)/M(\lambda)$ inherits a natural action of $\sl_2(\C)$. This quotient space is then the unique finite-dimensional irreducible representation of $\sl_2(\C)$ with highest weight $\lambda$.
\end{example}
\subsubsection{Finite dimensional modules}
Suppose $V$ is a finite dimensional simple $\g$-module. Then $V$ has at least one maximal vector, of uniquely determined weight $\lambda$, and the submodule it generates must be all of $V$. Therefore, $V$ is isomorphic to $V(\lambda)$.\par
For each simple root $\alpha\in\Delta$, let $\g(\alpha)$ be the corresponding copy of $\sl_2(\K)$ in $\g$. Then $V(\lambda)$ is also a (finite dimensional) module for $\g(\alpha)$, and a maximal vector for $\g$ is also a maximal vector for $\g(\alpha)$.
\begin{proposition}\label{Lie algebra simple module highese weight is dominant}
Let $V$ be a finite-dimensional $\g$-module, then all of its weights are integral. If $\lambda$ is the highest weight of $V$ then $\lambda$ is dominant.
\end{proposition}
\begin{proof}
Let $\lambda$ be a weight of $V$, $\alpha\in\Delta$, and $\g(\alpha)$ be the subalgebra isomorphic to $\sl_2(\K)$. Then $\langle\lambda,\alpha\rangle$ is the eigenvalue of $h_\alpha$ on $V$. Thus by Proposition~\ref{sl_2(K)-module string property} we have $\langle\lambda,\alpha\rangle\in\Z$ and $\langle\lambda,\alpha\rangle\geq 0$ if $\lambda$ is highest.
\end{proof}
Now we prove that the simple standard cyclic module $V(\lambda)$ is finite-dimensional if and only if $\lambda$ is dominant integral. For this, we first need the following definition.
\begin{definition}
A linear operator $\phi$ on a vector space $V$ is called \textbf{locally nilpotent} if for each $v\in V$, there exists a positive integer $n$ such that $\phi^n(v)=0$.
\end{definition}
If $V$ is finite dimensional, then a locally nilpotent operator must actually be nilpotent, that is, there must exist a single $n$ such that $\phi^n=0$. In the infinite-dimensional case, the value of $n$ depends on $v$ and there may be no single value of $n$ that works for all $v$. If $\phi$ is locally nilpotent, then we define $e^\phi$ to be the operator satisfying
\[e^\phi(v)=\sum_{k=0}^{\infty}\frac{\phi^k(v)}{k!}\]
where for each $v\in V$ the series on the right terminates.
\begin{proposition}\label{Lie algebra act locally nilpotent if dominant integral}
For each $\alpha\in\Delta$, let $(h_\alpha,e_\alpha,f_\alpha)$ be an $\sl_2$-triple corresponding to $\alpha$. If $\lambda$ is dominant integral, then $e_\alpha$ and $f_\alpha$ act locally nilpotently on the module $V=V(\lambda)$.
\end{proposition}
\begin{proof}
For each simple root $\alpha\in\Delta$, we choose an $\sl_2$-triple $(h_\alpha,e_\alpha,f_\alpha)$. We say a vector in $V$ is \textbf{$\g(\alpha)$-finite} if it is contained in a finite-dimensional $\g(\alpha)$-submodule of $V$. Let $v\in V$ be a maximal vector. For $\alpha,\beta\in\Delta$, we put $m_\alpha:=\lambda(h_\alpha)=\langle\lambda,\alpha\rangle$ (which is a non-negative integer) and observe that
\[e_\beta f_\alpha^{m_\alpha+1}\cdot v=\begin{cases}
0&\text{if }\alpha\neq\beta,\\
(m_\alpha+1)f_\alpha^{m_\alpha}(h_\alpha-m_\alpha\cdot 1)\cdot v=0&\text{if }\alpha=\beta.
\end{cases}\]
Here we use that $[e_\beta,f_\alpha]\in\g_{\beta-\alpha}=\{0\}$ for $\alpha\neq\beta$, and for $\alpha=\beta$ we use the formulas from Lemma~\ref{sl_2(K) representation commutator relation}(b) and the fact that $h_\alpha\cdot v=\lambda(h_\alpha)v=m_\alpha v$ and that $e_\beta\cdot v=0$. Since the set $\{e_\alpha:\alpha\in\Delta\}$ generate the subalgebra $\n^+=\sum_{\alpha\in\Phi^+}\g_\alpha$, the vector $f_\alpha^{m_\alpha+1}\cdot v$ is a maximal vector for $\g$, and hence generates a proper standard cyclic submodule of $V$ with highest weight $\lambda-(1+m_\alpha)\alpha$, so that the irreducibility of $V$ yields $f_\alpha^{m_\alpha+1}\cdot v=0$. Therefore the span of $\{v,f_\alpha v,\dots,f_\alpha^{m_\alpha}v\}$ is a finite-dimensional $\g(\alpha)$-module with highest weight $\lambda(h_\alpha)=m_\alpha$. In particular, this shows the subspace $V_\alpha$ of $V$ spanned by all finite-dimensional $\g(\alpha)$-submodules is nonzero.\par
Let $M\sub V$ be a finite-dimensional $\g(\alpha)$-submodule. Then the span of $g\cdot M$ is finite-dimensional and $\g(\alpha)$-stable, so it is contained in $V_\alpha$. This implies that $V_\alpha$ is a $\g$-submodule, and $V_\alpha=V$ since $V$ is irreducible. We conclude that every $v\in V$ is contained in a finite-dimensional $\g(\alpha)$-invariant subspace. It then follows from Example~\ref{Laurent polynomial rep of sl_2} that $e_\alpha^n\cdot v=f_\alpha^n\cdot v=0$ for some $n$, showing that $e_\alpha$ and $f_\alpha$ are locally nilpotent.
\end{proof}
\begin{proposition}\label{Lie algebra rep locally nilpotent then unvariant}
Let $(V,\rho)$ be a $\g$-module, and for each $\alpha\in\Delta$ let $(h_\alpha,e_\alpha,f_\alpha)$ be an $\sl_2$-triple corresponding to $\alpha$. If each $e_\alpha$ and $f_\alpha$ act locally nilpotently on $V$, then the weights of $V$ and their multiplicities are invariant under the action of $W$ on $\h$.
\end{proposition}
\begin{proof}
Since $W$ is generated by the reflections $s_\alpha$ with $\alpha\in\Delta$, it suffices to show that the weights of $V$ are invariant under each such reflection. Since each $e_\alpha$ and $f_\alpha$ are locally nilpotent, it makes sense to define operators $\phi_\alpha$ by
\[\phi_\alpha=e^{\rho(e_\alpha)}e^{-\rho(f_\alpha)}e^{\rho(e_\alpha)}\]
By Proposition~\ref{sl_2(K)-module eigenspace of h}, we know that $\phi_\alpha\rho(h_\alpha)\phi_\alpha^{-1}=\rho(-h_\alpha)$. From the definition of the root decomposition, for $h\in\h$ we also have
\[[h,e_\alpha]=\alpha(h)e_\alpha,\quad[h,f_\alpha]=-\alpha(h)f_\beta.\]
Thus if $\alpha(h)=0$, then $\rho(h)$ commutes with $\phi_\alpha$, which means $\phi_\alpha\rho(h)\phi_\alpha^{-1}=\rho(h)$. These together imply that
\[\phi_\alpha\rho(h)\phi_\alpha^{-1}=\rho(s_{h_\alpha}(h))\for h\in\h\]
where $s_{h_\alpha}(h):=h-2(h,h_\alpha)/(h_\alpha,h_\alpha)h_\alpha$ is the reflection in $\h$ corresponding to $h_\alpha$. Suppose now that $v$ is a weight vector with some weight $\lambda$. Then
\[\rho(h)\phi_\alpha^{-1}v=\phi_\alpha^{-1}\rho(s_{h_\alpha}(h))v=\phi_\alpha^{-1}\lambda(s_{h_\alpha}(h))v=\lambda(s_{h_\alpha}(h))\phi_\alpha^{-1}v.\]
Also, we note that
\begin{align*}
\lambda(s_{h_\alpha}(h))&=\lambda(h)-\frac{2(h_\alpha,h)}{(h_\alpha,h_\alpha)}\lambda(h_\alpha)=\lambda(h)-2\frac{\alpha(h)\cdot\frac{2}{(\alpha,\alpha)}}{(\alpha,\alpha)\cdot\frac{4}{(\alpha,\alpha)^2}}(\lambda,\alpha)\cdot\frac{2}{(\alpha,\alpha)}\\
&=\lambda(h)-\frac{2\alpha(h)}{(\alpha,\alpha)}(\lambda,\alpha)=\lambda(h)-\frac{2(\lambda,\alpha)}{(\alpha,\alpha)}\alpha(h)=s_\alpha(\lambda)(h).
\end{align*}
These together show that $\phi_\alpha^{-1}$ maps $V_\lambda$ into $V_{s_\alpha(\lambda)}$. Meanwhile, essentially the same argument shows that $\phi_\alpha$ maps $V_{s_\alpha^{-1}(\lambda)}=V_{s_\alpha(\lambda)}$ into $V_\lambda$, showing that the two spaces are isomorphic. Thus, $s_\alpha(\lambda)$ is again a weight with the same multiplicity as $\lambda$. Thus, the weights and multiplicities are invariant under each $s_\alpha$ and thus under $W$.
\end{proof}
\begin{theorem}\label{Lie algebra simple standard cyclic module finite-dim if weight dominant}
If $\lambda\in\h^*$ is dominant integral with respect to $\Delta$, then the simple standard cyclic module $V=V(\lambda)$ of highest weight $\lambda$ is finite-dimensional.
\end{theorem}
\begin{proof}
Apply Proposition~\ref{Lie algebra act locally nilpotent if dominant integral} and Proposition~\ref{Lie algebra rep locally nilpotent then unvariant}, we get $\dim V_{\lambda}=\dim V_{w(\lambda)}$ for all $w\in W$ as well as the $W$-invariance of $\mathcal{P}(V)$. It follows from the definition of the ordering $\preceq$ that the set of dominant integral elements $\mu\in\h^*$ with $\mu\preceq\lambda$ is finite. Since all weights of $V$ are integral, and every integral element is $W$-conjugate to a dominant integral element, the argument above shows that $\mathcal{P}\sub\mathrm{conv}(W\cdot\lambda)$, whence whence $\mathcal{P}(V)$ is finite. As all weight spaces $V_\mu$ are finite dimensional by Theorem~\ref{Lie algebra standard cyclic module prop}, this concludes the proof.
\end{proof}
\begin{theorem}[\textbf{Highest Weight Theorem}]
The assignment $\lambda\mapsto V(\lambda)$ defines a bijection between the set $\Lambda^+$ of dominant integral weights and the set of isomorphism classes of finite-dimensional simple $\g$-modules.
\end{theorem}
\begin{proof}
First, we recall that each finite-dimensional simple $\g$-module is a highest weight module with some highest weight $\lambda$. In view of Proposition~\ref{Lie algebra simple module highese weight is dominant}, $\lambda$ is dominant integral, and Theorem~\ref{Lie algebra simple standard cyclic module isomorphic if highest weight} shows that it only depends on the isomorphism class of the module. Finally, Theorem~\ref{Lie algebra simple standard cyclic module finite-dim if weight dominant} shows that any dominant integral weight is the highest weight of some finite-dimensional simple $\g$-module.
\end{proof}
\subsubsection{The structure of the weights}
Recall that the weights of a finite-dimensional $\g$-module are integral elements and that the weights and their multiplicities are invariant under the action of $W$. We now determine which weights occur in the representation with highest weight $\mu$. Recall that $\Lambda_r$ is the lattice generated by roots.
\begin{theorem}\label{Lie algebra weight char}
Let $V$ be a finite-dimensional simple $\g$-module with highest weight $\lambda$. An integral element $\mu$ is a weight of $V$ if and only if the following two conditions are satisfied.
\begin{itemize}
\item[(a)] $\mu\in\conv(W\cdot\lambda)$.
\item[(b)] $\lambda-\mu\in\Lambda_r$.
\end{itemize}
\end{theorem}
Theorem~\ref{Lie algebra weight char} says, in effect, that there are no unexpected holes in the set of weights of $V$. The key to the proof is the "no holes" result.
\begin{lemma}[\textbf{"No Holes" Lemma}]\label{Lie algebra no holes lemma}
Suppose $V$ is a finite-dimensional $\g$-module. Suppose that $\lambda$ is a weight of $V$ and $\alpha$ is a root. Then every element $\mu$ on the line segment joining $\lambda$ to $s_\alpha(\lambda)$ with the property that $\lambda-\mu$ is an integer multiple of $\alpha$ is also a weight of $V$. In particular, $\lambda-\alpha$ is a weight of $V$.
\end{lemma}
\begin{proof}
Let $U$ be the subspace of $V$ spanned by weight spaces with weights $\eta$ of the form $\eta=\lambda-k\alpha$ for $k\in\Z$. Since $e_\alpha$ and $f_\alpha$ shift weights by $\pm\alpha$; the space $U$ is invariant under $\g(\alpha)$. Note that since $\langle\alpha,\alpha\rangle=2$, we have
\[\langle\lambda-k\alpha,\alpha\rangle=k_\alpha-2k,\]
where $k_\alpha=\langle\lambda,\alpha\rangle$. That is, the weight space corresponding to weight $\lambda-k\alpha$ is precisely the eigenspace for $h_\alpha$ inside $U$ corresponding to the eigenvalue $k_\alpha-2k$.\par
By Proposition~\ref{sl_2(K)-module string property}, since $k_\alpha>0$ is an eigenvalue for $h_\alpha$ inside $U$, all of the integers $k_\alpha-2k$, $0\leq k\leq k_\alpha$, must also be eigenvalues for $h_\alpha$ inside $U$. Thus, $\lambda-k\alpha$ must be a weight of $V$ for $0\leq k\leq k_\alpha$. Since $k_\alpha$ is a positive integer, $k_\alpha$ must be at least $1$, and, thus, $\lambda-\alpha$ must be a weight of $V$.
\end{proof}
\begin{proposition}\label{Lie algebra weight of highest module if}
Let $\lambda$ be a dominant integral element. Suppose $\mu$ is dominant, lower than $\lambda$, and $\lambda-\mu$ can be expressed as an integer combination of roots. Then $\mu$ is a weight of the simple $\g$-module with highest weight $\lambda$.
\end{proposition}
\begin{proof}
Since $\lambda-\mu$ is an integer combination of roots, it is also an integer combination of the positive simple roots $\alpha_1,\dots,\alpha_r$. Since we also have $\mu\preceq\lambda$,
\[\lambda=\mu+\sum_{i=1}^{r}k_i\alpha_i\]
for some non-negative integers $k_1,\dots,k_r$. Consider now the following set $P$ of integral elements,
\[P=\{\eta=\mu+\sum_{i=1}^{r}\ell_i\alpha_i:0\leq\ell_i\leq k_i\}.\]
The elements of $P$ form a discrete parallelepiped.\par
We do not claim that every element of $P$ is a weight of $V$, which is not, in general, true. Rather, we will show that if $\eta\neq\mu$ is a weight of $V$ in $P$; then there is another weight of $V$ in $P$ that is "closer" to $\lambda$. Specifically, for each element $\eta$ of $P$, let $L(\eta)=\sum_{i=1}^{r}\ell_i$. Starting from $\lambda$, we will construct a sequence of weights of $V$ in $P$ with decreasing values of $L(\eta)$, until we reach one with $L(\eta)=0$, which means that $\eta=\mu$.\par
Suppose then that $\eta$ is a weight of $V$ in $P$ with $L(\eta)>0$. In that case, the second term in the formula for $\eta$ is nonzero, and, thus
\[\Big\|\sum_{i=1}^{r}\ell_i\alpha_i\Big\|=\sum_{j=1}^{r}\ell_j\langle\sum_{i=1}^{r}\ell_i\alpha_i,\alpha_k\rangle.\]
Since each $\ell_j$ is non-negative, there must be some $\ell_j$ for which $\ell_j>0$ and for which
\[\langle\sum_{i=1}^{r}\ell_i\alpha_i,\alpha_j\rangle>0.\]
On the other hand, since $\mu$ is dominant, $\langle\mu,\alpha_j\rangle>0$, and we conclude that $\langle\eta,\alpha_j\rangle>0$. Thus, by the "no holes" lemma, $\eta-\alpha_j$ is a weight of $V$.\par
Now, since $\ell_j$ is positive, $\ell_j-1$ is non-negative, meaning that $\eta-\alpha_j$ is still in $P$, where all the $\ell_i$'s are unchanged except that $\ell_j$ is replaced by $\ell_j-1$. Thus, $L(\eta-\alpha_j)=L(\eta)-1$. We can then repeat the process starting with $\eta=\lambda$ and obtain a sequence of weights of $V$ with successively smaller values of $L$, until we reach $L=0$, which corresponds to $\eta=\lambda$.
\end{proof}
\begin{proof}[Proof of Theorem~\ref{Lie algebra weight char}]
Let $U\sub V$ denote the span of all weight vectors whose weights differ from $\lambda$ by a linear combination of roots, and $v$ be the maximal vector of $V$. Then $U$ is easily seen to be invariant under the action of $\g$ and $U$ contains $v$, so $U=V$. Thus, every weight of $V$ must satisfy the conditions of Theorem~\ref{Lie algebra weight char}. Furthermore, if $\mu$ is a weight of $V$, then $w(\mu)\preceq\lambda$ for all $w\in W$. Thus, by Proposition~\ref{root system dominant not in convex hull lemma}, $\mu\in\conv(W\cdot\lambda)$.\par
Conversely, suppose that $\mu$ satisfies the two conditions of Theorem~\ref{Lie algebra weight char}. We can choose $w\in W$ so that $\mu':=w(\mu)$ is dominant. Clearly, $\mu'$ still belongs to $\conv(W\cdot\lambda)$. Furthermore, since $\mu$ is integral, $w(\mu)-\mu$ is an element of the root lattice: after all, the definition of integrality implies that $s_\alpha(\mu)-\mu$ is an integer multiple of $\alpha$, since the $s_\alpha$'s generate $W$, the result holds for all $w\in W$. Thus, $\lambda-\mu'=\lambda-\mu+\mu-w(\mu)$ is an element of the root lattice, which means that $\mu'$ also satisfies the two conditions of Theorem~\ref{Lie algebra weight char}. Thus, by Proposition~\ref{Lie algebra weight of highest module if}, $\mu'$ is a weight of $V$, which means that $\mu=w^{-1}(\mu')$ is also a weight.
\end{proof}
\subsubsection{The eigenvalue of the Casimir operator}
To conclude, we construct a special element $C_\g$ in the center of the enveloping algebra of $\g$ and calculate its (scalar) action in a highest weight module. In special cases, this allows us to identify a given simple $\g$-module.\par
Let $\g$ be a finite-dimensional split semisimple Lie algebra with Cartan-Killing form $\kappa$. As before, we choose for each $\alpha\in\Phi^+$ an $\sl_2$-triple $(h_\alpha,e_\alpha,f_\alpha)$ and $e^\alpha\in\g_{-\beta}$, $f^\alpha\in\g_\beta$ with
\[\kappa(e_\alpha,e^\alpha)=\kappa(f_\alpha,f^\alpha)=1.\]
We further choose a basis $h_1,\dots,h_r$ for $\h$, and we write $h^1,\dots,h^r$ for the dual basis with respect to the nondegenerate restriction of $\kappa$ to $\h\times\h$. Then
\[\{h_i,e_\alpha,f_\alpha:i=1,\dots,r,\alpha\in\Phi^+\}\]
is a basis for $\g$ and
\[\{h^i,e^\alpha,f^\alpha:i=1,\dots,r,\alpha\in\Phi^+\}\]
is the dual basis with respect to $\kappa$. We therefore obtain a central element of $U(\g)$ by
\[C_\g=\sum_{i=1}^{r}h_ih^i+\sum_{\alpha\in\Phi^+}e_\alpha e^\alpha+f_\alpha f^\alpha.\]
It is called the \textbf{universal Casimir element}.
\begin{proposition}\label{Lie algebra universal Casimir element}
If $(V,\rho_V)$ is a standard cyclic module with highest weight $\lambda$ (also considered as a $U(\g)$-module) and $\delta$ is the Weyl vector, then
\[\rho_V(C_\g)=(\lambda,\lambda+2\delta)\id=(\|\lambda+\delta\|^2-\|\delta\|^2)\id.\]
If $\lambda$ is dominant and nonzero, then $\rho_V(C_\g)\neq 0$.
\end{proposition}
\begin{proof}
We compute the action of $C_\g$ on $V$. Let $v$ be a maximal vector in $V$. Then $e_\alpha\cdot v=e^\alpha\cdot v=0$ for each $\alpha\in\Phi^+$, and $[e_\alpha,e^\alpha]=h_\alpha'$ (proposition~\ref{Lie algebra semisimple braket via Killing form}) implies
\[e_\alpha e^\alpha\cdot v=[e_\alpha,e^\alpha]\cdot v+e^\alpha e_\alpha\cdot v=\lambda(h_\alpha')v=(\lambda,\alpha)v,\]
so that $\sum_{\alpha\in\Phi^+}(e_\alpha e^\alpha+f_\alpha f^\alpha)\cdot v=2(\lambda,\delta)v$. On the other hand, we calculate
\[\sum_{i=1}^{r}\lambda(h_i)\lambda(h^i)=\lambda\Big(\sum_{i=1}^{r}\kappa(h_\lambda',h^i)h_i\Big)=\lambda(h_\lambda')=(\lambda,\lambda).\]
Putting these facts together yields the first claim. Since $C_\g$ is central in $U(\g)$, $C_\g$ acts by the same scalar on the entire $U(\g)$-module $V=U(\g)v$.\par
Finally, we assume that $\lambda$ is dominant and nonzero. Then $\langle\lambda,\alpha\rangle\geq 0$ for all $\alpha\in\Phi^+$ implies that $(\lambda,\alpha)\geq 0$, and hence that $(\lambda,\rho)\geq 0$. This leads to $(\lambda,\lambda+2\delta)\geq(\lambda,\lambda)\geq 0$, and $(\lambda,\lambda+2\delta)>0$ if $\lambda\neq 0$.
\end{proof}
\subsection{The Weyl character formula}
The character formula is a major result in the structure of the irreducible representations of $\g$. Its consequences include a formula for the dimension of an irreducible representation and a formula for the multiplicities of the weights in an irreducible representation.
\begin{definition}
Suppose $(V,\rho)$ is a finite-dimensional representation of a complex semisimple Lie algebra $\g$. The character of $V$ is the function $\chi_W:\g\to\C$ given by
\[\chi_V(x)=\tr(e^{\rho(x)}).\]
\end{definition}
It turns out that the character of $V$ encodes many interesting properties of $V$. We will give a formula for the character of an irreducible representation of a semisimple Lie algebra in terms of the highest weight of the representation. For any $\lambda\in\h^*$, let's write $e^\lambda$ for the function that maps $h$ to $e^{\lambda(h)}$. First we have the following observation.
\begin{proposition}\label{Lie algebra module character prop}
If $(V,\rho)$ is a finite-dimensional representation of $\g$; we have the following results.
\begin{itemize}
\item[(a)] The dimension of $V$ is equal to the value of $\chi_V$ at the origin:
\[\dim V=\chi_V(0).\] 
\item[(b)] Suppose $V$ decomposes as a direct sum of weight spaces $V_\lambda$ with multiplicity $m_\lambda$. Then for $h\in\h$ we have
\begin{align}\label{Lie algebra module character prop-1}
\chi_V=\sum_\lambda m_\lambda e^{\lambda}.
\end{align} 
\end{itemize}
\end{proposition}
\begin{proof}
The first point holds because $\tr(I)=\dim V$. Meanwhile, since $\rho(h)$ acts as $\lambda(h)I$ in each weight space $V_\lambda$, the second point follows from the definition of $\chi_V$.
\end{proof}
Recall that each element $w$ of the Weyl group $W$ acts as an orthogonal linear transformation of $\h^*$. We let $\mathrm{det}(w)$ denote the determinant of this transformation, so that $\mathrm{det}(w)=\pm 1$. Let $\delta$ be the Weyl vector of $\g$ (the half the sum of the positive roots). Now we are now ready to state the main result of this part.
\begin{theorem}[\textbf{Weyl Character Formula}]\label{Weyl character formula}
If $(V,\rho)$ is an irreducible representation of $\g$ with highest weight $\lambda$, then
\begin{align}\label{Weyl character formula-1}
\chi_V=\frac{\sum_{w\in W}\mathrm{det}(w) e^{w(\lambda+\delta)}}{\sum_{w\in W}\mathrm{det}(w) e^{w(\delta)}}.
\end{align}
\end{theorem}
\begin{example}
Let $V$ denote the irreducible representation of $\sl_2(\C)$ of dimension $\lambda+1$ and let $h=(\begin{smallmatrix}1&0\\0&-1\end{smallmatrix})$. Then
\[\chi_V(ah)=\chi_V\Big(\begin{pmatrix}
e^a&0\\
0&e^{-a}
\end{pmatrix}\Big)=e^{\lambda a}+e^{(\lambda-2)a}+\cdots+e^{-\lambda a}.\]
Note that whenever $a$ is not an integer multiple if $i\pi$, we can write
\begin{align*}
(e^a-e^{-a})\chi_V(ah)&=e^{(\lambda+1)a}+e^{(\lambda-1)a}+\cdots+e^{-(\lambda-1)a}-e^{(\lambda-1)a}-\cdots-e^{-(\lambda-1)a}-e^{-(\lambda+1)a}\\
&=e^{(\lambda+1)a}-e^{-(\lambda+1)a},
\end{align*}
and therefore
\[\chi_V(ah)=\frac{e^{(\lambda+1)a}-e^{(\lambda+1)a}}{e^a-e^{-a}}=\frac{\sinh((\lambda+1)a)}{\sinh(a)}.\]
\end{example}
\begin{example}
Now consider the case $\g=\sl_n(\C)$, where we have
\[\Phi=\{\eps_i-\eps_j:1\leq i,j\leq n\},\quad \Delta=\{\eps_i-\eps_{i+1}:1\leq i\leq n-1\},\quad \Phi^+=\{\eps_i-\eps_j:i<j\}\]
with $W=S_n$. The Weyl vector $\delta$ can be then computed by
\begin{align*}
\delta&=\frac{1}{2}\sum_{i<j}(\eps_i-\eps_j)=\frac{1}{2}\sum_{i=1}^{n}\Big((n-i)\eps_i-\sum_{j=i+1}^{n}\eps_j\Big)=\frac{1}{2}\sum_{i=1}^{n}(n+1-2i)\eps_i\\
&=\frac{1}{2}\Big((n-1)\sum_{i=1}^{n-1}+\sum_{i=1}^{n-1}(n+1-2i)\eps_i\Big)=\sum_{i=1}^{n-1}(n-i)\eps_i
\end{align*}
where we use the relation $\eps_1+\cdots+\eps_n=0$. Now consider a finite dimensional irreducible representation $V$ of $\g$ with highest weight $\lambda$. Since $\lambda$ is dominant, we may write $\lambda=\sum_{i=1}^{n}\lambda_i\eps_i$ with $\lambda_1\geq\cdots\geq\lambda_n$ and $\lambda_n=0$. Then the Weyl character formula takes the form
\begin{align}\label{Weyl character formula for sl_n}
\chi_V(h)=\frac{\sum_{\sigma\in S_n}\sgn(\sigma)e^{\sum_{i=1}^{n-1}(\lambda_i+n-i)h_{\sigma(i)}}}{\sum_{\sigma\in S_n}\sgn(\sigma)e^{\sum_{i=1}^{n-1}(n-i)h_{\sigma(i)}}}=\frac{\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^{n}z_{\sigma(i)}^{\lambda_i+n-i}}{\sum_{\sigma\in S_n}\sgn(\sigma)\prod_{i=1}^{n}z_{\sigma(i)}^{n-i}}=\frac{\det(z_j^{\lambda_i+n-i})}{\det(z_j^{n-i})}
\end{align}
where we set $z_i:=e^{h_i}$. The right side of $(\ref{Weyl character formula for sl_n})$ is a famous formula for the Schur polynomial $s_\lambda(z_1,\dots,z_n)$, which can be proved by Young tableaux.
\end{example}
Since we will have frequent occasion to refer to the function in the denominator in $(\ref{Weyl character formula-1})$, we give it a name.
\begin{definition}
Let $q:\h\to\C$ be the function given by
\[q=\sum_{w\in W}\mathrm{det}(w) e^{w(\delta)}.\]
The function $q$ is called the \textbf{Weyl denominator}.
\end{definition}
The character formula may also be written as
\begin{align}\label{Weyl character formula-2}
q\cdot\chi_V=\sum_{w\in W}\mathrm{det}(w) e^{w(\lambda+\delta)}.
\end{align}
Let us pause for a moment to reflect on what is going on in $(\ref{Weyl character formula-2})$. The Weyl denominator $q$ is a sum of $|W|$ exponentials with coefficients equal to $\pm 1$. Meanwhile, the character $\chi_V$ is a large sum of exponentialswith positive integer coefficients, as in $(\ref{Lie algebra module character prop-1})$. When we multiply these two functions, we seemingly obtain an even larger sum of exponentials of the form $e^{(w(\delta)+\mu)}$, for $w\in W$ and $\mu$ a weight of $V$.\par
The character formula, however, asserts that most of these terms are not actually present. Specifically, the only exponentials that actually appear are those of the form $e^{w(\lambda+\delta)}$, which occurs with a coefficient of $\mathrm{det}(w)$. The point is that, in most cases, if a weight $\eta$ can be written in the form $\eta=w(\delta)+\mu$, with $\mu$ a weight of $V$, then $\eta$ can be written in this form in more than one way. The character formula asserts that unless $\eta$ is in the Weyl-group orbit of $\delta+\lambda$; the coefficient of $e^{\eta}$, after all the different contributions are taken into account, ends up being zero.\par
By contrast, the weight $\eta=\delta+\lambda$ only occurs once, since it corresponds to taking the highest weight occurring in $q$ (namely $\delta$) and the highest weight occurring in $\chi_V$ (namely the highest weight $\lambda$ of $V$). (Note that $w(\delta)\preceq\delta$ for all $w\in W$.) Furthermore, since the weights occurring in both $q$ and $\chi_V$ are Weyl invariant, the weight $w(\lambda+\delta)$ also occurs only once. The Weyl character formula, then, can be expressed as stating that if we compute the product $q\chi_V$; a huge cancellation occurs. Every exponential in the product ends canceling out to zero, except for those that occur only once, namely those of the form $w(\lambda+\delta)$.\par
Before proving the Weyl character formula, we first establish two of its concequences, namely the Weyl dimension formula and the Kostant multiplicitiy formula.
\subsubsection{The Kostant multiplicity formula}
In this part, we will obtain Kostant's multiplicity formula from the Weyl character formula by developing a method for dividing by the Weyl denominator $q$. For this we need the concept of formal exponentials. The following proposition will be our starting point.
\begin{proposition}\label{Lie algebra exponential linearly independent}
The exponential functions $e^{\lambda}$, with $\lambda\in\h^*$, are linearly independent in $C^\infty(\h)$.
\end{proposition}
\begin{proof}
We need to show that if the function $f:\h\to\C$ given by
\[f(h)=c_1e^{\lambda_1(h)}+\cdots+c_ne^{\lambda_n(h)}\]
is identically zero, where $\lambda_1,\dots,\lambda_n$ are distinct elements of $\h^*$; then $c_1=\cdots=c_n=0$. If $n=1$; we evaluate at $h=0$ and conclude that $c_1$ must be zero. If $n>1$, we choose, for each $k=2,\dots,n$ some $h_k\in\h$ such that $(\lambda_1(h_k)\neq\lambda_k(h_k)$. Since $f$ is identically zero, so is the function
\[g=(D_{h_2}-\lambda_2(h_2))\cdots(D_{h_n}-\lambda_n(h_n))f\]
where $D_x$ denotes the directional derivative in the direction of $x$:
\[(D_xf)(h)=\frac{d}{dt}\Big|_{t=0}f(h+tx).\]
Direct calculation then shows that
\begin{align}\label{Lie algebra exponential linearly independent-1}
g(h)&=\sum_{j=1}^{n}c_j\Big(\prod_{k=2}(\lambda_j(h_k)-\lambda_k(h_k))\Big)e^{\lambda_j(h)}=c_1\Big(\prod_{k=2}(\lambda_1(h_k)-\lambda_k(h_k))\Big)e^{\lambda_1(h)}.
\end{align}
By evaluating at $h=0$ and noting that, by construction, the second product in $(\ref{Lie algebra exponential linearly independent-1})$ is nonzero, we conclude that $c_1=0$. An entirely similar argument then shows that each $c_1=0$ as well.
\end{proof}
\begin{definition}
A \textbf{formal exponential series} is a formal series of the form
\[f=\sum_\lambda c_\lambda e^{\lambda}\]
where each $c_\lambda$ is an integral element and where the $c_\lambda$'s are complex numbers. A \textbf{finite exponential series} is a series of the same form where all but finitely many of the $c_\lambda$'s are equal to zero.
\end{definition}
Since we make no restrictions on the coefficients $c_\lambda$; a formal exponential series may not converge. Thus, the series should properly be thought of not as a function of $h$ but simply as a list of coefficients $c_\lambda$. If $f$ is a formal exponential series with coefficients $c_\lambda$ and $g$ is a \textit{finite} exponential series with coefficients $d_\lambda$, the product of $f$ and $g$ is a well-defined formal exponential series with coefficients $e_\lambda$ given by
\begin{align}\label{Lie algebra formal exponential product}
e_\lambda=\sum_{\mu+\eta=\lambda}c_\mu d_\eta.
\end{align}
Note that only finitely many of the terms in $(\ref{Lie algebra formal exponential product})$ are nonzero, since $g$ is a finite exponential series. (The product of two formal exponential series is, in general, not defined, because the sum defining the coefficients in the product might be divergent.)
\begin{definition}
If $\lambda$ is an integral element, we let $p(\lambda)$ denote the number of ways (possibly zero) that $\lambda$ can be expressed as a non-negative integer combination of positive roots. The function $p$ is known as the \textbf{Kostant partition function}.
\end{definition}
We are now ready to explain how to invert the Weyl denominator.
\begin{proposition}[\textbf{Formal Reciprocal of the Weyl Denominator}]\label{Lie algebra formal reciprocal of Weyl denominator}
At the level of formal exponential series, we have
\begin{align}\label{Lie algebra formal reciprocal of Weyl denominator-1}
\frac{1}{q}=\sum_{\eta\succeq 0}p(\eta)e^{-(\eta+\delta)}.
\end{align}
Here the sum is nominally over all integral elements $\eta$ with $\eta\succeq 0$; but $p(\eta)=0$ unless $\eta$ is an integer combination of roots.
\end{proposition}
The proposition means, more precisely, that the product of $q(H)$ and the formal exponential series on the right-hand side of $(\ref{Lie algebra formal reciprocal of Weyl denominator-1})$ is equal to $1$. To prove this, we first rewrite $q$ as a product.
\begin{lemma}\label{Lie algebra product form of Weyl denominator}
The Weyl denominator may be computed as
\begin{align}\label{Lie algebra product form of Weyl denominator-1}
q=\prod_{\alpha\in\Phi^+}(e^{\alpha/2}-e^{-\alpha/2}).
\end{align}
\end{lemma}
\begin{proof}
Let $\tilde{q}$ denote the product on the right-hand side of $(\ref{Lie algebra product form of Weyl denominator-1})$. If we expand out the product in the definition of $\tilde{q}$, there will be a term equal to
\[\prod_{\alpha\in\Phi^+}e^{\alpha/2}=e^{\delta}\]
where $\delta$ is the Weyl vector. Note that even though $\alpha/2$ is not necessarily integral, the element $\delta$ is integral (Proposition~\ref{root system half sum of positive is integral}). We now claim that every other exponential in the expansion will be of the form $\pm e^{\lambda}$ with $\lambda$ integral and strictly lower than $\delta$. To see this, note that every time we take $e^{-\alpha/2}$ instead of $e^{\alpha/2}$, we lower the exponent by $\alpha$. Thus, any $\lambda$ appearing will be of the form
\[\lambda=\delta-\sum_{\alpha\in\Phi_0}\alpha\]
for some subset $\Phi_0$ of $\Phi^+$. Such a $\lambda$ is integral and lower than $\delta$.\par
Meanwhile, the function $\tilde{q}$ is alternating with respect to the action of $W$ by exactly the same argument as in Proposition~\ref{Lie algebra Weyl alternating function iff}. Thus, if we write $\tilde{q}=\sum_\lambda c_\lambda e^{\lambda}$, then the coefficients $c_\lambda$ must satisfy
\begin{align}\label{Lie algebra product form of Weyl denominator-2}
c_{w(\lambda)}=\mathrm{det}(w) c_\lambda.
\end{align}
Since the exponential $e^{\delta}$ occurs in the expansion of $\tilde{q}$ with a coefficient of $1$, each $e^{w(\delta)}$ must occur with a coefficient of $\mathrm{det}(w)$. Thus, it remains only
to show that no other exponentials can occur in $\tilde{q}$. To see this, note that if any exponential $e^{\lambda}$ occurs for which $\lambda$ is not in the $W$-orbit of $\delta$; then by $(\ref{Lie algebra product form of Weyl denominator-2})$, another exponential $e^{\mu}$ must appear with $\mu$ dominant but strictly lower than $\delta$. Since $\delta$ is the minimal strictly dominant integral element, $\mu$ cannot be strictly dominant (see Proposition~\ref{root system strict dominant bigger than delta}) and thus must be orthogonal to one of the positive simple roots $\alpha_j$. Thus, $s_{\alpha_j}(\mu)=\mu$, where $\mathrm{det}(s_{\alpha_j})=-1$. Applying $(\ref{Lie algebra product form of Weyl denominator-2})$ to this case shows that the coefficient of $e^{\mu}$ must be zero.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{Lie algebra formal reciprocal of Weyl denominator}]
We first observe that
\[\frac{1}{e^{\alpha/2}-e^{-\alpha/2}}=e^{-\alpha}\frac{1}{1-e^{-\alpha}}=e^{-\alpha}(1+e^{-\alpha}+e^{-2\alpha}+\cdots)\]
at the level of formal exponential series. Taking a product over $\alpha\in\Phi^+$ gives
\[\frac{1}{q}=e^{\delta}\prod_{\alpha\in\Phi^+}(1+e^{-\alpha}+e^{-2\alpha}+\cdots).\]
In the product, a term of the form $e^{-\langle\eta,h\rangle}$ will occur precisely as many times there are ways to express $\eta$ as a non-negative integer combination of the $\alpha$'s, namely, $p(\eta)$ times.
\end{proof}
\begin{theorem}[\textbf{Kostant's Multiplicty Formula}]
Suppose $\lambda$ is a dominant integral element and $V(\lambda)$ is the finite-dimensional irreducible representation with highest weight $\lambda$. Then if $\mu$ is a weight of $V(\lambda)$, the multiplicity of $\mu$ is given by
\[\mult(\mu)=\sum_{w\in W}\mathrm{det}(w) p(w(\lambda+\delta)-(\mu+\delta)).\]
\end{theorem}
\begin{proof}
By the Weyl character formula and Proposition~\ref{Lie algebra formal reciprocal of Weyl denominator}, we have
\[\chi^\lambda=\Big(\sum_{\eta\succeq 0}p(\eta)e^{-(\eta+\delta)}\Big)\Big(\sum_{w\in W}\mathrm{det}(w)e^{w(\lambda+\delta)}\Big).\]
For a fixed weight $\mu$, the coefficient of $e^{\mu}$ in the character $\chi^\lambda$ is just the multiplicity of $\mu$ in $V(\lambda)$. This coefficient is the sum of the quantity $p(\eta)\mathrm{det}(w)$ over all pairs $(\eta,w)$ for which
\[-\eta-\delta+w(\lambda+\delta)=\mu,\]
or equivalently
\[\eta=w(\lambda+\delta)-(\mu+\delta).\]
Substituting this into $p(\eta)$ and summing over $W$ gives Kostant's formula.
\end{proof}
\subsubsection{The character formula for Verma modules}
Before coming to the proof of the Weyl character formula, we consider a "warm up case", that of the character of a Verma module. In the character formula for Verma modules, there is an even larger cancellation than in theWeyl character formula. The product of the Weyl denominator and the character would appear to be an infinite sum of exponentials, and yet all but one of these exponentials cancels out! The proof of this character formula follows a very natural course, consisting of writing down explicitly the multiplicities for the various weight spaces and then using the formula for the multiplicities to establish the desired cancellation. The character formula for Verma modules is a key ingredient in the proof of the character formula for finite dimensional representations.\par
Since a Verma module $(W(\lambda),\rho)$ is infinite dimensional, the operator $e^{\rho}$ may not have a well-defined trace. On the other hand, Part (b) of Proposition~\ref{Lie algebra module character prop} gives an expression for the character of a finite-dimensional representation, evaluated at a point in $\h$; in terms of the weights for the representation. This observation allows us to define the character of any representation that decomposes as a direct sum of weight spaces, as a formal exponential series on $h$:
\begin{definition}
Let $V$ be any representation (possibly infinite dimensional) that decomposes as a direct sum of integralweight spaces of finite multiplicity. We define the formal character of $V$ by the formula
\[Q_V=\sum_\lambda\mult(\lambda)e^{\lambda}\]
where the $Q_V$ is interpreted as a formal exponential series.
\end{definition}
\begin{theorem}\label{Lie algebra character of Verma module}
For any integral element $\lambda$, the formal character of the Verma module is given by
\begin{align}\label{Lie algebra character of Verma module-1}
Q_{W(\lambda)}=\sum_{\eta\succeq 0}p(\eta)e^{(\lambda-\eta)}=\frac{e^{(\lambda+\delta)}}{q}
\end{align}
where $p$ is the Kostant partition function.
\end{theorem}
The formula $(\ref{Lie algebra character of Verma module-1})$ is similar to the Weyl character formula, except that in the numerator we have only a single exponential rather than an alternating sum of exponentials. Of course, $(\ref{Lie algebra character of Verma module-1})$ implies that
\[q\cdot Q_{W(\lambda)}=e^{(\lambda+\delta)}.\]
This result means that when we multiply the Weyl denominator (an alternating finite sum of exponentials) and the formal character $Q_{W(\lambda)}$ (an infinite sum of exponentials), the result is just a single exponential, with all the other terms cancelling out. As usual, it is easy to see this cancellation explicitly in the case of $\sl_2(\C)$. There, if $\lambda$ is any integer (possibly negative) and $h$ is the usual diagonal basis element, we have
\[Q_{W(\lambda)}(ah)=e^{\lambda a}+e^{(\lambda-2)a}+\cdots.\]
Since $q(ah)=e^a-e^{-a}$, we obtain
\[q(ah)Q_{W(\lambda)}(ah)=e^{(\lambda+1)a}+e^{(\lambda-1)a}+\cdots-e^{(\lambda-1)a}-e^{(\lambda-3)a}-\cdots=e^{(\lambda+1)a}.\]
\begin{proof}[Proof of Theorem~\ref{Lie algebra character of Verma module}]
Enumerate the positive roots as $\beta_1,\dots,\beta_m$ and let $v$ be a maximal vector for $W(\lambda)$. Then by Theorem~\ref{Lie algebra standard cyclic module prop}, the elements of the form $f_{\beta_1}^{i_1}\cdots f_{\beta_m}^{i_m}\cdot v$, $i_j\in\N$ spanns $W(\lambda)$. As we have seen in Theorem~\ref{Lie algebra standard cyclic module prop}, an element of this form is a weight vector with weight
\[\xi=\lambda-\sum_ji_j\beta_j.\]
The number of times the weight will occur is the number of ways that $\lambda-\xi$ can be written as a non-negative integer combinations of the positive roots. Thus, we obtain the first expression for $Q_{W(\lambda)}$ The second expression the follows easily from the first expression and the formula for the formal inverse of the Weyl denominator.
\end{proof}
\subsubsection{Proof of the Weyl Character Formula}
Our strategy in proving the character formula is as follows. We will show first that the (formal) character of a Verma module $W(\eta)$ can be expressed as a finite linear combination of characters of irreducible highest weight cyclic representations $V(\lambda)$. (The $V(\lambda)$'s may be infinite dimensional.) By using the action of the Casimir element, we will see that the only $\lambda$'s appearing in this decomposition are those satisfying
\[|\eta+\delta|=|\lambda+\delta|.\]
We will then invert this relationship and express the character of an irreducible representation $V(\lambda)$ as a linear combination of characters of Verma modules $W(\eta)$, with $\eta$ again satisfying $|\eta+\delta|=|\lambda+\delta|$. We will then specialize to the case in which $\lambda$ is dominant integral, where $V(\lambda)$ is finite dimensional and its character $\chi^\lambda$ is a finite sum of exponentials. By the character formula for Verma modules, we obtain the following conclusion: The product $q\cdot \chi^\lambda$ is a finite linear combination of exponentials $e^{\mu}$, where each $\mu=\eta+\delta$ satisfies $|\mu|=|\lambda+\delta|$. From this point, it is a short step to show that only $\mu$'s of the form $\mu=w(\lambda+\delta)$ occur and then to prove the character formula.\par
We know from general principles that the product $q\cdot\chi^\lambda$ is a finite sum of exponentials. We need to show that the only exponential that actually occur in this product (with a nonzero coefficient) are those of the form $e^{w(\lambda+\delta)}$ and that such exponentials occur with a coefficient of $\mathrm{det}(w)$. We begin with a simple observation that limits which exponentials could possibly occur in the product.
\begin{proposition}\label{Lie algebra exponential in character first condition}
If an exponential $e^{\mu}$ occurs with nonzero coefficient in $q\cdot\chi^\lambda$, then we have
\begin{itemize}
\item[(a)] $\mu\in\conv(W\cdot(\lambda+\delta))$.
\item[(b)] $\mu-(\lambda+\delta)\in\Lambda_r$.
\end{itemize}
\end{proposition}
\begin{proof}
For each root $\alpha$,
\[s_\alpha(\delta)=\mu-\langle\delta,\alpha\rangle\alpha\]
will differ from $\delta$ by an integer multiple of $\alpha$. Since roots are integral, we conclude that $s_\alpha(\delta)$ is, again, integral. It follows that for any $w\in W$, the element $w(\delta)$ is integral and differs from $\mu$ by an integer combination of roots. Similarly, each weight of $V(\lambda)$ is integral and differs from $\lambda$ by an integer combination of roots (Theorem~\ref{Lie algebra weight char}). Thus, for each exponential $e^{\mu}$ occurring in the product $q\cdot\chi^\lambda$, the element $\mu$ will be integral and will differ from $\mu+\delta$ by an integer combination of roots.\par
Meanwhile, since each exponential in $q$ is in the Weyl-orbit of $\delta$ and each exponential in $\chi^\lambda$ is in the convex hull of the Weyl-orbit of $\lambda$ (again by Theorem~\ref{Lie algebra weight char}), each exponential in the product will be in the convex hull of the Weyl-orbit of $(\lambda+\delta)$, as claimed.
\end{proof}
Our next result is the key to the proof of the character formula. In fact, we will see that it, in conjunction with Proposition~\ref{Lie algebra exponential in character first condition}, limits the exponentials that can occur in $q\cdot\chi^\lambda$ to only those whose weights are in the Weyl orbit of $\lambda+\delta$.
\begin{proposition}\label{Lie algebra exponential in character second condition}
If an exponential $e^{\mu}$ occurs with nonzero coefficient in $q\cdot\chi^\lambda$, then $\mu$ must satisfy $\|\mu\|=\|\lambda+\delta\|$.
\end{proposition}
\begin{proof}[Proof of Character Formula Assuming Proposition~\ref{Lie algebra exponential in character second condition}]
Suppose that $v_1,\dots,v_n$ are distinct elements of a real inner product space, all of which have the same norm $s$. From the definition of a convex hull, it is then not hard to show by induction on $n$ that the only way such a convex combination can have norm $s$ is if one of the $a_i$'s is equal to 1 and all the others are zero. Applying this with the $v_i$'s equal to $w(\lambda+\delta)$ shows that the only exponentials $e^{\mu}$ satisfying both Propositions~\ref{Lie algebra exponential in character first condition} and \ref{Lie algebra exponential in character second condition} are those of the form $w(\lambda+\delta)$. Thus, the only exponentials appearing in the product $q\cdot\chi^\lambda$ are those of the form $e^{w(\lambda+\delta)}$.\par
Now, the exponential $e^{(\lambda+\delta)}$ occurs in the product exactly once, since it corresponds to taking the highest weight from $q$ (i.e., $\delta$) and the highest weight from $\chi^\lambda$ (i.e., $\lambda$) and the coefficient of $e^{(\lambda+\delta)}$ is $1$. Since the character is Weyl invariant and the Weyl denominator is Weyl alternating, their product is Weyl alternating. Thus the coefficients in the expansion of $q\cdot\chi^\lambda$ are alternating; that is, the coefficient of $e^{w(\lambda+\delta)}$ must equal $\mathrm{det}(w)$.
\end{proof}
We now begin the process of proving Proposition~\ref{Lie algebra exponential in character second condition}.
\begin{definition}
Let $\Lambda$ denote the set of integral elements. If $\lambda$ is a dominant integral element, define a set $S_\lambda\sub\Lambda$ by
\[S_\lambda=\{\eta\in\Lambda:\|\eta+\delta\|=\|\lambda+\delta\|\}.\]
\end{definition}
Note that $S_\lambda$ is the intersection of the integral lattice $\Lambda$ with sphere of radius $\|\lambda+\delta\|$ centered at $-\delta$. Since there are only finitely many elements of $\Lambda$ in any bounded region, the set $S_\lambda$ is finite, for any fixed $\lambda$. Our strategy for the proof of Proposition~\ref{Lie algebra exponential in character second condition} is as discussed at the beginning of this part. We will decompose the formal character of each Verma module $W(\eta)$ with $\eta\in S_\lambda$ as a finite sum of formal characters of irreducible representations $V(\gamma)$, with each $\gamma$ also belonging to $S_\lambda$. This expansion turns out to be of an "upper triangular with ones on the diagonal" form, allowing us to invert the expansion to express the formal character of irreducible representations in terms of formal characters of Verma modules. In particular, the character of the finite-dimensional representation $V(\lambda)$ will be expressed as a linear combination of formal characters of Verma modules $W(\eta)$ with $\eta\in S_\lambda$. When we multiply both sides of this formula by the Weyl denominator and use the character formula for the Verma module, we obtain the claimed form for $\chi^\lambda$.\par
Of course, a key point in the argument it verify is that whenever the character of an irreducible representation $V(\gamma)$ appears in the expansion of the character of $W(\eta)$, $\eta\in S_\lambda$, the highest weight $\gamma$ is also in $S_\lambda$. This claim holds because any subrepresentation $V(\gamma)$ occurring in the decomposition of $W(\eta)$ must have the same eigenvalue of the Casimir as $W(\eta)$. In light of Proposition~\ref{Lie algebra universal Casimir element}, this means $\|\gamma+\delta\|=\|\eta+\delta\|$, which is assumed to be equal to $\|\lambda+\delta\|$.
\begin{proposition}\label{Lie algebra character of Verma into irreducible}
For each $\eta\in S_\lambda$, the formal character of the Verma module $W(\eta)$ can be expressed as a linear combination of formal characters of irreducible representations $V(\gamma)$ with $\gamma\in S_\lambda$ and $\gamma\preceq\eta$:
\begin{align}\label{Lie algebra character of Verma into irreducible-1}
Q_{W(\eta)}=\sum_{\gamma\in S_\lambda,\gamma\preceq\eta}a_{\eta}^{\gamma}Q_{V(\gamma)}.
\end{align}
Furthermore, we have $a_{\eta,\eta}=1$.
\end{proposition}
\begin{lemma}\label{Lie algebra module multiplicity lemma}
Let $(V,\rho)$ be a representation of $\g$, possibly infinite dimensional, that decomposes as direct sum of weight spaces of finite multiplicity, and let $U$ be a nontrivial submodule of $V$. Then both $U$ and the quotient representation $V/U$ decompose as a direct sum of weight spaces. Furthermore, the multiplicity of any weight in $V$ is the sum of its multiplicity in $U$ and its multiplicity in $V/U$.
\end{lemma}
\begin{proof}
Let $u$ be an element of $U$. By assumption, we can decompose $u$ as $u=v_1+\cdots+v_k$ where the $v_i$'s belong to weight spaces in $V$ corresponding to distinct weights $\lambda_1,\dots,\lambda_k$. We wish to show that each $v_i$ actually belongs to $U$. If $k=1$, there is nothing to prove. If $j>1$, then $\lambda_k\neq\lambda_1$, which means that there is some $h\in\h$ for which $\lambda_1(h)\neq\lambda_k(h)$. Then apply to $u$ the operator $\rho(h)-\lambda_1(h)I$, we get
\begin{align}\label{Lie algebra module multiplicity lemma-1}
(\rho(h)-\lambda_1(h)I)u=\sum_{i=1}^{k}(\lambda_i(h)-\lambda_1(h))v_i.
\end{align}
Since the coefficient of $v_1$ is zero, the vector in $(\ref{Lie algebra module multiplicity lemma-1})$ is the sum of fewer than $k$ weight vectors. Thus, by induction on $k$, we can assume that each term on the right hand side of $(\ref{Lie algebra module multiplicity lemma-1})$ belongs to $U$. In particular, a nonzero multiple of $v_k$ belongs to $U$, which means $v_k$ itself belongs to $U$. Now, if $v_k$ is in $U$, then $u-v_k=v_1+\cdots+v_{k-1}$ is also in $U$. Thus, using induction again, we see $v_1$ also belongs to $U$.\par
We conclude that the sum of the weight spaces in $U$ is all of $U$. Since weight vectors with distinct weights are linearly independent, the sum must be direct. We turn, then, to the quotient space $V/U$. It is evident that the images of the weight spaces in $V$ are weight spaces in $V/U$ with the same weight. Thus, the sum of the weight spaces in $V/U$ is all of $V/U$ and, again, the sum must be direct.\par
Finally, consider a fixed weight $\gamma$ occurring in $V$, and let $V_\gamma$ be the associated weight space. Let $q_\gamma$ be the restriction to $V_\gamma$ of the quotient map $q:V\to V/U$. The kernel of $q_\gamma$ consists precisely of the weight vectors with weight $\gamma$ in $U$. Thus, the dimension of the image of $q_\gamma$, which is the weight space in $V/U$ with weight $\gamma$, is equal to $\dim V_\gamma-\dim(V_\gamma\cap U)$. The claim about multiplicities in $V$, $U$ and $V/U$ follows.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{Lie algebra character of Verma into irreducible}]
We actually prove a stronger result, that the formal character of any standard cyclic representation $U(\eta)$ with $\eta\in S_\lambda$ can be decomposed as in $(\ref{Lie algebra character of Verma into irreducible-1})$. As the proof of Proposition~\ref{Lie algebra standard cyclic module prop}, any such $U(\eta)$ decomposes as a direct sum of weight spaces with weights lower than $\eta$ and with the multiplicity of
the weight $\eta$ being $1$. For any such $U(\eta)$, let
\[M=\sum_{\gamma\in S_\lambda}\mult(\gamma).\]
Our proof will be by induction on $M$.\par
We first argue that if $M=1$, then $U(\eta)$ must be irreducible. If not, $U(\eta)$ would have a nontrivial invariant subspace and this subspace would, by Lemma~\ref{Lie algebra module multiplicity lemma}, decompose as a direct sum of weight spaces, all of which are lower than $\eta$. Thus, it would have to contain a weight vector $w$ that is annihilated by each $\rho(f_\alpha),\alpha\in\Phi^+$. Thus, $U(\eta)$ would contain a standard cyclic subspace $X$ with some highest weight $\gamma_0$. By Proposition~\ref{Lie algebra universal Casimir element}, the Casimir would act as the scalar $\|\gamma_0+\delta\|^2-\|\delta\|^2$ in $X$. On the other hand, since $X$ is contained in $U(\eta)$, the Casimir has to act as $\|\eta+\delta\|^2-\|\delta\|^2$ in $X$, which, since $\eta\in S_\lambda$, is equal to $\|\lambda+\delta\|^2-\|\delta\|^2$. Thus, $\gamma_0$ must belong to $S_\lambda$. Meanwhile, $\gamma_0$ cannot equal $\eta$ or else $X_0$ would be all of $U(\eta)$. Thus, both $\gamma_0$ and $\eta$ would have to have positive multiplicities and $M$ would have to be at least $2$.\par
Thus, when $M=1$, the representation $U(\eta)$ is irreducible, in which case the claim holds trivially. Assume now that the proposition holds for standard cyclic representations with $M<M_0$, and consider a representation $U(\eta)$ with $M=M_0$. If $U(\eta)$ is irreducible, there is nothing to prove. If not, then as we argued in the previous paragraph, $U(\eta)$ must contain a nontrivial invariant subspace $X$ that is standard cyclic with some highest weight $\gamma_0$ that belongs to $S_\lambda$ and is strictly lower than $\eta$. We can then form the quotient vector space $U/X$, which will still be standard cyclic with highest weight $\eta$. By Lemma~\ref{Lie algebra module multiplicity lemma}, the multiplicity of $\xi$ in $U(\eta)$ is the sum of the multiplicities of $\xi$ in $X$ and in $U/X$. Thus,
\[Q_{U(\eta)}=Q_X+Q_{U(\eta)/X}.\]

Now, both $X$ and $U(\eta)/X$ contain at least one weight in $S_\lambda$ with nonzero multiplicity, namely $\gamma_0$ for $X$ and $\eta$ for $U(\eta)/X$. Thus, both of these spaces must have $M<M_0$ and we may assume, by induction, that their formal characters decompose as a sum of characters of irreducible representations with highest weights in $S_\lambda$. These highest weights will be lower than $\eta$, in fact lower than $\gamma_0$ in the case of $X$. Thus, the character of $V(\eta)$ will not occur in the expansion of $Q_X$. But since $U(\eta)/X$ still has highest weight $\eta$, we may assume by induction that the character of $V(\eta)$ will occur exactly once in the expansion of $Q_{U(\eta)/X}$ and, thus, exactly once in the expansion of $Q_{U(\eta)}$.
\end{proof}
\begin{corollary}\label{Lie algebra character of irreducible into Verma}
If we enumerate the elements of $S_\lambda$ in nondecreasing order as $S_\lambda=\{\eta_1,\dots,\eta_r\}$, then the matrix $(A_{ij})=(a_{\eta_j}^{\gamma_i})$ is upper triangular with ones on the diagonal. Thus, $A$ is invertible, and the inverse of $A$ is also upper triangular with ones on the diagonal. It follows that we can invert the decomposition in $(\ref{Lie algebra character of Verma into irreducible-1})$ to a decomposition of the form
\begin{align}\label{Lie algebra character of irreducible into Verma-1}
Q_{V(\eta)}=\sum_{\gamma\in S_\lambda}b_{\eta}^{\gamma}Q_{W(\gamma)}
\end{align}
with $b^\eta_\eta=1$.
\end{corollary}
\begin{proof}
For any finite partially ordered set, it is possible to enumerate the elements in nondecreasing order. In our case, this means that we can enumerate the elements of $S_\lambda$ as $\eta_1,\dots,\eta_r$ in such a way that if $\eta_i\preceq\eta_j$ then $i\leq j$.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{Lie algebra exponential in character second condition}]
We apply $(\ref{Lie algebra character of irreducible into Verma-1})$ with $\eta=\lambda$, so that $Q_{V(\eta)}$ is the character $\chi^\lambda$ of the finite-dimensional, irreducible representation with highest weight $\lambda$. We then multiply both sides of $(\ref{Lie algebra character of irreducible into Verma-1})$ by theWeyl denominator $q$. Using the character formula for Verma modules, we obtain
\[q\cdot\chi^\lambda=\sum_{\gamma\in S_\lambda}b^\gamma_\lambda e^{(\gamma+\delta)}.\]
Since each $\gamma$ belongs to $S_\lambda$, each weight $\mu=\gamma+\delta$ occurring on the right-hand side of $(\ref{Lie algebra character of irreducible into Verma-1})$ satisfies $\|\mu\|=\|\lambda+\delta\|$. Thus, we have expressed $q\cdot\chi^\lambda$ as a linear combination of exponentials with weights $\eta$ satisfying Proposition~\ref{Lie algebra exponential in character second condition}. Since any such decomposition is unique, it must be the one obtained by multiplying together the exponentials in $q$ and $\chi^\lambda$.
\end{proof}
\subsection{Lie algebra cohomology}
The cohomology of Lie algebras is the natural tool to understand how we can build new Lie algebras $\hat{\g}$ from given Lie algebras $\g$ and $\n$ in such a way that $\n\unlhd\g$ and $\hat{\g}/\n\cong\g$. An important special case of this situation arises if $\n$ is assumed to be abelian, so that $\n$ is simply a $\g$-module. We shall see, in particular, how the abelian extensions of Lie algebras can be parameterized by a certain cohomology space. We shall also deal with the extension problem for g-modules, i.e., the problem to determine, for a pair $(V,W)$ of $\g$-modules, how many nonisomorphic modules $\tilde{V}$ exist which contain $W$ as a submodule and satisfy $\tilde{V}/W\cong V$. Throughout this part, $\g$ denotes a Lie algebra over the field $\K$.
\subsubsection{Basic definitions and properties}
Let $\g$ be a Lie algebra and $V$ a $\g$-module. We write $C^p(\g,V)$ for the space of alternating $p$-linear mappings $\g^p\to V$ (the $p$-cochains) and put $C^0(\g,V):=V$. We also define
\[C(\g,V):=\bigoplus_{p=1}^{\infty}C^p(\g,V)\]
On $C^p(\g,V)$, we define the \textbf{Chevalley-Eilenberg differential} $d$ by
\begin{align*}
d\omega(x_0,\dots,x_{p})&=\sum_{i=0}^{p}(-1)^ix_i\cdot\omega(x_1,\dots,\widehat{x}_i,\dots,x_{p})\\
&+\sum_{i<j}(-1)^{i+j}\omega([x_i,x_j],x_0,\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots,x_{p}).
\end{align*}
Putting the differentials on all the spaces $C^p(\g,V)$ together, we obtain a linear map $d:C(\g,V)\to C(\g,V)$.\par
The elements of the subspace
\[Z^p(\g,V)=\ker(d|_{C^p(\g,V)})\]
are called $p$-cocycles, and the elements of the spaces
\[B^p(\g,V)=\im(d|_{C^{p-1}(\g,V)}),\quad B^0(\g,V)=\{0\}\]
are called $p$-coboundaries. We will see below that $d^2=0$, which implies that $B^p(\g,V)\sub Z^p(\g,V)$, so that it makes sense to define the $p$-th cohomology space of $\g$ with values in the module $V$:
\[H^p(\g,V)=\frac{Z^p(\g,V)}{B^p(\g,V)}.\]
We further define for each $x\in\g$ and $p>0$ the \textbf{insertion map} or \textbf{contraction}
\[i_x:C^p(\g,V)\to C^{p-1}(\g,V),\quad(i_x\omega)(x_1,\dots,x_{p-1})=\omega(x,x_1,\dots,x_{p-1}).\]
We further define $i_x$ to be $0$ on $C^0(\g,V)$.
\begin{example}\label{Lie algebra differential low degree}
For elements of low degree, we have in particular:
\begin{align*}
d\omega(x)&=x\cdot\omega,\quad\quad\quad d\omega(x,y)=x\cdot\omega(y)-y\cdot\omega(x)-\omega([x,y])\\
d\omega(x,y,z)&=x\cdot\omega(y,z)-y\cdot\omega(x,z)+z\cdot\omega(x,y)-\omega([x,y],z)+\omega([x,z],y)-\omega([y,z],x)\\
&=x\cdot\omega(y,z)+y\cdot\omega(z,x)+z\cdot\omega(x,y)-\omega([x,y],z)-\omega([y,z],x)-\omega([z,x],y).
\end{align*}
\end{example}
\begin{example}
The space $Z^0(\g,V)=V^\g=\{v\in V:\g\cdot v=\{0\}\}$ is the maximal trivial submodule of $V$. Since $B^0(\g,V)$ is trivial by definition, we obtain $H^0(\g,V)=V^\g$.
\end{example}
\begin{example}
The elements $\omega\in Z^1(\g,V)$ are also called \textbf{crossed homomorphisms}. They are defined by the condition
\[\omega([x,y])=x\cdot\omega(y)-y\cdot\omega(x),\quad x,y\in\g.\]
The elements of the subspace $B^1(\g,V)$ are also called \textbf{principal
crossed homomorphisms}. They are given by the formula
\[\omega(x)=x\cdot v\]
where $v\in C^0(\g,V)=V$. It follows immediately from the definition of a
$\g$-module that each principal crossed homomorphism is a crossed homomorphism.\par
If $V$ is a trivial module, then it is not hard to compute the cohomology spaces in degree one. In this case $B^1(\g,V)=\{0\}$, so $H^1(\g,V)=Z^1(\g,V)$, and the condition that $\omega:\g\to V$ is a crossed homomorphism reduces to $\omega([x,y])=\{0\}$ for $x,y\in\g$. This leads to
\[H^1(\g,V)\cong\Hom_{\K}(\g/[\g,\g],V).\]
\end{example}
Our first goal will be to show that $d^2=0$. This can be proved directly by an lengthy computation. We will follow another way which is more conceptual and leads to additional insights and tools which are useful in other situations.
\begin{proposition}
For any $\g$-module $V$, there exists a representation $\mathcal{L}$ of $\g$ on $C(\g,V)$ given on the subspace $C^p(\g,V)$ by
\begin{align*}
(\mathcal{L}_x\omega)(x_1,\dots,x_p)&=x\cdot\omega(x_1,\dots,x_p)-\sum_{i=1}^p\omega(x_1,\dots,[x,x_i],\dots,x_p).
\end{align*}
\end{proposition}
\begin{proof}
We only need to show that $\mathcal{L}_{[x,y]}=[\mathcal{L}_x,\mathcal{L}_y]$. First, we note that
\begin{align*}
(\mathcal{L}_x\mathcal{L}_y\omega)(x_1,\dots,x_p)&=x\cdot(\mathcal{L}_y\omega)(x_1,\dots,x_p)-\sum_{i=1}^{p}(\mathcal{L}_y\omega)(x_1,\dots,[x,x_i],\dots,x_p).
\end{align*}
Now we compute the two terms on the right-hand side:
\begin{align*}
x\cdot(\mathcal{L}_y\omega)(x_1,\dots,x_p)=x\cdot y\cdot\omega(x_1,\dots,x_p)-\sum_{i=1}^{p}x\cdot\omega(x_1,\dots,[y,x_i],\dots,x_p),
\end{align*}
and
\begin{align*}
(\mathcal{L}_y\omega)&(x_1,\dots,[x,x_i],\dots,x_p)=y\cdot\omega(x_1,\dots,[x,x_i],\dots,x_p)\\
&-\sum_{j\neq i}\omega(x_1,\dots,[y,x_j],\dots,[x,x_i],\dots,x_p)-\omega(x_1,\dots,[y,[x,x_i]],\dots,x_p).
\end{align*}
Therefore we conclude that
\begin{equation*}\scriptstyle
\begin{aligned}
&(\mathcal{L}_x\mathcal{L}_y\omega)(x_1,\dots,x_p)=x\cdot y\cdot\omega(x_1,\dots,x_p)\\
&-\sum_{i=1}^{p}x\cdot\omega(x_1,\dots,[y,x_i],\dots,x_p)-\sum_{i=1}^{p}y\cdot\omega(x_1,\dots,[x,x_i],\dots,x_p)\\
&+\sum_{j\neq i}\omega(x_1,\dots,[y,x_j],\dots,[x,x_i],\dots,x_p)+\omega(x_1,\dots,[y,[x,x_i]],\dots,x_p)
\end{aligned}
\end{equation*}
Exchanging $x$ and $y$ and substracting, we then get
\begin{align*}
([\mathcal{L}_x,\mathcal{L}_y]\omega)(x_1,\dots,x_p)&=[x,y]\cdot\omega(x_1,\dots,x_p)+\omega(x_1,\dots,[y,[x,x_i]]-[x,[y,x_i]],\dots,x_p)\\
&=[x,y]\cdot\omega(x_1,\dots,x_p)+\omega(x_1,\dots,[[x,y],x_i],\dots,x_p)\\
&=(\mathcal{L}_{[x,y]}\omega)(x_1,\dots,x_p),
\end{align*}
which prove the claim.
\end{proof}
After defining the representation $\mathcal{L}$, we now give some important formulas for $\mathcal{L}$, which will be the key to proving $d^2=0$.
\begin{proposition}
The representation $\mathcal{L}:\g\to\gl(C(\g,V))$ satisfies, for $x\in\g$, the \textbf{Cartan formula}
\[\mathcal{L}_x=d\circ i_x+i_x\circ d.\]
\end{proposition}
\begin{proof}
Using the insertion map $i_{x_0}$, we can rewrite the formula for the differential as
\begin{equation*}\scriptstyle
\begin{aligned}
&(i_{x_0}d\omega)(x_1,\dots,x_p)=d\omega(x_0,x_1,\dots,x_p)\\
&=x_0\cdot\omega(x_1,\dots,x_p)+\sum_{i=1}^{p}(-1)^{i}x_i\cdot\omega(x_0,\dots,\widehat{x}_i,\dots,x_{p})\\
&+\sum_{j=1}^{p}(-1)^j\omega([x_0,x_j],x_1,\dots,\widehat{x}_j,\dots,x_p)+\sum_{1\leq i<j}(-1)^{i+j}\omega([x_i,x_j],x_0,\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots,x_{p})
\end{aligned}
\end{equation*}
Now it suffices to note that
\[x_0\cdot\omega(x_1,\dots,x_p)+\sum_{j=1}^{p}(-1)^j\omega([x_0,x_j],x_1,\dots,\widehat{x}_j,\dots,x_p)=(\mathcal{L}_{x_0}\omega)(x_1,\dots,x_p)\]
and that
\begin{equation*}\scriptstyle
\begin{aligned}
&\sum_{i=1}^{p}(-1)^{i}x_i\cdot\omega(x_0,\dots,\widehat{x}_i,\dots,x_{p})+\sum_{1\leq i<j}(-1)^{i+j}\omega([x_i,x_j],x_0,\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots,x_{p})\\
&=-\sum_{i=1}^{p}(-1)^{i-1}x_i\cdot\omega(x_0,\dots,\widehat{x}_i,\dots,x_p)-\sum_{1\leq i<j}(-1)^{i+j}\omega(x_0,[x_i,x_j],\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots,x_p)\\
&=-d(i_{x_0}\omega)(x_1,\dots,x_p).
\end{aligned}
\end{equation*}
Thus the claim is proved.
\end{proof}
\begin{proposition}\label{Lie algebra Lie derivative and action}
Let $V$ be a $\g$-module. Then any two elements $x,y\in\g$ satisfy $[\mathcal{L}_x,\rho_V(y)]=\rho_V([x,y])$.
\end{proposition}
\begin{proposition}\label{Lie algebra contraction braket}
Any two elements $x,y\in\g$ satisfy $i_{[x,y]}=[i_x,\mathcal{L}_y]$.
\end{proposition}
\begin{proof}
The explicit formula for $\mathcal{L}_y$ yields the relation
\begin{align*}
(i_{x_1}\mathcal{L}_y\omega)(x_2,\dots,x_p)=\mathcal{L}_y\omega)(x_1,\dots,x_p)=y\cdot\omega(x_1,\dots,x_p)-\sum_{i=1}^{p}\omega(x_1,\dots,[y,x_i],\dots,x_p),
\end{align*}
and
\begin{align*}
(\mathcal{L}_yi_{x_1}\omega)(x_2,\dots,x_p)=y\cdot\omega(x_1,\dots,x_p)-\sum_{i=2}^{p}\omega(x_1,\dots,[y,x_i],\dots,x_p).
\end{align*}
Therefore
\[([i_{x_1},\mathcal{L}_y]\omega)(x_2,\dots,x_p)=\omega([y,x_1],x_2,\dots,x_p)=(i_{x_1,y}\omega)(x_2,\dots,x_p),\]
which proves the claim.
\end{proof}
\begin{corollary}\label{Lie algebra Lie derivative commutes with d}
For each $x\in\g$, the Lie derivative $\mathcal{L}_x$ commutes with $d$.
\end{corollary}
\begin{proof}
In view of Proposition~\ref{Lie algebra contraction braket}, we obtain with the Cartan formula that $[\mathcal{L}_x,\mathcal{L}_y]=[d\circ i_x,\mathcal{L}_y]+[i_x\circ d,\mathcal{L}_y]$. Now note that
\begin{align*}
[d\circ i_x,\mathcal{L}_y]&=d\circ i_x\circ\mathcal{L}_y-\mathcal{L}_y\circ d\circ i_x\\
&=d\circ\mathcal{L}_y\circ i_x-\mathcal{L}_y\circ d\circ i_x+d\circ[i_x,\mathcal{L}_y]\\
&=[d,\mathcal{L}_y]\circ i_x+d\circ i_{[x,y]}.
\end{align*}
and that
\begin{align*}
[i_x\circ d,\mathcal{L}_y]&=i_x\circ d\circ\mathcal{L}_y-\mathcal{L}_y\circ i_x\circ d\\
&=i_x\circ d\circ\mathcal{L}_y-i_x\circ\mathcal{L}_y\circ d+[i_x,\mathcal{L}_y]\circ d\\
&=i_x\circ[d,\mathcal{L}_y]+i_{[x,y]}\circ d.
\end{align*}
Therefore we conlcude that
\[\mathcal{L}_{[x,y]}=[\mathcal{L}_x,\mathcal{L}_y]=[d,\mathcal{L}_y]\circ i_x+d\circ i_{[x,y]}+i_{[x,y]}\circ d+i_x\circ[d,\mathcal{L}_y]=[d,\mathcal{L}_y]\circ i_x+\mathcal{L}_{[x,y]}+i_x\circ[d,\mathcal{L}_y],\]
which implies that
\begin{align}\label{Lie algebra Lie derivative commutes with d-1}
[d,\mathcal{L}_y]\circ i_x+i_x\circ[d,\mathcal{L}_y]=0.
\end{align}
We now prove by induction over $p$ that $[d,\mathcal{L}_y]$ vanishes on $C^p(\g,V)$. For $\omega\in C^0(\g,V)\cong V$, we have
\begin{align*}
([d,\mathcal{L}_y]\omega)(x)&=(d\mathcal{L}_y\omega)(x)-(\mathcal{L}_y(d\omega))(x)=x\cdot(\mathcal{L}_y\omega)-y\cdot(d\omega(x))+(d\omega)([y,x])\\
&=x\cdot y\cdot\omega-y\cdot x\cdot\omega+[y,x]\cdot\omega=0.
\end{align*}
Suppose that $[d,\mathcal{L}_y]C^p(\g,V)=\{0\}$. Then $(\ref{Lie algebra Lie derivative commutes with d-1})$ implies that
\[i_x[d,\mathcal{L}_y]C^{p+1}(\g,V)=-[d,\mathcal{L}_y]i_x C^{p+1}(\g,V)\sub[d,\mathcal{L}_y]C^p(\g,V)=\{0\}\]
for each $x\in\g$, hence $[d,\mathcal{L}_y]C^{p+1}(\g,V)=\{0\}$. This finishes the induction and hence the proof.
\end{proof}
\begin{proposition}
The differential $d$ satisfies $d^2=0$.
\end{proposition}
\begin{proof}
By Corollary~\ref{Lie algebra Lie derivative commutes with d} and Cartan's formula,
\[0=[d,\mathcal{L}_y]=d^2\circ i_x-i_x\circ d^2.\]
For $\omega\in C^0(\g,V)\cong V$, we have
\[(d^2\omega)(x,y)=x\cdot d\omega(y)-y\cdot d\omega(x)-d\omega([x,y])=x\cdot y\cdot\omega-y\cdot x\cdot\omega-[x,y]\cdot\omega=0\]
and by induction we obtain $d^2=0$.
\end{proof}
Since the differential commutes with the action of $\g$ on the graded vector space $C(\g,V)$, the space of $p$-cocycles and of $p$-coboundaries is $\g$-invariant, so that we obtain a natural representation of $\g$ on the quotient spaces $H^p(\g,V)$. Note that Cartan's formula tells us that $\mathcal{L}_x$ is null homotopic, so we have the following claim.
\begin{proposition}
The action of $\g$ on $H^p(\g,V)$ is trivial, i.e., $\mathcal{L}_\g Z^p(\g,V)\sub B^p(\g,V)$.
\end{proposition}
\subsubsection{Extensions and cocycles}
In this part, we interpret the cohomology spaces in low degrees in terms of extensions of modules and Lie algebras.\par
First, as we have mentioned, the space $H^0(\g,V)$ is isomorphic to the maximal trivial subspace $V^\g$ of $V$, with trivial $\g$-action. Now we consider spaces of higher degree. Before giving the argument, we make the following definition.\par
Let $\g$ be a Lie algebra and $V$ and $W$ modules of $\g$. The short exact sequence
\[\begin{tikzcd}
0\ar[r]&W\ar[r,"\iota"]&U\ar[r,"q"]&V\ar[r]&0
\end{tikzcd}\]
is called an \textbf{extension} of $V$ by $W$. If we identify $V$ with its image in $U$, this means that $U$ is a $\g$-module containing $V$ as a submodule such that $U/W\cong V$.\par
Two extensions $U_1$ and $U_2$ are called equivalent if there exists a $\g$-module homomorphism $\varphi:U_1\to U_2$ such that the diagram
\[\begin{tikzcd}
0\ar[r]&W\ar[d,equal]\ar[r]&U_1\ar[d,"\varphi"]\ar[r]&V\ar[r]\ar[d,equal]&0\\
0\ar[r]&W\ar[r]&U_2\ar[r]&V\ar[r]&0
\end{tikzcd}\]
commutes. We write $\Ext_\g(V,W)$ for the set of equivalence classes of module extensions of $V$ by $W$. We call an extension \textbf{trivial}, or \textbf{splits}, if there exists a homomorphism $\sigma:W\to U$ with $q\circ\sigma=\id_V$. In this case, the map
\[V\oplus W\to U,\quad (v,w)\mapsto(\iota(v),\sigma(w))\]
is a $\g$-module isomorphism.
The following proposition gives a cohomological interpretation of the set $\Ext_\g(V,W)$ for two $\g$-modules $V$ and $W$. In particular, it shows that this set carries a natural vector space structure.
\begin{proposition}\label{Lie algebra cohomology H^1 extension char}
For $\g$-modules $(V,\rho_V)$ and $(W,\rho_W)$,
\[\Ext_\g(V,W)\cong H^1(\g,\Hom(V,W))\]
where the representation of $\g$ on $\Hom(V,W)$ is given by
\[x\cdot\varphi=\rho_W(x)\varphi-\varphi\rho_V(x)\]
\end{proposition}
\begin{proof}
For any $\omega\in C^1(\g,\Hom(V,W))$, we define a module
\[U_\omega=V\oplus W\quad\text{with}\quad x\cdot(v,w)=(x\cdot v,x\cdot w+\omega(x)(v)).\]
It is easy to check that $U_\omega$ is a $\g$-module if and only if $\omega\in Z^1(\g,\Hom(V,W))$. Now we will show that any extension of $V$ by $W$ can be written into $U_\omega$ for some $\omega\in Z^1(\g,\Hom(V,W))$.\par
Let 
\[\begin{tikzcd}
0\ar[r]&W\ar[r,"\iota"]&U\ar[r,"q"]&V\ar[r]&0
\end{tikzcd}\]
be such an extension, and we consider $W$ as a submodule of $U$. Choose a linear map $\sigma:V\to U$ with $q\circ\sigma=\id_V$ (since an extension of vector spaces always splits, this is possible), we define a map $\omega_{\sigma}:\g\to\Hom(V,W)$ by
\[\omega_{\sigma}(x)(v)=(d\sigma)(x)(v)=(x\cdot\sigma)(v)=x\cdot\sigma(v)-\sigma(x\cdot v).\]
It is easy to see $\omega_{\sigma}$ is linear, and
\[q(\omega_\sigma(x)(v))=q(x\cdot\sigma(v))-q(\sigma(x\cdot v))=x\cdot q(\sigma(v))-x\cdot v=0\]
so $\omega_\sigma(x)(v)\in\ker q=W$ for each $x\in\g$ and $v\in V$, whence $\omega_\sigma\in C^1(\g,\Hom(V,W))$. Also, we note that 
\begin{align*}
(x\cdot\omega_{\sigma}(y))(v)&=x\cdot(\omega_{\sigma}(y)(v))-\omega_{\sigma}(y)(x\cdot v)\\
&=x\cdot y\cdot\sigma(v)-x\cdot\sigma(y\cdot v)-y\cdot\sigma(x\cdot v)+\sigma(y\cdot x\cdot v),
\end{align*}
and so
\begin{align*}
(x\cdot\omega_{\sigma}(y))(v)-(y\cdot\omega_{\sigma}(x))(v)&=[x,y]\cdot v-\sigma([x,y]\cdot v)=\omega_{\sigma}([x,y])(v).
\end{align*}
Thus $\omega_\sigma\in Z^1(\g,\Hom(V,W))$, and $U_{\omega_\sigma}$ defines a $\g$-module. Now we consider the following linear map
\[\varphi:U_{\omega_{\sigma}}\to U,\quad (v,w)\mapsto \sigma(v)+w.\]
From the definition of $\omega_{\sigma}$, it is easy to see that $\varphi$ is a homomorphism:
\begin{align*}
\varphi(x\cdot(v,w))&=\varphi(x\cdot v,x\cdot w+\omega_{\sigma}(x)(v))=\sigma(x\cdot v)+x\cdot w+\omega_\sigma(x)(v)\\
&=x\cdot w+x\cdot\sigma(v)=x\cdot\varphi(v,w).
\end{align*}
Also, since we have $q\circ\sigma=\id_V$, $\varphi$ induces an equivalence between the extensions $U_\sigma$ and $U$.\par
Now for any $\gamma\in\Hom(V,W)$, since $\omega_\sigma=d\sigma$, we have $\omega_{\sigma}+d\gamma=\omega_{\sigma+\gamma}$. In this case, the bijective linear map
\[\eta_\gamma:U_{\omega_{\sigma+\gamma}}\to U_{\omega_\sigma} ,\quad (v,w)\mapsto(v,w+\gamma(v))\]
is an isomorphism because
\begin{equation}\label{Lie algebra cohomology H^1 extension char-1}
\begin{aligned}
\eta_\gamma(x\cdot(v,w))&=\eta_\gamma(x\cdot v,x\cdot w+\omega_\sigma(x)(v))=(x\cdot v,x\cdot w+\gamma(x\cdot v)+\omega_{\sigma+\gamma}(x)(v))\\
&=(x\cdot v,x\cdot w+\gamma(x\cdot v)+\omega_{\sigma}(x)(v)+(d\gamma)(x)(v))\\
&=(x\cdot v,x\cdot w+x\cdot\gamma(v)+\omega_{\sigma}(x)(v))=x\cdot(v,w+\gamma(v))=x\cdot\eta_\gamma(v,w).
\end{aligned}
\end{equation}
Since the linear map $\sigma:V\to U$ satisfies $q\circ\sigma=\id_V$ and $\ker q=W$, if $\sigma':V\to U$ is another linear map with $q\circ\sigma'=\id_V$, then we have
\[\gamma:=\sigma-\sigma'\in\Hom(V,W).\]
Whence $\omega_\sigma=\omega_{\sigma'}+d\gamma$ and $[\omega_\sigma]=[\omega_{\sigma'}]$. These together porve that, for each extension $U$, we can associate a unique element $[\omega_U]$ in $H^1(\g,\Hom(V,W))$. Conversely, an element $[\omega]$ in $H^1(\g,\Hom(V,W))$ determines a unique extension $U_{[\omega]}$ of $V$ by $W$. Moreover, we have seen that $U_{[\omega_U]}\cong U$.\par
It remains to show that if $U_\omega\cong U_{\omega'}$ then $[\omega]=[\omega']$. Since if this holds, then from the equality $U_{[\omega_{U_{[\omega]}}]}=U_{[\omega]}$, we see $[\omega]=[U_{[\omega]}]$, and the proof is finished. To this end, let $\varphi:U_\omega\to U_{\omega'}$ be an equivalence of extensions. Then from the commutative diagram, for all $v\in V$ and $w\in W$, we have
\[\varphi(0,w)=(0,w),\quad v=q(v,w)=q\circ\varphi(v,w).\]
Since $q$ is the projection from $V\oplus W$ to $V$, these imply that 
\[\varphi(v,w)=(v,\Gamma(v,w))\]
where $\Gamma\in\Hom(V\oplus W,W)$ satisfies $\Gamma(0,w)=w$. Since $\Gamma$ is linear, we have
\[\Gamma(v,w)=\Gamma(v,0)+\Gamma(0,w)=w+\Gamma(v,0).\]
Write $\gamma(v)=\Gamma(v,0)$, we then get $\varphi(v,w)=(v,w+\gamma(v))$. Now by a computation like $(\ref{Lie algebra cohomology H^1 extension char-1})$, since $\varphi$ is a homomorphism, we conclude that $\omega=\omega'+d\gamma$. This proves $[\omega]=[\omega']$, hence finishes the proof.
\end{proof}
View $\K$ as a trivial $\g$-module, we see $\Hom(\K,V)$ is isomorphic to $V$ as $\g$-modules. Therefore we get the following corollary.
\begin{corollary}\label{Lie algebra cohomology extension by trivial module}
For a $\g$-module $V$, we have $\Ext_\g(\K,V)\cong H^1(\g,V)$.
\end{corollary}
Now we consider extension of Lie algebras. Let $\g$ and $\n$ be Lie algebras. The short exact sequence
\[\begin{tikzcd}
0\ar[r]&\n\ar[r,"\iota"]&\hat{\g}\ar[r,"q"]&\g\ar[r]&0
\end{tikzcd}\]
is called an \textbf{extension} of $\g$ by $\n$. If we identify $\n$ with its image in $\hat{\g}$, this means that $\hat{\g}$ is a Lie algebra containing $\n$ as an ideal such that $\hat{\g}/\n\cong\g$. If $\n$ is \textbf{abelian} (\textbf{central}) in $\hat{\g}$, then the extension is called abelian (central). Two extensions $\hat{\g}_1$ and $\hat{\g}_2$ are called equivalent if there exists a Lie algebra isomomorphism $\varphi:\hat{\g}_1\to\hat{\g}_2$ such that the diagram
\[\begin{tikzcd}
0\ar[r]&\n\ar[r,"\iota_1"]\ar[d,equal]&\hat{\g}_1\ar[d,"\varphi"]\ar[r,"q_1"]&\g\ar[d,equal]\ar[r]&0\\
0\ar[r]&\n\ar[r,"\iota_2"]&\hat{\g}_2\ar[r,"q_2"]&\g\ar[r]&0
\end{tikzcd}\]
is commutative. We call an extension $q:\hat{\g}\to\g$ trivial, or say that the extension splits, if there exists a Lie algebra homomorphism $\sigma:\g\to\hat{\g}$ with $q\circ\sigma=\id$. In this case, the map
\[\n\rtimes_\delta\g\to\hat{\g},\quad (n,x)\mapsto n+\sigma(x)\]
is an isomorphism, where the semidirect sum is defined by the homomorphism
\[\delta:\g\to\Der(\n),\quad \delta(x)(n)=[\sigma(x),n].\]
For a trivial central extension, we have $\delta=0$, and therefore $\hat{\g}\cong\n\times\g$.\par
A particular important case arises if $\n$ is abelian. In this case, each Lie algebra extension $q:\hat{\g}\to\g$ of $\g$ by $\n$ leads to a $\g$-module structure on $\n$ defined by
\[x\cdot n:=[q^{-1}(x),n]\]
where $x\in\g$, $n\in\n$. This is well-defined because $[\n,\n]=\{0\}$. Note that if $q_1:\hat{\g}_1\to\g$ and $q_2:\hat{\g}_2\to\g$ are equivalent extensions and $\varphi:\hat{\g}_1\to\hat{\g}_2$ is an equivalence, then from the commutative diagram we have
\[\varphi([q_1^{-1}(x),\iota_1(n)])=[\varphi(q_1^{-1}(x)),\varphi(\iota_1(n))]=[q_2^{-1}(x),\iota_2(n)],\]
thus equivalent extensions lead to the same module structure. Therefore, it makes sense to write $\Ext_\rho(\g,\n)$ for the set of equivalence classes of extensions of $\g$ by $\n$ corresponding to the module structure given by the representation $\rho:\g\to\gl(\n)$.\par
For a $\g$-module $V$, we also write $\Ext(\g,V):=\Ext_{\rho_V}(\g,V)$, where $\rho_V$ is the representation of $\g$ on $V$ corresponding to the module structure and $V$ is considered as an abelian Lie algebra.
\begin{theorem}\label{Lie algebra cohomology H^2 extension char}
Let $V$ be a $\g$-module.
\begin{itemize}
\item[(a)] For an element $\omega\in C^2(\g,V)$, the formula
\[[(x,v),(y,w)]=([x,y],x\cdot w-y\cdot v+\omega(x,y))\]
defines a Lie bracket on $\g\times V$ if and only if $\omega\in Z^2(\g,V)$.
\item[(b)] For a cocycle $\omega\in Z^2(\g,V)$, the corresponding Lie algebra $\g_\omega:=\g\oplus_\omega V$ is an extension of $\g$ by $V$.
\item[(c)] The map $Z^2(\g,V)\to\Ext(\g,V)$ defined by assigning to $\omega$ the equivalence class of the extension $\g_\omega$ induces a bijection
\[H^2(\g,V)\to\Ext(\g,V)\]
Therefore, $H^2(\g,V)$ classifies the abelian extensions of $\g$ by $V$ for which the corresponding representation of $\g$ on $V$ is given by the module structure on $V$.
\end{itemize}
\end{theorem}
\begin{proof}
For (a), we only need to verify the Jacobi identity. Note that
\begin{align*}
\big[[(x,u),(y,v)],(z,w)\big]&=\big[([x,y],x\cdot v-y\cdot u+\omega(x,y)),(z,w)\big]\\
&=\big([[x,y],z],[x,y]\cdot w-z\cdot(x\cdot v-y\cdot u+\omega(x,y))+\omega([x,y],z)\big)\\
&=\big([[x,y],z],[x,y]\cdot w-z\cdot x\cdot v+z\cdot y\cdot u-z\cdot\omega(x,y)+\omega([x,y],z)\big).
\end{align*}
Therefore
\begin{align*}
&[[(x,u),(y,v)],(z,w)]+[[(y,v),(z,w)],(x,u)]+[[(z,w),(x,u)],(y,v)]\\
&=\big([[x,y],z],[x,y]\cdot w-z\cdot x\cdot v+z\cdot y\cdot u-z\cdot\omega(x,y)+\omega([x,y],z)\big)\\
&+\big([[y,z],x],[y,z]\cdot u-x\cdot y\cdot w+x\cdot z\cdot v-x\cdot\omega(y,z)+\omega([y,z],x)\big)\\
&+\big([[z,x],y],[z,x]\cdot v-y\cdot z\cdot u+y\cdot x\cdot w-y\cdot\omega(z,x)+\omega([z,x],y)\big)\\
&=\big(0,\omega([x,y],z)+\omega([y,z],x)+\omega([z,x],y)-x\cdot\omega(y,z)-y\cdot\omega(z,x)-z\cdot(x,y)\big).
\end{align*}
By the definition of $d\omega$, this proves the first claim. For (b), it remains to note that, if $q:\g_\omega\to \g$ is the projection map and $\iota:V\to\g_\omega$ is the inclusion, then
\[[q^{-1}(x),\iota(v)]=[(x,w),(0,v)]=(0,\rho_V(x)v)=\iota(\rho_V(x)v).\]
Therefore $\g_\omega$ is an extension of $\g$ by $V$.\par
To see that every abelian Lie algebra extension $q:\hat{\g}\to\g$
with $\ker q=V$ is equivalent to some $\g_\omega$, let $\sigma:\g\to\hat{\g}$ be a linear map with $q\circ\sigma=\id$. Such a section exists, because the sequence of vector spaces is split, while $\sigma$ cannot be taken to be a morphism of Lie algebras in general. Note that by our assumption, the action of $\g$ on $V$ is given by
\[x\cdot v=[q^{-1}(x),v]=[\sigma(x),v]\]
for $x\in\g$ and $v\in V$.\par
We associate to $\sigma$ the failure to be a morphism of Lie algebras:
\[\omega_\sigma(x,y)=[\sigma(x),\sigma(y)]-\sigma([x,y])\]
for $x,y\in\g$. As $q$ is a homomorphism of Lie algebras, we have $q(\omega_\sigma(x,y))=0$, hence $\omega_\sigma$ has its values in $V$. 
Also, $\omega_\sigma$ is a cocycle because
\begin{align*}
&x\cdot\omega_\sigma(y,z)+y\cdot\omega_\sigma(z,x)+z\cdot\omega(x,y)\\
&=x\cdot[\sigma(y),\sigma(z)]-x\cdot\sigma([y,z])+y\cdot[\sigma(z),\sigma(x)]-y\cdot\sigma([z,x])+z\cdot[\sigma(x),\sigma(y)]-z\cdot\sigma([x,y])\\
&=[\sigma(x),[\sigma(y),\sigma(z)]]-[\sigma(x),\sigma([y,z])]+[\sigma(y),[\sigma(z),\sigma(x)]]-[\sigma(y),\sigma([z,x])]\\
&\,+[\sigma(z),[\sigma(x),\sigma(y)]]-[\sigma(z),\sigma([x,y])]\\
&=-[\sigma(x),\sigma([y,z])]-[\sigma(y),\sigma([z,x])]-[\sigma(z),\sigma([x,y])]\\
&=\omega_\sigma([x,y],z)+\omega_\sigma([y,z],x)+\omega_\sigma([z,x],y).
\end{align*}

Now we define the map $\varphi:\g_{\omega_\sigma}\to\hat{\g}$ by $\varphi(x,v)=\sigma(x)+v$. It is clear that $\varphi$ is bijective. It is a Lie algebra homomorphism because
\begin{align*}
\varphi([(x,v),(y,w)])&=\varphi([x,y],x\cdot w-y\cdot v+[\sigma(x),\sigma(y)]-\sigma([x,y]))\\
&=x\cdot w-y\cdot v+[\sigma(x),\sigma(y)]\\
&=[\sigma(x),w]-[\sigma(y),v]+[\sigma(x),\sigma(y)]\\
&=[\sigma(x)+v,\sigma(y)+w],
\end{align*}
where we use the fact that $V$ is an abelian Lie algebra. Since $q\circ\sigma=\id$, it is easy to see $\varphi$ is an equivalence of extensions, and therefore that the map $Z^2(\g,V)\to\Ext(\g,V)$ is surjective.\par
Finally, it remains to prove that Lie algebras $\g_\omega$ and $\g_{\omega'}$ are equivalent as $V$-extensions of $\g$ if and only if $[\omega]=[\omega']$. To see this, first let $\omega=\omega'+d\gamma$ where $\gamma\in C^1(\g,V)$. Then consider the map
\[\psi:\g_\omega\to\g_{\omega'},\quad (x,v)\mapsto(x,\gamma(x)+v).\]
We note that $\psi$ is a homomorphism:
\begin{equation}
\begin{aligned}\label{Lie algebra cohomology H^2 extension char-1}
\psi([(x,v),(y,w)])&=\psi([x,y],x\cdot w-y\cdot v+\omega(x,y))\\
&=\gamma([x,y])+x\cdot w-y\cdot v+\omega'(x,y)+d\gamma(x,y)\\
&=x\cdot w-y\cdot v+\omega'(x,y)+x\cdot\gamma(y)-y\cdot\gamma(x)\\
&=[(x,\gamma(x)+v),(y,\gamma(y)+w)]=[\psi(x,v),\psi(y,w)].
\end{aligned}
\end{equation}
Since $\psi$ is bijective, it follows that $\g_\omega\cong\g_{\omega'}$ as extensions.\par
Conversely, let $\varphi:\g_{\omega}\to\g_{\omega'}$ be an equivalence of extensions. Then from the commutative diagram we get
\[\varphi(0,v)=(0,v),\quad q\circ\varphi(x,v)=x.\]
It follows that there exists a linear map $\gamma:\g\to V$ such that
\[\varphi(x,v)=(x,\gamma(x)+v)\for x\in\g,v\in V.\]
From this and the computation $(\ref{Lie algebra cohomology H^2 extension char-1})$, it follows that $\omega=\omega'+d\gamma$, so $[\omega]=[\omega']$. This finishes the proof.
\end{proof}
\subsubsection{Invariant volume forms and cohomology}
In this part, we characterize the existence of volume forms in terms of cohomology. Recall that we have an action $\mathcal{L}$ of $\g$ on $C(\g,V)$. A form $\mu\in C^p(\g,V)$ is called \textbf{invariant under $\g$} if $\mathcal{L}_\g\mu=\{0\}$.
\begin{proposition}\label{Lie algebra invariant volume form}
For a $n$-dimensional real Lie algebra $\g$, the following are equivalent:
\begin{itemize}
\item[(a)] $\tr(\ad(x))=0$ for all $x\in\g$.
\item[(b)] $\g$ carries a volume form $\mu\in C^n(\g,\R)$ invariant under $\g$.
\item[(c)] $H^n(\g,\R)\neq\{0\}$.
\item[(d)] $\dim H^n(\g,\R)=1$.
\end{itemize}
\end{proposition}
\begin{proof}
Let $x_1,\dots,x_n$ be a basis for $\g$ and $x_1^*,\dots,x_n^*$ the dual basis for $\g^*$. Then $\mu=x_1^*\wedge\cdots\wedge x_n^*$ is a nonzero volume form on $\g$, so that $C^n(\g,\R)=\R\mu$.\par
First, we claim that for $x\in\g$, we have
\[\mathcal{L}_x\mu=-\tr(\ad(x))\mu\]
In fact, let $[x,x_j]=\sum_{k=1}^{n}a_{kj}x_k$. Then since $\R$ is regarded as a trivial $\g$-module, we have
\begin{align*}
(\mathcal{L}_x\mu)(x_1,\dots,x_n)&=-\sum_{j=1}^{n}\mu(x_1,\dots,[x,x_j],\dots,x_n)=-\sum_{j=1}^{n}a_{jj}\mu(x_1,\dots,x_j,\dots,x_n)\\
&=-\sum_{j=1}^{n}a_{jj}=-\tr(\ad(x)).
\end{align*}
Therefore, $\tr(\ad(x))=0$ if and only if $\mathcal{L}_x\mu=0$ for all $x\in\g$. This proves (a)$\Leftrightarrow$(b). Also, the equivalence (c)$\Leftrightarrow$(d) is clear.\par
Now we prove that (b)$\Leftrightarrow$(c). For each $j$, we have
\[i_{x_j}\mu=(-1)^{j-1}x_1^*\wedge\cdots\wedge\hat{x_j^*}\wedge\cdots\wedge x_n^*\]
and these elements form a basis for $C^{n-1}(\g,\R)$. Therefore, $d\mu=0$ and the Cartan Formula imply that
\[B^n(\g,\R)=d(i_\g\mu)=\mathcal{L}_\g\mu.\]
This space vanishes if and only if $\mu$ is invariant if and only if $H^n(\g,\R)$ is nonzero.
\end{proof}
For reasons that we shall understand later in our discussion of invariant measures on Lie groups, Lie algebras satisfying the equivalent conditions from the preceding proposition are called unimodular.
\subsubsection{Cohomology of semisimple Lie algebras}
In this part, we discuss the connection between Weyl's and Levi's Theorems and a more general theorem of J.H.C. Whitehead concerning the cohomology of finite-dimensional modules of semisimple Lie algebras.\par
If $\g$ is a semisimple finite-dimensional Lie algebra, then Weyl's Theorem states that every finite-dimensional $\g$-module $V$ is semisimple. This means that every submodule has a module complement, and therefore that all module extensions are trivial. In view of Proposition~\ref{Lie algebra cohomology H^1 extension char} and Corollary~\ref{Lie algebra cohomology extension by trivial module}, this is equivalent to
\[H^1(\g,\Hom(V,W))=\{0\},\quad H^1(\g,V)=\{0\}\]
for each pair of finite-dimensional $\g$-modules $V$ and $W$.
\begin{lemma}[\textbf{First Whitehead Lemma}]
If $\g$ is a finite-dimensional semisimple Lie algebra, then each finite-dimensional $\g$-module $V$ satisfies $H^1(\g,V)=\{0\}$.
\end{lemma}
Now let $\omega\in Z^2(\g,V)$ be a $2$-cocycle and $\g_\omega:=\g\oplus_\omega V$ the corresponding abelian Lie algebra extension of $\g$ by $V$. Then $V=\rad(\g_\omega)$ because $V$ is an abelian ideal of g and the quotient $\g_\omega/V\cong\g$ is semisimple. Therefore, Levi's Theorem implies the existence of a Lie algebra homomorphism $\sigma:\g\to\g_\omega$ satisfying $q\circ\sigma=\id$. Now Proposition~\ref{Lie algebra cohomology H^2 extension char} implies that $\omega$ is a coboundary, which leads to the
\begin{lemma}[\textbf{Second Whitehead Lemma}]
If $\g$ is a finite-dimensional semisimple Lie algebra, then each finite-dimensional $\g$-module $V$ satisfies $H^2(\g,V)=\{0\}$.
\end{lemma}
Again we see from the argument above that the Second Whitehead Lemma is essentially the case of Levi's Theorem where $\varphi:\g\to\s$ is a surjective homomorphism onto a semisimple Lie algebra $\s$ with abelian kernel, but this was the crucial case in the proof of Levi's Theorem.\par
We now aim at more general results concerning also the higher degree cohomology of modules of semisimple Lie algebras.
\begin{lemma}\label{Lie algebra semisimple differential on trivial module}
If $\omega\in C^p(\g,V)^{\g}$ is a $\g$-invariant element with respect to the action of $\g$, and $d^0$ is the Chevalley-Eilenberg differential with respect to the trivial $\g$-module structure on $V$, then $d\omega=-d^0\omega$.
\end{lemma}
\begin{proof}
First, we note that for $x_0,\dots,x_p\in\g$, the invariance of $\omega$ implies that
\begin{equation*}\scriptstyle
\begin{aligned}
&x_i\cdot\omega(x_0,\dots,\widehat{x}_i,\dots,x_p)\\
&=\sum_{j<i}\omega(\dots,[x_i,x_j],\dots,\widehat{x}_i,\dots)+\sum_{j>i}\omega(\dots,\widehat{x}_i,\dots,[x_i,x_j],\dots)\\
&=\sum_{j<i}(-1)^j\omega([x_i,x_j],\dots,\widehat{x}_j,\dots,\widehat{x}_i,\dots)+\sum_{j>i}(-1)^{j+1}\omega([x_i,x_j],\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots).
\end{aligned}
\end{equation*}
This leads to
\begin{equation*}\scriptstyle
\begin{aligned}
&\sum_{i=0}^{p}(-1)^ix_i\cdot\omega(x_0,\dots,\widehat{x}_i,\dots,x_p)\\
&=\sum_{j<i}(-1)^{i+j}\omega([x_i,x_j],\dots,\widehat{x}_j,\dots,\widehat{x}_i,\dots)+\sum_{j>i}(-1)^{i+j+1}\omega([x_i,x_j],\dots,\widehat{x}_i,\dots,\widehat{x}_j,\dots)\\
&=\sum_{j<i}(-1)^{i+j}\omega([x_i,x_j],\dots,\widehat{x}_j,\dots,\widehat{x}_i,\dots)+\sum_{j<i}(-1)^{i+j+1}\omega([x_j,x_i],\dots,\widehat{x}_j,\dots,\widehat{x}_i,\dots)\\
&=2\sum_{j<i}(-1)^{i+j}\omega([x_i,x_j],\dots,\widehat{x}_j,\dots,\widehat{x}_j,\dots)=-2(d^0\omega)(x_0,\dots,x_p).
\end{aligned}
\end{equation*}
Thus we conclude $d\omega=d^0\omega-2d^0\omega=-d^0\omega$.
\end{proof}
\begin{proposition}\label{Lie algebra semisimple cohomology by invariant form}
If $V$ is a finite-dimensional module over the semisimple Lie algebra $\g$, then
\[H^p(\g,V)=Z^p(\g,V)^\g/B^p(\g,V)^\g,\]
i.e., each cohomology class can be represented by an invariant cocycle.
\end{proposition}
\begin{proof}
From Proposition~\ref{Lie semisimple algebra representation V^g+V_eff}, we obtain for each $p\in\N$ the decomposition
\[Z^p(\g,V)=Z^p(\g,V)^\g\oplus\g\cdot Z^p(\g,V).\]
Since $\g$ acts trivially on the quotient space $H^p(\g,V)$, we have $\g\cdot Z^p(\g,V)\sub B^p(\g,V)$, and the assertion follows.
\end{proof}
\begin{proposition}
If $V$ is a trivial $\g$-module, then
\[H^p(\g,V)\cong Z^p(\g,V)^\g=C^p(\g,V)\for p\in\N.\]
\end{proposition}
\begin{proof}
The preceding proposition shows that each cohomology class is represented by an invariant cocycle. Further, Proposition~\ref{Lie algebra semisimple differential on trivial module} and the triviality of $V$ implies that each $\g$-invariant cochain $\omega$ satisfies $d^0\omega=d\omega=-d^0\omega$, so that $\omega$ is a cocycle. This shows $C^p(\g,V)^\g=Z^p(\g,V)^\g$.\par
To see that $B^p(\g,V)^{\g}$ vanishes, we note that we have the direct sum decomposition
\[C^{p-1}(\g,V)=C^{p-1}(\g,V)^\g\oplus\g\cdot C^{p-1}(\g,V)\]
and likewise
\[B^{p}(\g,V)=B^{p}(\g,V)^\g\oplus\g\cdot B^{p}(\g,V).\]
Now since $d$ commutes with the action of $\g$, we have
\[d(\g\cdot C^{p-1}(\g,V))\sub\g\cdot d(C^{p-1}(\g,V))=\g\cdot B^p(\g,V),\]
and therefore
\[B^p(\g,V)^\g=d(C^{p-1}(\g,V)^\g)=d(Z^{p-1}(\g,V)^\g)=\{0\}.\]
so $H^p(\g,V)=Z^p(\g,V)^\g$ and the claim is proved.
\end{proof}
\begin{example}
For each semisimple Lie algebra $\g$, we have $H^3(\g,\K)\neq\{0\}$. In fact, the invariance of the Cartan-Killing form $\kappa$ implies that
\[\Gamma(\kappa)(x,y,z):=\kappa([x,y],z)\]
is an invariant alternating $3$-form on $\g$, hence a nonzero element of $C^3(\g,\K)\cong H^3(\g,\K)$.
\end{example}
\begin{theorem}[\textbf{Whitehead's Vanishing Theorem}]
If $V$ is a finite-dimensional module over the semisimple Lie algebra $\g$ with $V^\g=\{0\}$, then $H^p(\g,V)=\{0\}$ for any $p\in\N$.
\end{theorem}
\begin{proof}
First we note that if $V$ is a real module over the real Lie algebra $\g$ and $V_\C$ the corresponding complex $\g_\C$-module, then any alternating $p$-form $\omega\in C^p(\g,V)$ extends uniquely to a complex $p$-linear map $\omega_\C\in C^p(\g_\C,V_\C)$ and we have an embedding
\[C^p(\g,V)\hookrightarrow C^p(\g_\C,V_\C).\]
The image of this map is the set of all elements $\eta\in C^p(\g_{\C},V_{\C})$, whose values on $\g^p$ lie in the real subspace $V\cong 1\otimes V\sub V_\C$. Since any $\eta\in C^p(\g_{\C},V_{\C})$ can be written in a unique fashion as a $\eta_1+i\eta_2$ with $\eta_j(\g^p)\sub V$, we see that
\[C^p(\g_\C,V_\C)\cong C^p(\g,V)_\C.\]
The subspaces of cocycles and coboundaries inherit the corresponding property, and we conclude that $H^p(\g_\C,V_\C)=H^p(\g,V)_\C$. Therefore, we may assume that $\g$ and $V$ are complex.\par
In view of Weyl's Theorem, the condition $V^\g=\{0\}$ implies that $V$ is a direct sum of simple nontrivial $\g$-modules $\bigoplus_{i=1}^{m}V_i$. Then
\[H^p(\g,V)\cong\bigoplus_{i=1}^{m}H^p(\g,V_i),\]
so that we may also assume that $V$ is a simple nontrivial module.\par
In view of Proposition~\ref{Lie algebra semisimple cohomology by invariant form}, we have to show that $Z^p(\g,V)^\g\sub B^p(\g,V)^\g$. For $p=0$, this follows from our assumption $Z^0(\g,V)=V^\g=\{0\}$. Let $x_1,\dots,x_n$ be a basis for $\g$ and $x^1,\dots,x^n$ the dual basis with respect to the Cartan-Killing form. For $p>0$, we then define a linear map
\[\Gamma:C^p(\g,V)\to C^{p-1}(\g,V),\quad\Gamma(\omega):=\sum_{j=1}^{n}\rho_V(x_j)\circ i_{x^j}\omega.\]
First, we show that $\Gamma$ is $\g$-equivariant. For $x\in\g$, we have by Proposition~\ref{Lie algebra Lie derivative and action} and Proposition~\ref{Lie algebra contraction braket} that
\begin{align*}
\mathcal{L}_x\Gamma(\omega)&=\sum_{j=1}^{n}\mathcal{L}_x\circ\rho_V(x_j)\circ i_{x^j}\omega=\sum_{j=1}^{n}\rho_V([x,x_j])\circ i_{x^j}\omega+\sum_{j=1}^{n}\rho_V(x_j)\circ\mathcal{L}_x\circ i_{x^j}\omega\\
&=\sum_{j=1}^{n}\rho_V([x,x_j])\circ i_{x^j}\omega+\sum_{j=1}^{n}\rho_V(x_j)\circ i_{[x_j,x^j]}\omega+\sum_{j=1}^{n}\rho_V(x_j)\circ i_{x^j}\circ\mathcal{L}_x\omega\\
&=\sum_{j=1}^{n}\rho_V([x,x_j])\circ i_{x^j}\omega+\sum_{j=1}^{n}\rho_V(x_j)\circ i_{[x_j,x^j]}\omega+\Gamma(\mathcal{L}_x\omega).
\end{align*}
If we set $[x,x_j]=\sum_{k=1}^{n}a_{kj}x_k$ and $[x,x^j]=\sum_{k=1}^{n}a^{kj}x^k$, then
\[a_{kj}=\kappa([x,x_j],x^k)=-\kappa([x_j,[x,x^k]])=-a^{jk}.\]
Therefore
\begin{equation*}\scriptstyle
\begin{aligned}
\sum_{j=1}^{n}(\rho_V([x,x_j])\circ i_{x^j}\omega+\rho_V(x_j)\circ i_{[x_j,x^j]}\omega)=\sum_{j,k}a_{kj}(\rho_V(x_k)\circ i_{x^j}\omega+a^{kj}\rho_V(x_j)\circ i_{x^k}\omega)=0.
\end{aligned}
\end{equation*}
We conclude that $\Gamma$ is $\g$-equivariant, hence maps $\g$-invariant cochains to $\g$-invariant cochains.\par
For $\omega\in Z^p(\g,V)^\g$, by Lemma~\ref{Lie algebra semisimple differential on trivial module} we obtain $d^0\omega=-d\omega=0$. We also note that the representation $\mathcal{L}$ of $\g$ on $C(\g,V)$ can be written as $\mathcal{L}_x=\rho_V(x)+\rho_0(x)$, where $\rho_0$ is the representation corresponding to the trivial $\g$-module structure on $V$. Since $\omega$ is $\g$-invariant, we therefore have
\[\rho_V(x)\omega=-\rho_0(x)\omega.\]
Also, we note that
\[\mathcal{L}_{x_j}\circ i_{x^j}\omega=[\mathcal{L}_{x_j},i_{x^j}]\omega+i_{x^j}\circ\mathcal{L}_{x^j}\omega=i_{[x^j,x^j]}\omega=0,\]
and thus
\[\rho_V(x_j)\circ i_{x^j}\omega=-\rho_0(x_j)\circ i_{x^j}\omega.\]
With these, we now compute with the Cartan formulas for the trivial $\g$-module $V$ and the corresponding representation $\rho_0$ of $\g$ on $C^p(\g,V)$ that
\begin{align*}
d(\Gamma(\omega))&=-d^0(\Gamma(\omega))=-\sum_{j=1}^{n}d^0\circ\rho_V(x_j)\circ i_{x^j}\omega=-\sum_{j=1}^{n}d^0\circ\rho_0(x_j)\circ i_{x^j}\omega\\
&=-\sum_{j=1}^{n}\rho_0(x_j)\circ d^0\circ i_{x^j}\omega=-\sum_{j=1}^{n}\rho_V(x_j)\circ(\rho_0(x^j)\omega-i_{x^j}\circ d^0\omega)\\
&=-\sum_{j=1}^{n}\rho_V(x_j)\circ\rho_0(x^j)\omega=\sum_{j=1}^{n}\rho_V(x_j)\circ\rho_V(x^j)\omega=\rho_V(C_\g)\omega,
\end{align*}
where $C_\g\in U(\g)$ is the universal Casimir operator of $\g$. Since $V$ is a simple $\g$-module, $\rho_V(C_\g)$ is a nonzero multiple of $\id_V$, so that $\omega$ is a coboundary.
\end{proof}
To conclude this part, we prove an important property for $\sl(2,\K)$-modules which will be used later. We consider the elements
\[\theta=\exp(\ad(e))\exp(-\ad(f))\exp(\ad(e))\in\Aut(\sl_2(\K))\]
and
\[\phi=\exp(e)\exp(-f)\exp(e)=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}\in\sl_2(\K).\]
Recall that $\exp\circ\,\mathrm{ad}=\Ad\circ\exp$, so for $z\in\sl_2(\K)$ we have
\[\theta(z)=\Ad(\exp(e))\Ad(\exp(-f))\Ad(\exp(e))(z)=\Ad(\exp(e)\exp(-f)\exp(e))(z)=\phi z\phi^{-1}.\]
In particular, a calculation shows $\theta(h)=-h$. This gives rise to the following result.
\begin{proposition}\label{sl_2(K)-module eigenspace of h}
Let $(V,\rho)$ be a finite-dimensional representation of $\sl_2(\K)$ and set
\[\phi_V:=e^{\rho(e)}e^{-\rho(f)}e^{\rho(e)}\in\GL(V).\]
Then for $z\in\sl_2(\K)$, we have
\[\phi_V\rho(z)\phi_V^{-1}=\rho(\phi z\phi^{-1})\]
and for each eigenspace $V_\alpha(\rho(h))$ we have
\[\phi_V(V_\alpha(\rho(h)))=V_{-\alpha}(\rho(h)).\]
In particular, $\dim V_\alpha(\rho(h))=V_{-\alpha}(\rho(h))$.
\end{proposition}
\begin{proof}
For $z\in\sl_2(\K)$, we obtain from $\exp\circ\,\mathrm{ad}=\Ad\circ\exp$ that $\phi_V\rho(z)\phi_V^{-1}=\rho(\phi z\phi^{-1})$. This implies the second claim because for $v\in V_\alpha(\rho(h))$,
\[\rho(h)(\phi_V(v))=\phi_V(\phi_V^{-1}\rho(h)\phi_V)(v)=\phi_V\rho(\phi h\phi^{-1})(v)=\phi_V\rho(-h)(v)=-\alpha\phi_V(v).\]
Simialrly, for $w\in V_{-\alpha}(\rho(h))$ we have
\[\rho(h)(\phi_V^{-1}(w))=\phi_V^{-1}(\phi_V\rho(h)\phi_V^{-1})(w)=\phi_V^{-1}\rho(\phi h\phi^{-1})(w)=\phi_V^{-1}\rho(-h)(w)=\alpha\phi_V^{-1}(w).\]
This completes the proof.
\end{proof}